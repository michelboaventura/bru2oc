{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement error types and diagnostic system",
        "description": "Create comprehensive error handling types for parse errors, I/O errors, transform errors, and verification errors with file path and line number tracking",
        "details": "Create `src/errors.zig` with:\n- `ParseError` enum: UnclosedBlock, InvalidMultistring, InvalidAnnotation, CommentAfterValue, InvalidNumber, etc.\n- `DiagnosticInfo` struct with file_path ([]const u8), line (usize), column (usize), message ([]const u8)\n- `ConversionError` tagged union combining ParseError, IoError, TransformError, VerifyError\n- Helper function `formatError` that produces user-friendly error messages\n- Error context wrapper that preserves stack traces in debug builds\nUse Zig's error union types (`!T`) and error sets for composability. Include source location tracking for every error variant.",
        "testStrategy": "Unit tests for error formatting, verify error messages include file path and line number, test error propagation through error unions",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ParseError error set with all parsing error variants",
            "description": "Define comprehensive ParseError set covering all possible parsing failures in Bru format",
            "dependencies": [],
            "details": "In `src/errors.zig`, create ParseError error set with variants:\n- UnclosedBlock (missing }, ], or closing quote)\n- InvalidMultistring (incorrect indentation or nested quote issues)\n- InvalidAnnotation (malformed @syntax or invalid name)\n- CommentAfterValue (# appearing after value on same line)\n- InvalidNumber (malformed numeric literal)\n- UnexpectedToken (token in wrong context)\n- MissingColon (key without colon separator)\n- DuplicateBlock (contradictory blocks like multiple auth:bearer)\n- InvalidIndentation (multistring content not properly indented)\n- UnterminatedString (missing closing quote)\n\nUse Zig's error set syntax: `error{UnclosedBlock, InvalidMultistring, ...}`. This will integrate with Zig's error union types (!T) for composable error handling throughout the parser.",
            "status": "pending",
            "testStrategy": "Unit tests verifying each error variant can be returned and caught in error unions, test error set composition with other error sets using || operator"
          },
          {
            "id": 2,
            "title": "Create IoError, TransformError, and VerifyError sets",
            "description": "Define error sets for I/O operations, IR transformation, and YAML verification phases",
            "dependencies": [
              1
            ],
            "details": "In `src/errors.zig`, create three additional error sets:\n\n1. IoError covering file operations:\n- FileNotFound\n- PermissionDenied\n- InvalidPath\n- ReadFailure\n- WriteFailure\n- DirectoryCreationFailed\n\n2. TransformError for IR to OpenCollection conversion:\n- MissingRequiredField (e.g., missing HTTP method)\n- InvalidFieldValue (e.g., invalid auth type)\n- ConflictingBlocks (multiple method blocks)\n- UnsupportedFeature (Bru feature not mappable to OpenCollection)\n- AnnotationResolutionFailed\n\n3. VerifyError for output validation:\n- InvalidYamlSyntax\n- MalformedOutput\n- SchemaValidationFailed\n\nThese will compose with ParseError using `||` operator for unified error handling across the conversion pipeline.",
            "status": "pending",
            "testStrategy": "Unit tests for each error set, test error composition with || operator, verify errors propagate correctly through error unions"
          },
          {
            "id": 3,
            "title": "Implement DiagnosticInfo struct with source location tracking",
            "description": "Create struct to capture file path, line number, column, and error context for user-friendly error reporting",
            "dependencies": [],
            "details": "In `src/errors.zig`, implement DiagnosticInfo struct:\n\n```zig\npub const DiagnosticInfo = struct {\n    file_path: []const u8,\n    line: usize,\n    column: usize,\n    message: []const u8,\n    \n    // Optional context: show source line with error position\n    source_line: ?[]const u8 = null,\n    \n    pub fn init(file_path: []const u8, line: usize, column: usize, message: []const u8) DiagnosticInfo {\n        return .{\n            .file_path = file_path,\n            .line = line,\n            .column = column,\n            .message = message,\n        };\n    }\n};\n```\n\nThis struct will be embedded in ConversionError to provide context for every error. Lines and columns are 1-indexed to match editor conventions.",
            "status": "pending",
            "testStrategy": "Unit tests for struct initialization, verify field access, test with optional source_line context, validate 1-indexed line/column handling"
          },
          {
            "id": 4,
            "title": "Create ConversionError tagged union combining all error types",
            "description": "Implement tagged union that wraps all error categories with diagnostic information",
            "dependencies": [
              2,
              3
            ],
            "details": "In `src/errors.zig`, create ConversionError as a tagged union:\n\n```zig\npub const ConversionError = union(enum) {\n    parse: struct {\n        err: ParseError,\n        diagnostic: DiagnosticInfo,\n    },\n    io: struct {\n        err: IoError,\n        diagnostic: DiagnosticInfo,\n    },\n    transform: struct {\n        err: TransformError,\n        diagnostic: DiagnosticInfo,\n    },\n    verify: struct {\n        err: VerifyError,\n        diagnostic: DiagnosticInfo,\n    },\n    \n    pub fn fromParse(err: ParseError, diagnostic: DiagnosticInfo) ConversionError {\n        return .{ .parse = .{ .err = err, .diagnostic = diagnostic } };\n    }\n    \n    // Similar helper constructors for io, transform, verify\n};\n```\n\nThis provides type-safe error categorization while preserving diagnostic context. Each variant carries both the specific error and its location.",
            "status": "pending",
            "testStrategy": "Unit tests for creating each error variant, test helper constructors, verify tagged union pattern matching, test error extraction and diagnostic access"
          },
          {
            "id": 5,
            "title": "Implement formatError and error context utilities",
            "description": "Create user-friendly error formatting function and debug-mode stack trace preservation",
            "dependencies": [
              4
            ],
            "details": "In `src/errors.zig`, implement error formatting and context utilities:\n\n1. `formatError` function that produces readable error messages:\n```zig\npub fn formatError(allocator: std.mem.Allocator, err: ConversionError) ![]const u8 {\n    // Format: \"error: <file>:<line>:<column>: <message>\"\n    // Include source line with caret (^) pointing to error column if available\n    // Use ArrayList to build formatted string\n}\n```\n\n2. Error context wrapper for debug builds:\n```zig\npub fn errorWithContext(comptime err_type: type, err: err_type, diagnostic: DiagnosticInfo) ConversionError {\n    if (builtin.mode == .Debug) {\n        // Capture stack trace using std.debug.dumpCurrentStackTrace\n    }\n    // Return appropriate ConversionError variant\n}\n```\n\nThis completes the error handling system with user-friendly output and debug support.",
            "status": "pending",
            "testStrategy": "Unit tests for formatError with various error types, test output format matches expected pattern (file:line:column: message), test source line context rendering with caret, verify debug mode stack trace capture, test with std.testing.allocator for memory safety"
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement intermediate representation (IR) types",
        "description": "Define type-safe IR structures for representing parsed Bru content before transformation to OpenCollection",
        "details": "Create `src/ir.zig` with:\n- `Value` tagged union: Null, Bool(bool), Number([]const u8), String([]const u8), Multimap([]Entry), Array([]Value), Multistring([]const u8)\n- `Entry` struct: annotations ([]Annotation), key ([]const u8), value (Value), disabled (bool)\n- `Annotation` struct: name ([]const u8), args ([]Value)\n- `BruDocument` struct containing the top-level multimap\n- Memory management strategy: use ArenaAllocator for entire document lifetime\n- String interning for duplicate keys/values to save memory\nPreserve original string representation for numbers to maintain fidelity. Store line numbers with each Entry for error reporting.",
        "testStrategy": "Unit tests for IR construction, memory allocation/deallocation with ArenaAllocator, verify number string preservation",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Value tagged union with all variant types",
            "description": "Create the core Value tagged union representing all possible Bru value types including primitives, composites, and multistrings",
            "dependencies": [],
            "details": "In `src/ir.zig`, define the Value tagged union:\n```zig\npub const Value = union(enum) {\n    null: void,\n    bool: bool,\n    number: []const u8,  // Preserve original string for fidelity\n    string: []const u8,\n    multimap: []Entry,\n    array: []Value,\n    multistring: []const u8,\n    \n    pub fn deinit(self: Value, allocator: std.mem.Allocator) void {\n        // Note: When using ArenaAllocator, deinit is typically a no-op\n        // but keep structure for flexibility\n        _ = self;\n        _ = allocator;\n    }\n};\n```\nThis tagged union provides type-safe discrimination between all value types. Numbers stored as strings preserve exact representation (e.g., '1.0' vs '1', large integers). Slices ([]const u8, []Entry, []Value) point to memory managed by ArenaAllocator.",
            "status": "pending",
            "testStrategy": "Unit tests: create each variant type, verify tag discrimination with switch statements, test number string preservation (\"123\", \"1.0\", \"1e10\"), verify memory layout with @sizeOf and @alignOf"
          },
          {
            "id": 2,
            "title": "Implement Annotation struct with name and arguments",
            "description": "Define the Annotation structure for storing Bru annotations like @disabled, @description, and custom annotations with primitive arguments",
            "dependencies": [],
            "details": "In `src/ir.zig`, create the Annotation struct:\n```zig\npub const Annotation = struct {\n    name: []const u8,      // e.g., \"disabled\", \"description\", \"enum\"\n    args: []Value,         // Array of primitive Value arguments\n    \n    pub fn init(name: []const u8, args: []Value) Annotation {\n        return .{ .name = name, .args = args };\n    }\n};\n```\nAnnotations capture metadata from @ syntax in Bru files. Name is the identifier after @. Args are primitives only (null, bool, number, string) - no nested arrays/multimaps per spec. Empty args slice for annotations without parentheses like @disabled.",
            "status": "pending",
            "testStrategy": "Unit tests: create annotations with various names (@disabled, @description, @custom), test with no args, single arg, multiple args, verify primitive-only constraint in parser (not here), test init function"
          },
          {
            "id": 3,
            "title": "Implement Entry struct with key-value pairs and metadata",
            "description": "Create the Entry structure representing key-value pairs in Bru multimaps with support for annotations, disabled state, and line tracking",
            "dependencies": [
              1,
              2
            ],
            "details": "In `src/ir.zig`, implement the Entry struct:\n```zig\npub const Entry = struct {\n    key: []const u8,\n    value: Value,\n    annotations: []Annotation,  // Annotations from lines immediately before this entry\n    disabled: bool,              // true if entry has ~prefix or @disabled annotation\n    line: usize,                 // Source line number for error reporting (1-indexed)\n    \n    pub fn init(key: []const u8, value: Value, line: usize) Entry {\n        return .{\n            .key = key,\n            .value = value,\n            .annotations = &[_]Annotation{},  // Empty slice by default\n            .disabled = false,\n            .line = line,\n        };\n    }\n};\n```\nEntries are the fundamental key-value pairs in Bru multimaps. Line tracking enables precise error messages. Disabled state can come from ~key: syntax or @disabled annotation. Annotations slice allows metadata attachment.",
            "status": "pending",
            "testStrategy": "Unit tests: create entries with various value types, test init function, verify default values (empty annotations, disabled=false), test with line numbers, create entries with annotations and disabled=true"
          },
          {
            "id": 4,
            "title": "Implement BruDocument struct with top-level multimap",
            "description": "Define the root document structure that holds the parsed Bru file as a top-level multimap with metadata",
            "dependencies": [
              3
            ],
            "details": "In `src/ir.zig`, create the BruDocument struct:\n```zig\npub const BruDocument = struct {\n    entries: []Entry,           // Top-level multimap entries (implicit, no braces in .bru)\n    file_path: []const u8,      // Original source file path for error reporting\n    arena: *std.heap.ArenaAllocator,  // Arena managing all document memory\n    \n    pub fn init(allocator: std.mem.Allocator, file_path: []const u8) !BruDocument {\n        const arena = try allocator.create(std.heap.ArenaAllocator);\n        arena.* = std.heap.ArenaAllocator.init(allocator);\n        \n        return BruDocument{\n            .entries = &[_]Entry{},\n            .file_path = try arena.allocator().dupe(u8, file_path),\n            .arena = arena,\n        };\n    }\n    \n    pub fn deinit(self: *BruDocument) void {\n        const backing_allocator = self.arena.child_allocator;\n        self.arena.deinit();\n        backing_allocator.destroy(self.arena);\n    }\n};\n```\nRepresents complete parsed .bru file. Top-level is always an implicit multimap (no braces). Arena allocator owns all strings, slices, and nested structures.",
            "status": "pending",
            "testStrategy": "Unit tests: init/deinit lifecycle, verify arena allocator setup, test with std.testing.allocator to detect leaks, create document with entries, verify file_path storage, test empty document"
          },
          {
            "id": 5,
            "title": "Add string interning support and memory management utilities",
            "description": "Implement string interning functionality to deduplicate common strings and reduce memory usage, plus helper utilities for IR construction",
            "dependencies": [
              4
            ],
            "details": "In `src/ir.zig`, add string interning and utilities:\n```zig\npub const StringInterner = struct {\n    map: std.StringHashMap([]const u8),  // Maps strings to their interned versions\n    arena: std.mem.Allocator,\n    \n    pub fn init(arena: std.mem.Allocator) StringInterner {\n        return .{\n            .map = std.StringHashMap([]const u8).init(arena),\n            .arena = arena,\n        };\n    }\n    \n    pub fn intern(self: *StringInterner, str: []const u8) ![]const u8 {\n        const entry = try self.map.getOrPut(str);\n        if (!entry.found_existing) {\n            const duped = try self.arena.dupe(u8, str);\n            entry.value_ptr.* = duped;\n        }\n        return entry.value_ptr.*;\n    }\n};\n\n// Helper function for creating values in arena\npub fn createString(arena: std.mem.Allocator, str: []const u8) ![]const u8 {\n    return arena.dupe(u8, str);\n}\n\npub fn createArray(arena: std.mem.Allocator, values: []const Value) ![]Value {\n    return arena.dupe(Value, values);\n}\n\npub fn createEntries(arena: std.mem.Allocator, entries: []const Entry) ![]Entry {\n    return arena.dupe(Entry, entries);\n}\n```\nString interning saves memory for duplicate keys (\"Content-Type\", \"Authorization\", etc.) and common values (\"null\", \"true\", \"false\"). Helper functions simplify IR construction in parser.",
            "status": "pending",
            "testStrategy": "Unit tests: test string interning with duplicates (verify same pointer returned), test with unique strings, test helper functions with arena allocator, benchmark memory savings with interning vs without on realistic data, test createArray and createEntries functions"
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Bru tokenizer",
        "description": "Create a tokenizer that converts raw Bru text into a stream of tokens with position tracking",
        "details": "Create tokenizer in `src/bru_parser.zig`:\n- `Token` enum: LBrace, RBrace, LBracket, RBracket, Colon, String, Number, Bool, Null, Newline, Comment, Annotation, TripleQuote, Eof\n- `TokenWithPos` struct: token (Token), line (usize), column (usize), lexeme ([]const u8)\n- `Tokenizer` struct with methods: init(source: []const u8), next() !?TokenWithPos, peek() ?TokenWithPos\n- Normalize CRLF → LF upfront in init\n- Recognize annotations starting with @ on their own line\n- Handle tilde prefix (~) for disabled keys\n- Track position for error reporting\n- Implement lookahead for disambiguating unquoted strings vs keywords (null, true, false)\nDon't allocate - return slices into original source buffer.",
        "testStrategy": "Unit tests with fixtures for all token types, position tracking verification, CRLF normalization, edge cases like numbers vs strings",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Token enum and TokenWithPos struct",
            "description": "Create the foundational token types and position tracking structures for the Bru tokenizer",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, define:\n\n1. `Token` enum with all required variants:\n   - Structural: LBrace, RBrace, LBracket, RBracket, Colon\n   - Literals: String, Number, Bool, Null\n   - Special: Newline, Comment, Annotation, TripleQuote, Eof\n\n2. `TokenWithPos` struct containing:\n   - token: Token (the token type)\n   - line: usize (1-indexed line number for error reporting)\n   - column: usize (1-indexed column number for error reporting)\n   - lexeme: []const u8 (slice into original source, no allocation)\n\nUse Zig's tagged union for Token to allow carrying associated data where needed (e.g., String token could carry the string slice). The lexeme field provides access to the exact source text for all token types. Position tracking is critical for diagnostic error messages.\n\nExample structure:\n```zig\npub const Token = enum {\n    LBrace,\n    RBrace,\n    // ... other variants\n};\n\npub const TokenWithPos = struct {\n    token: Token,\n    line: usize,\n    column: usize,\n    lexeme: []const u8,\n};\n```",
            "status": "pending",
            "testStrategy": "Unit tests: create TokenWithPos instances with each Token variant, verify struct field access, test position tracking (1-indexed), verify lexeme slices point into source without allocation"
          },
          {
            "id": 2,
            "title": "Implement Tokenizer struct with initialization and CRLF normalization",
            "description": "Create the Tokenizer state machine with source text normalization and position tracking infrastructure",
            "dependencies": [
              1
            ],
            "details": "In `src/bru_parser.zig`, implement the `Tokenizer` struct:\n\n```zig\npub const Tokenizer = struct {\n    source: []const u8,     // Normalized source (CRLF → LF)\n    pos: usize,              // Current position in source\n    line: usize,             // Current line (1-indexed)\n    column: usize,           // Current column (1-indexed)\n    peek_token: ?TokenWithPos, // Lookahead token for peek()\n    \n    pub fn init(source: []const u8) Tokenizer {\n        // Normalize CRLF → LF in-place strategy:\n        // Create normalized version by filtering out \\r characters\n        // This requires allocation - use caller's allocator\n        // Track initial position and line/column\n        return .{\n            .source = normalized_source,\n            .pos = 0,\n            .line = 1,\n            .column = 1,\n            .peek_token = null,\n        };\n    }\n};\n```\n\nKey implementation details:\n- CRLF normalization: scan source for \\r\\n sequences, replace with \\n\n- Position tracking: increment line on \\n, reset column to 1; increment column otherwise\n- Store peek token for lookahead support\n- No-allocation goal: consider if CRLF normalization can be lazy/virtual\n\nNote: The init function may need to accept an allocator for CRLF normalization, or use a different strategy that doesn't allocate (e.g., skip \\r during tokenization).",
            "status": "pending",
            "testStrategy": "Unit tests: init with LF-only source, init with CRLF source (verify normalization), init with mixed line endings, test position tracking initialization, verify no memory leaks with std.testing.allocator"
          },
          {
            "id": 3,
            "title": "Implement core tokenization logic for structural tokens and primitives",
            "description": "Build the main next() method to tokenize structural characters, keywords, numbers, and strings",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, implement the `next()` method:\n\n```zig\npub fn next(self: *Tokenizer) !?TokenWithPos {\n    // Skip whitespace (spaces, tabs) but not newlines\n    self.skipWhitespace();\n    \n    // Check for EOF\n    if (self.pos >= self.source.len) return self.makeToken(.Eof, \"\");\n    \n    const start_line = self.line;\n    const start_column = self.column;\n    const start_pos = self.pos;\n    const ch = self.source[self.pos];\n    \n    // Match tokens:\n    // Structural: { } [ ] :\n    // Newline: \\n\n    // Comment: # at line start\n    // Annotation: @ at line start\n    // String: quoted (' or \") or unquoted\n    // Number: [-+]?[0-9]+\n    // Bool: true, false (keywords)\n    // Null: null (keyword)\n    // TripleQuote: ''' or \"\"\"\n    \n    // Implement lookahead for disambiguation:\n    // - true/false/null vs unquoted string\n    // - numbers vs unquoted strings starting with digits\n    \n    self.advance(); // consume character\n    const lexeme = self.source[start_pos..self.pos];\n    return self.makeToken(token_type, lexeme);\n}\n\nfn makeToken(self: *Tokenizer, token: Token, lexeme: []const u8) TokenWithPos {\n    return .{ .token = token, .line = line, .column = column, .lexeme = lexeme };\n}\n```\n\nHandle special cases:\n- Tilde prefix (~) for disabled keys: include in lexeme\n- Comments: # must be at start of line (after whitespace)\n- Annotations: @ must be at start of line\n- Triple quotes: detect ''' or \"\"\" as single token\n- Number recognition: regex-like pattern matching\n- Keyword vs identifier: peek ahead to check word boundaries",
            "status": "pending",
            "testStrategy": "Unit tests: tokenize structural chars ({, }, [, ], :), tokenize newlines, tokenize keywords (null, true, false), tokenize numbers (integer, float, scientific, negative), tokenize quoted strings (single, double quotes), tokenize unquoted strings, test lookahead disambiguation, test tilde prefix, test position tracking accuracy"
          },
          {
            "id": 4,
            "title": "Implement annotation and comment tokenization with position validation",
            "description": "Add specialized tokenization for Bru annotations (@name) and comments (#) with line position constraints",
            "dependencies": [
              3
            ],
            "details": "Extend the `next()` method in `src/bru_parser.zig` to handle annotations and comments:\n\n```zig\n// In next() method, add cases:\n\nif (ch == '#') {\n    // Comment: must be at start of line (column 1 after whitespace skip)\n    // Extract everything from # to end of line\n    // Return Comment token with text as lexeme\n    return self.scanComment();\n}\n\nif (ch == '@') {\n    // Annotation: must be at start of line\n    // Format: @name or @name(args)\n    // Scan identifier after @\n    // Return Annotation token\n    return self.scanAnnotation();\n}\n```\n\nImplement helper methods:\n\n```zig\nfn scanComment(self: *Tokenizer) !TokenWithPos {\n    // Scan from # to end of line\n    // Do not include # in lexeme, only comment text\n    // Stop at \\n but don't consume it\n    const start = self.pos;\n    while (self.pos < self.source.len and self.source[self.pos] != '\\n') {\n        self.pos += 1;\n        self.column += 1;\n    }\n    const lexeme = self.source[start..self.pos];\n    return self.makeToken(.Comment, lexeme);\n}\n\nfn scanAnnotation(self: *Tokenizer) !TokenWithPos {\n    // Scan @name pattern\n    // Name must match: [_a-zA-Z][-_a-zA-Z0-9]*\n    // Include @ in lexeme\n    // Full annotation parsing (with args) handled in parser, not tokenizer\n    const start = self.pos;\n    self.pos += 1; // skip @\n    // Scan identifier characters\n    while (self.pos < self.source.len and isAnnotationChar(self.source[self.pos])) {\n        self.pos += 1;\n    }\n    const lexeme = self.source[start..self.pos];\n    return self.makeToken(.Annotation, lexeme);\n}\n```\n\nPosition validation:\n- Track whether we're at the start of a line (after newline or at position 0)\n- Comments and annotations only valid at line start (after whitespace)\n- Set flag `at_line_start` that resets on newline, cleared on non-whitespace",
            "status": "pending",
            "testStrategy": "Unit tests: tokenize valid comments (# at line start), tokenize valid annotations (@disabled, @description), test position validation (reject # after value on same line), test annotation name patterns (valid identifiers), test lexeme extraction (verify content), test edge cases (empty comment, annotation at EOF)"
          },
          {
            "id": 5,
            "title": "Implement peek() method and triple-quote tokenization",
            "description": "Add lookahead support and specialized handling for triple-quoted multistring delimiters",
            "dependencies": [
              4
            ],
            "details": "Complete the Tokenizer implementation in `src/bru_parser.zig`:\n\n1. Implement `peek()` method for one-token lookahead:\n\n```zig\npub fn peek(self: *Tokenizer) ?TokenWithPos {\n    if (self.peek_token) |token| {\n        return token;\n    }\n    \n    // Save current position\n    const saved_pos = self.pos;\n    const saved_line = self.line;\n    const saved_column = self.column;\n    \n    // Get next token\n    const token = self.next() catch return null;\n    \n    // Restore position\n    self.pos = saved_pos;\n    self.line = saved_line;\n    self.column = saved_column;\n    \n    // Cache for subsequent peek calls\n    self.peek_token = token;\n    \n    return token;\n}\n```\n\n2. Add triple-quote detection in `next()`:\n\n```zig\nif (ch == '\\'' or ch == '\"') {\n    // Check for triple quote: ''' or \"\"\"\n    if (self.pos + 2 < self.source.len and\n        self.source[self.pos + 1] == ch and\n        self.source[self.pos + 2] == ch) {\n        // Triple quote\n        self.pos += 3;\n        self.column += 3;\n        const quote_type = if (ch == '\\'') \"'''\" else \"\\\"\\\"\\\"\";\n        return self.makeToken(.TripleQuote, quote_type);\n    }\n    // Regular quoted string\n    return self.scanQuotedString(ch);\n}\n```\n\n3. Implement `scanQuotedString()` helper:\n\n```zig\nfn scanQuotedString(self: *Tokenizer, quote: u8) !TokenWithPos {\n    const start = self.pos;\n    self.pos += 1; // skip opening quote\n    \n    while (self.pos < self.source.len) {\n        const ch = self.source[self.pos];\n        if (ch == quote) {\n            self.pos += 1; // include closing quote\n            break;\n        }\n        if (ch == '\\\\') {\n            self.pos += 2; // skip escape sequence\n        } else {\n            self.pos += 1;\n        }\n    }\n    \n    const lexeme = self.source[start..self.pos];\n    return self.makeToken(.String, lexeme);\n}\n```\n\nKey features:\n- peek() enables parser to look ahead without consuming tokens\n- peek() caches result to avoid re-tokenizing\n- Triple-quote detection distinguishes ''' and \"\"\" from regular quotes\n- All methods return slices into original source (zero-copy)",
            "status": "pending",
            "testStrategy": "Unit tests: test peek() without consuming token, test peek() caching (multiple calls return same token), test peek() followed by next() (verify same token), test triple-quote detection (''' and \"\"\"), test regular quoted strings (single, double), test escape sequences in strings, test position restoration in peek(), verify zero-copy tokenization (all lexemes are slices)"
          }
        ]
      },
      {
        "id": 4,
        "title": "Implement primitive value parsers",
        "description": "Create composable parsing functions for Bru primitives: null, boolean, number, quoted strings, unquoted strings",
        "details": "Add to `src/bru_parser.zig`:\n- `parseNull() !Value` - matches 'null' keyword\n- `parseBool() !Value` - matches 'true' or 'false'\n- `parseNumber() !Value` - regex: ^[-+]?[0-9]+(\\.[0-9]+)?([eE][-+]?[0-9]+)?$, preserve original string\n- `parseQuotedString() !Value` - handles single/double quotes, JSON-style escapes (\\n, \\t, \\\", \\\\, etc.)\n- `parseUnquotedString() !Value` - trim surrounding whitespace, preserve interior whitespace, literal backslash\n- `parseString() !Value` - dispatcher that tries quoted then unquoted\n- Special char detection: values starting with {}[],:'\\'\"# must be quoted\n- Number fidelity: if number exceeds JS safe integer range or has high precision, keep as quoted string\nAll functions consume from Tokenizer and return IR Values.",
        "testStrategy": "Unit tests for each primitive type, edge cases: empty strings, escaped characters, large numbers, special characters, whitespace handling",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement parseNull and parseBool primitive parsers",
            "description": "Create parsing functions for null and boolean literals with keyword matching",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, implement two primitive value parsers:\n\n1. `parseNull(tokenizer: *Tokenizer) !Value`:\n   - Consume token from tokenizer\n   - Match against 'null' keyword exactly\n   - Return Value.null (void variant)\n   - Error if token is not null keyword\n   - Use case-sensitive matching\n\n2. `parseBool(tokenizer: *Tokenizer) !Value`:\n   - Consume token from tokenizer\n   - Match against 'true' or 'false' keywords exactly\n   - Return Value.bool(true) or Value.bool(false)\n   - Error if token is neither true nor false\n   - Use case-sensitive matching\n\nBoth functions consume from Tokenizer (depends on Task 3) and return IR Value types (depends on Task 2). These are the simplest primitive parsers with fixed keyword matching.",
            "status": "pending",
            "testStrategy": "Unit tests: parse valid null, parse valid true, parse valid false, error on invalid keywords, error on case mismatch (Null, TRUE, False), verify Value type returned, test with tokenizer position tracking"
          },
          {
            "id": 2,
            "title": "Implement parseNumber with string preservation and fidelity handling",
            "description": "Create number parser that preserves original string representation and handles large numbers and high precision",
            "dependencies": [
              1
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`parseNumber(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value`:\n- Consume number token from tokenizer\n- Validate against regex pattern: ^[-+]?[0-9]+(\\.[0-9]+)?([eE][-+]?[0-9]+)?$\n- Preserve original string representation exactly (store as []const u8)\n- Number fidelity checks:\n  - If number exceeds JavaScript safe integer range (2^53 - 1, approximately ±9,007,199,254,740,991), keep as quoted string\n  - If number has high precision decimals (> 15-17 significant digits), keep as quoted string\n  - This preserves exact representation for large integers and precise decimals\n- Allocate string copy using provided allocator\n- Return Value.number([]const u8) with original string\n- Error on invalid number format\n\nExamples:\n- \"123\" → Value.number(\"123\")\n- \"1.0\" → Value.number(\"1.0\") (preserve .0)\n- \"1e10\" → Value.number(\"1e10\")\n- \"9007199254740992\" → quote as string (exceeds JS safe integer)\n- \"-3.14159265358979323846\" → preserve exact precision",
            "status": "pending",
            "testStrategy": "Unit tests: parse integers, parse floats, parse scientific notation, parse negative numbers, parse numbers with + prefix, verify string preservation (\"1.0\" ≠ \"1\"), test large number handling (beyond JS safe integer), test high precision decimals, test invalid formats, verify no precision loss, memory allocation testing"
          },
          {
            "id": 3,
            "title": "Implement parseQuotedString with escape sequence handling",
            "description": "Create quoted string parser supporting single/double quotes and JSON-style escape sequences",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`parseQuotedString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value`:\n- Consume string token from tokenizer\n- Detect opening quote: single (') or double (\")\n- Parse string content until matching closing quote\n- Handle JSON-style escape sequences:\n  - \\n → newline (U+000A)\n  - \\t → tab (U+0009)\n  - \\r → carriage return (U+000D)\n  - \\\" → double quote\n  - \\' → single quote\n  - \\\\ → backslash\n  - \\/ → forward slash\n  - \\b → backspace (U+0008)\n  - \\f → form feed (U+000C)\n  - \\uXXXX → Unicode code point (4 hex digits)\n- Allocate unescaped string using allocator\n- Return Value.string([]const u8)\n- Error on:\n  - Unterminated string (missing closing quote)\n  - Invalid escape sequence\n  - Invalid Unicode escape\n  - Mismatched quotes\n\nExamples:\n- '\"hello\"' → \"hello\"\n- \"it's\" → it's\n- \"line1\\nline2\" → line1[newline]line2\n- \"path\\\\to\\\\file\" → path\\to\\file",
            "status": "pending",
            "testStrategy": "Unit tests: parse single-quoted strings, parse double-quoted strings, parse each escape sequence (\\n, \\t, \\r, \\\", \\', \\\\, \\/, \\b, \\f), parse Unicode escapes (\\u0041 → A), test nested quotes (\"it's\" and 'say \"hi\"'), test empty strings, error on unterminated strings, error on invalid escapes, error on mismatched quotes, verify allocation and memory safety"
          },
          {
            "id": 4,
            "title": "Implement parseUnquotedString with whitespace handling and special character detection",
            "description": "Create unquoted string parser with whitespace trimming and validation for special characters that require quoting",
            "dependencies": [
              3
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`parseUnquotedString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value`:\n- Consume token from tokenizer\n- Extract unquoted string value\n- Whitespace handling:\n  - Trim leading whitespace\n  - Trim trailing whitespace\n  - Preserve interior whitespace exactly\n- Backslash handling:\n  - Backslashes are literal characters (no escape processing)\n  - \"path\\to\\file\" remains \"path\\to\\file\"\n- Special character detection - ERROR if value starts with:\n  - { } (braces - multimap delimiters)\n  - [ ] (brackets - array delimiters)\n  - , (comma - separator)\n  - : (colon - key-value separator)\n  - ' \" (quotes - string delimiters)\n  - # (hash - comment marker)\n  - These MUST be quoted to be valid\n- Allocate trimmed string using allocator\n- Return Value.string([]const u8)\n- Error on:\n  - Empty string after trimming (use empty quoted string \"\" instead)\n  - Value starting with special characters\n\nExamples:\n- \"  hello  \" → \"hello\"\n- \"hello world\" → \"hello world\" (interior space preserved)\n- \"path\\to\\file\" → \"path\\to\\file\" (literal backslash)\n- \"{test}\" → ERROR (must be quoted)\n- \"#comment\" → ERROR (must be quoted)",
            "status": "pending",
            "testStrategy": "Unit tests: parse simple unquoted strings, test whitespace trimming (leading, trailing), test interior whitespace preservation, test literal backslashes, error on special char prefixes ({, }, [, ], ,, :, ', \", #), test with multi-word strings, test edge cases (single char, all whitespace), verify allocation and memory safety"
          },
          {
            "id": 5,
            "title": "Implement parseString dispatcher and integration with tokenizer",
            "description": "Create main string parsing dispatcher that tries quoted then unquoted strings and integrates all primitive parsers",
            "dependencies": [
              4
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`parseString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value`:\n- Dispatcher function that handles both quoted and unquoted strings\n- Peek at next token from tokenizer\n- Decision logic:\n  - If token starts with ' or \" → call parseQuotedString()\n  - Otherwise → call parseUnquotedString()\n- Return result from appropriate parser\n- Handle errors from both parsers\n\nIntegration testing:\n- Ensure all primitive parsers (parseNull, parseBool, parseNumber, parseString) work together\n- Verify tokenizer consumption is correct\n- Test disambiguation between:\n  - Keywords (null, true, false) vs unquoted strings\n  - Numbers vs unquoted strings starting with digits\n  - Quoted vs unquoted strings\n- Memory management:\n  - All parsers use same allocator pattern\n  - Proper error cleanup if parsing fails\n  - No memory leaks on error paths\n\nPublic API summary:\n- parseNull(tokenizer: *Tokenizer) !Value\n- parseBool(tokenizer: *Tokenizer) !Value\n- parseNumber(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value\n- parseQuotedString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value\n- parseUnquotedString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value\n- parseString(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !Value\n\nAll functions return IR Value types from Task 2 and consume from Tokenizer from Task 3.",
            "status": "pending",
            "testStrategy": "Integration tests: test parseString dispatcher with quoted strings, test with unquoted strings, test disambiguation (keywords vs strings, numbers vs strings), test all primitive types together, verify correct parser selection, test error propagation from sub-parsers, memory leak detection with std.testing.allocator, test with various tokenizer states, verify position tracking after parsing"
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement annotation parser",
        "description": "Parse Bru annotations with name validation and primitive argument parsing",
        "details": "Add to `src/bru_parser.zig`:\n- `parseAnnotation() !Annotation` - parses @name or @name(arg1, arg2, ...)\n- Name validation: regex ^[_a-zA-Z][-_a-zA-Z0-9]*$\n- Argument parsing: primitives only (null, bool, number, string), no nested structures\n- Handle parentheses-less annotations (@disabled)\n- Parse comma-separated argument lists\n- String arguments should be quoted in source\n- Error on non-primitive arguments\n- Store original source position for error reporting\nAnnotations must be on the line immediately before their associated key.",
        "testStrategy": "Unit tests for valid annotations (@disabled, @description('text'), @enum('a','b')), invalid names, non-primitive arguments, position tracking",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement annotation name validation function",
            "description": "Create a function to validate annotation names according to the regex pattern ^[_a-zA-Z][-_a-zA-Z0-9]*$",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, implement `isValidAnnotationName(name: []const u8) bool` function:\n- First character must be underscore or letter (a-zA-Z_)\n- Subsequent characters can be letters, digits, hyphens, or underscores ([-_a-zA-Z0-9])\n- Return true if name matches pattern, false otherwise\n- Use character-by-character validation (Zig standard library doesn't have regex)\n- Handle empty string edge case (return false)\n- Examples: valid=\"disabled\", \"my_annotation\", \"_private\", \"test-1\"; invalid=\"1test\", \"@name\", \"\", \"has space\"",
            "status": "pending",
            "testStrategy": "Unit tests: valid names (@disabled, @_private, @test-123, @my_annotation), invalid names (@1test, @-start, @has space, empty string), boundary cases (single char, all character types)"
          },
          {
            "id": 2,
            "title": "Implement primitive argument value parser",
            "description": "Create a function to parse individual primitive arguments (null, bool, number, string) from annotation argument lists",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, implement `parsePrimitiveValue(tokenizer: *Tokenizer) !Value` function:\n- Parse null keyword → Value.null\n- Parse true/false keywords → Value.bool\n- Parse numeric literals → Value.number (preserve as string)\n- Parse quoted strings (single or double quotes) → Value.string\n- Return ParseError.InvalidAnnotation if token is not a primitive (LBrace, LBracket indicate nested structures)\n- Use existing tokenizer methods (peek, next) to consume tokens\n- Validate that strings are properly quoted in source\n- Store lexeme content in returned Value for strings and numbers",
            "status": "pending",
            "testStrategy": "Unit tests: parse null, true, false, numbers (123, 1.5, -10), quoted strings ('text', \"text\"), error on non-primitives ({, [), error on unquoted non-keyword strings, verify Value variants match input"
          },
          {
            "id": 3,
            "title": "Implement argument list parser with comma separation",
            "description": "Create a function to parse comma-separated argument lists within annotation parentheses",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, implement `parseAnnotationArgs(allocator: Allocator, tokenizer: *Tokenizer) ![]Value` function:\n- Expect opening '(' token (caller validates this)\n- Parse arguments using parsePrimitiveValue in loop\n- Expect comma between arguments, error if missing\n- Allow trailing comma before ')' (common pattern)\n- Collect args into ArrayList, return owned slice\n- Return empty slice for parentheses-less annotations (caller checks for '(')\n- Handle whitespace/newlines between args gracefully\n- Return ParseError.InvalidAnnotation on syntax errors\n- Close with ')' token, error if missing",
            "status": "pending",
            "testStrategy": "Unit tests: no args (empty list), single arg, multiple args with commas, trailing comma allowed, error on missing comma, error on unclosed parens, error on non-primitive arg, whitespace handling between args"
          },
          {
            "id": 4,
            "title": "Implement source position tracking for annotations",
            "description": "Create a position tracking structure and integrate it into annotation parsing for error reporting",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, enhance Annotation struct or create wrapper:\n- Add `line: usize` and `column: usize` fields to capture annotation start position\n- Capture position from tokenizer when @ token is encountered\n- Store position in Annotation or create AnnotationWithPos struct similar to TokenWithPos\n- Position should point to @ symbol for clarity in error messages\n- Integrate with existing DiagnosticInfo system (from task 1)\n- Ensure position tracking works with CRLF normalization (positions in normalized source)\n- 1-indexed line and column numbers to match editor conventions",
            "status": "pending",
            "testStrategy": "Unit tests: capture position for annotation at start of file (1:1), middle of file, after multiple lines, verify position accuracy with CRLF vs LF sources, test position in error messages"
          },
          {
            "id": 5,
            "title": "Implement main parseAnnotation function with full integration",
            "description": "Create the main parseAnnotation function that orchestrates name validation, argument parsing, and position tracking",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "In `src/bru_parser.zig`, implement `parseAnnotation(allocator: Allocator, tokenizer: *Tokenizer) !Annotation` function:\n- Expect current token to be Annotation token (@name syntax)\n- Extract annotation name from token lexeme (strip @ prefix)\n- Validate name using isValidAnnotationName, return ParseError.InvalidAnnotation if invalid\n- Capture source position (line, column) from token\n- Check next token: if '(', parse args with parseAnnotationArgs; else empty args\n- Verify annotation is on line immediately before associated key (caller responsibility to validate positioning)\n- Construct and return Annotation struct with name, args, and position\n- Handle errors: invalid name, malformed args, non-primitive args\n- Integration point: called when tokenizer encounters Annotation token before Entry parsing",
            "status": "pending",
            "testStrategy": "Unit tests: @disabled (no args), @description('text') (single arg), @enum('a','b','c') (multiple args), invalid name (@1test), non-primitive args error (@obj({})), position tracking, integration with tokenizer flow, verify annotation-before-key constraint"
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement multistring parser with indentation handling",
        "description": "Parse triple-quoted multistrings with indentation-sensitive content extraction and nested quote handling",
        "details": "Add to `src/bru_parser.zig`:\n- `parseMultistring() !Value` - handles ''' or \"\"\"\n- Track opening quote indentation level (column position)\n- Content must be indented 2 spaces beyond opening line\n- Trim leading whitespace up to indentation level on each line\n- Preserve whitespace beyond indentation level\n- Match closing quotes only at exact opening indentation\n- Nested ''' at deeper indentation = literal text, not closing marker\n- Trailing newline logic: included only if there's a blank line before closing quotes\n- State machine: OPENING → CONTENT → CLOSING\n- Suspend normal Bru parsing inside multistring body\nHandle both single and double triple-quote variants.",
        "testStrategy": "Unit tests: basic multistring, nested triple quotes, indentation variations, trailing newline cases, multistrings in arrays, error cases (wrong indentation)",
        "priority": "high",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define multistring state machine enum and tracking structures",
            "description": "Create the state machine types and indentation tracking structures needed for multistring parsing",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, add data structures for multistring parsing state management:\n- `MultistringState` enum with variants: OPENING, CONTENT, CLOSING\n- `MultistringContext` struct with fields: state (MultistringState), quote_type (u8 for ' or \"), opening_indent (usize), current_line_indent (usize), content_buffer (ArrayList(u8)), has_blank_line_before_close (bool)\n- Helper method `initContext(allocator: Allocator, quote_type: u8, opening_indent: usize) MultistringContext` to initialize parsing context\n- Helper method `shouldIncludeTrailingNewline() bool` that returns true only if has_blank_line_before_close is true\nThese structures will track the parser state as it transitions through OPENING → CONTENT → CLOSING phases and enforce indentation rules.",
            "status": "pending",
            "testStrategy": "Unit tests: initialize MultistringContext with different quote types and indentation levels, verify state transitions are tracked correctly, test trailing newline logic with and without blank lines"
          },
          {
            "id": 2,
            "title": "Implement opening quote detection and indentation capture",
            "description": "Parse triple-quote opening markers (''' or \"\"\") and record the column position for indentation validation",
            "dependencies": [
              1
            ],
            "details": "Add to `src/bru_parser.zig` in the `parseMultistring() !Value` function:\n- Detect TripleQuote token from tokenizer (either ''' or \"\"\")\n- Record quote_type (single or double) for matching closing quotes\n- Capture opening_indent as the column position of the opening token using TokenWithPos.column\n- Validate that the opening quote is followed by a newline (no content on same line)\n- Calculate expected_content_indent = opening_indent + 2 (content must be indented 2 spaces beyond opening)\n- Set state to CONTENT after successful opening\n- Return ParseError.InvalidMultistring if opening quote has content on same line\nThis establishes the indentation baseline for all subsequent content lines.",
            "status": "pending",
            "testStrategy": "Unit tests: parse valid ''' and \"\"\" openings, verify indentation column captured correctly, test error when content appears on opening line, verify expected_content_indent calculation"
          },
          {
            "id": 3,
            "title": "Implement content line processing with indentation trimming",
            "description": "Process multistring content lines by validating and trimming indentation while preserving relative whitespace",
            "dependencies": [
              2
            ],
            "details": "Add content processing logic to `parseMultistring()` in CONTENT state:\n- For each line, measure leading whitespace count using column tracking\n- Validate line_indent >= expected_content_indent (content must be indented at least 2 spaces beyond opening)\n- Trim exactly expected_content_indent characters from start of each line\n- Preserve all whitespace beyond the expected_content_indent threshold\n- Detect triple quotes at deeper indentation: if indent > opening_indent, treat ''' or \"\"\" as literal text, not closing marker\n- Track blank lines (lines with only whitespace) for trailing newline logic\n- Set has_blank_line_before_close = true if blank line detected before potential closing quote\n- Return ParseError.InvalidIndentation if line has insufficient indentation\nThis implements the core indentation-aware content extraction.",
            "status": "pending",
            "testStrategy": "Unit tests: content with exact expected indent, content with extra indent preserved, nested triple quotes at deep indent treated as literals, insufficient indent triggers error, blank line tracking for trailing newline, multiple content lines with varying indentation"
          },
          {
            "id": 4,
            "title": "Implement closing quote matching with indentation validation",
            "description": "Detect and validate closing triple quotes at the exact opening indentation level",
            "dependencies": [
              3
            ],
            "details": "Add closing detection logic to `parseMultistring()` when encountering triple quotes:\n- When TripleQuote token detected in CONTENT state, check if column == opening_indent\n- Verify quote_type matches opening quote type (''' closes ''', \"\"\" closes \"\"\")\n- If indentation matches: transition to CLOSING state and finalize content\n- If indentation doesn't match (column != opening_indent): treat as literal content text\n- Apply trailing newline rule: append final newline only if has_blank_line_before_close is true\n- Return completed multistring as Value.multistring with accumulated content_buffer\n- Return ParseError.UnclosedBlock if EOF reached without matching closing quotes\n- Return ParseError.InvalidMultistring if quote type mismatch\nThis ensures closing quotes are matched precisely at the opening indentation level.",
            "status": "pending",
            "testStrategy": "Unit tests: closing quotes at correct indentation, closing quotes at wrong indentation treated as content, quote type matching (''' vs \"\"\"), EOF without closing quote error, trailing newline included/excluded based on blank line, multistring with no content"
          },
          {
            "id": 5,
            "title": "Integrate parseMultistring into main parser and add comprehensive tests",
            "description": "Wire up parseMultistring to main parser value parsing and create full test suite covering all multistring edge cases",
            "dependencies": [
              4
            ],
            "details": "Complete multistring integration in `src/bru_parser.zig`:\n- In `parseValue()` function, add case for TripleQuote token that calls parseMultistring()\n- Handle both single (''') and double (\"\"\") triple-quote variants uniformly\n- Ensure normal Bru parsing is suspended inside multistring body (no block detection, no annotation processing)\n- Add memory management: use parser's allocator for content_buffer ArrayList\n- Create comprehensive test suite with fixtures:\n  * Basic multistring with simple content\n  * Nested triple quotes at deeper indentation (treated as literals)\n  * Various indentation levels (0, 2, 4 spaces opening indent)\n  * Trailing newline variations (with and without blank line before close)\n  * Multistrings inside arrays: ['''\n    content\n  ''']\n  * Error cases: wrong indentation, unmatched quotes, quote type mismatch, content on opening line\n  * Edge cases: empty multistring, single line, very long content\nEnsures parseMultistring is production-ready.",
            "status": "pending",
            "testStrategy": "Integration tests with complete .bru file fixtures, unit tests for each edge case listed in details, verify memory properly freed with testing.allocator, test error messages include correct line/column info, roundtrip test: parse then emit should preserve multistring content"
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement array parser",
        "description": "Parse Bru arrays including handling of multistrings as array elements",
        "details": "Add to `src/bru_parser.zig`:\n- `parseArray() !Value` - parses [...]\n- Elements are newline-separated, one per line\n- Each element can be any Value type: primitives, multistrings, nested arrays\n- No trailing commas\n- Multistrings in arrays: indentation relative to bracket\n- Use ArrayList to accumulate elements, then toOwnedSlice\n- Track bracket nesting level\n- Empty arrays allowed\n- Error on missing closing bracket\nNo nested multimaps allowed in arrays (not in spec).",
        "testStrategy": "Unit tests: primitive arrays, multistring arrays, nested arrays, empty arrays, indentation in array multistrings, missing bracket error",
        "priority": "medium",
        "dependencies": [
          4,
          6
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement array parsing state machine and token handling",
            "description": "Create the parseArray function skeleton with opening/closing bracket detection and basic state tracking",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, create `parseArray(allocator: std.mem.Allocator, tokenizer: *Tokenizer) !Value` function:\n- Expect and consume opening '[' token (LBracket)\n- Initialize bracket nesting level counter (starts at 1)\n- Track current position (line, column) from tokenizer for error reporting\n- Create state tracking: OPEN → PARSING_ELEMENTS → CLOSED\n- Peek at next token: if ']' immediately, return empty array Value.array(&[_]Value{})\n- Set up skeleton for element parsing loop (implemented in next subtask)\n- Consume closing ']' token (RBracket), decrement nesting level\n- Return ParseError.UnclosedBlock if EOF reached before closing bracket\n- Return Value.array with slice (population logic in next subtasks)\n- Position tracking: associate each element with line number for error messages",
            "status": "pending",
            "testStrategy": "Unit tests: detect opening bracket, detect closing bracket, empty array case [], error on missing closing bracket, verify nesting level tracking, test position tracking for error messages"
          },
          {
            "id": 2,
            "title": "Implement array element parsing loop with newline separation",
            "description": "Create the main parsing loop that processes newline-separated array elements",
            "dependencies": [
              1
            ],
            "details": "In `src/bru_parser.zig`, extend parseArray function:\n- Use ArrayList(Value) to accumulate elements dynamically\n- Main loop: while not at closing ']', parse elements\n- Element separation: expect Newline tokens between elements (one element per line)\n- Call parseValue() for each element (recursive - handles primitives, multistrings, nested arrays)\n- parseValue() dispatcher:\n  - Check token type: LBracket → recursive parseArray()\n  - TripleQuote → parseMultistring()\n  - String/Number/Bool/Null → primitive parsers from Task 4\n  - LBrace → ERROR (no nested multimaps in arrays per spec)\n- Append each parsed Value to ArrayList\n- Handle consecutive newlines (empty lines are skipped, not treated as null elements)\n- Error on unexpected tokens (comma, colon, etc.)\n- Continue until closing ']' encountered",
            "status": "pending",
            "testStrategy": "Unit tests: parse primitive element arrays [1, 2, 3], parse mixed type arrays [1, 'text', true], error on trailing commas, error on comma separators (not allowed), handle empty lines between elements, verify parseValue dispatcher works for all types, error on nested multimap attempt"
          },
          {
            "id": 3,
            "title": "Implement multistring indentation handling within arrays",
            "description": "Handle indentation-relative multistrings when they appear as array elements",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, enhance parseArray to handle multistring elements:\n- When parseValue() encounters TripleQuote token inside array, capture bracket indentation level\n- Multistring indentation rule in arrays: relative to the '[' bracket position\n- Calculate expected indentation: bracket column + 2 spaces\n- Pass indentation context to parseMultistring()\n- parseMultistring() validates each content line is indented correctly relative to bracket\n- Trim whitespace up to indentation level, preserve beyond it\n- Closing ''' must align with opening indentation (relative to bracket context)\n- Example:\n  ```\n  [\n    '''\n    content here\n    '''\n  ]\n  ```\n- Store parsed multistring Value.multistring in array ArrayList\n- Handle nested arrays containing multistrings recursively",
            "status": "pending",
            "testStrategy": "Unit tests: array with single multistring element, array with multiple multistring elements, mixed primitives and multistrings, nested array with multistrings, verify indentation relative to bracket, error on incorrect indentation, test closing quote alignment"
          },
          {
            "id": 4,
            "title": "Implement nested array parsing with bracket nesting level tracking",
            "description": "Support nested arrays with proper bracket balancing and recursive parsing",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, enhance parseArray for nested arrays:\n- When parseValue() encounters LBracket inside array, call parseArray() recursively\n- Track bracket nesting level throughout parsing:\n  - Increment on '[', decrement on ']'\n  - Each recursive call maintains its own nesting level\n- Nested arrays can contain any Value type including more nested arrays\n- Indentation handling for nested arrays: each level's elements are newline-separated\n- Example:\n  ```\n  [\n    [\n      1\n      2\n    ]\n    [\n      3\n      4\n    ]\n  ]\n  ```\n- Recursive parseArray() calls use same allocator (Arena)\n- Each nested array returns its own Value.array\n- Parent array accumulates nested arrays as elements\n- Error propagation: errors from nested parsing bubble up with position tracking\n- Maximum nesting depth check (prevent stack overflow): limit to reasonable depth like 32 levels",
            "status": "pending",
            "testStrategy": "Unit tests: single level nested array [[1, 2]], multi-level nested arrays [[[1]]], mixed nested and flat elements [1, [2, 3], 4], verify bracket balancing, test nesting level tracking, error on unclosed nested bracket, test maximum depth limit, verify position tracking through recursive calls"
          },
          {
            "id": 5,
            "title": "Finalize array parser with memory management and error handling",
            "description": "Complete parseArray with proper memory cleanup, owned slice conversion, and comprehensive error handling",
            "dependencies": [
              3,
              4
            ],
            "details": "In `src/bru_parser.zig`, complete parseArray implementation:\n- After parsing loop completes, convert ArrayList(Value) to owned slice using toOwnedSlice()\n- Return Value.array(owned_slice)\n- Memory management:\n  - Use provided ArenaAllocator from caller\n  - ArrayList automatically uses allocator for dynamic growth\n  - Owned slice memory managed by arena (no manual free needed)\n  - On error, arena cleanup handles deallocation\n- Comprehensive error handling:\n  - ParseError.UnclosedBlock: missing closing ']'\n  - ParseError.UnexpectedToken: commas, trailing commas, invalid tokens\n  - ParseError.InvalidIndentation: multistring indentation errors (bubbled from parseMultistring)\n  - Position tracking: include file_path, line, column in all errors via DiagnosticInfo\n- Edge cases:\n  - Empty arrays [] → valid, return empty slice\n  - Single element arrays [1] → valid\n  - Arrays with only multistrings → valid\n  - No trailing commas allowed → error\n- Integration: parseArray called from parseMultimap when '[' encountered, and from parseValue in nested contexts",
            "status": "pending",
            "testStrategy": "Integration tests: complete array parsing with all element types, memory leak detection with std.testing.allocator, error message formatting verification, test with arena allocator cleanup, edge case testing (empty, single element), test parseError variants, verify owned slice lifecycle, test integration with parseMultimap context, validate error positions match source locations"
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement multimap parser with duplicate key support",
        "description": "Parse Bru multimaps preserving duplicate keys and entry order",
        "details": "Add to `src/bru_parser.zig`:\n- `parseMultimap() !Value` - parses {...} or top-level implicit multimap\n- Key format: unquoted, no spaces, followed by colon\n- Empty value (key:) → empty string, not null\n- Preserve duplicate keys in order (use ArrayList of Entry)\n- Handle annotations on line before key\n- Handle tilde prefix (~key:) for disabled entries\n- Recursive: values can be nested multimaps/arrays\n- Track brace nesting level\n- Entries separated by newlines\n- Comments skipped but tracked for --keep-comments option\nTop-level document is an implicit multimap (no braces).",
        "testStrategy": "Unit tests: simple multimap, duplicate keys, nested multimaps, disabled entries, annotations, empty values, top-level implicit multimap",
        "priority": "high",
        "dependencies": [
          4,
          5,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Entry structure and braced multimap detection",
            "description": "Create Entry struct to hold key-value pairs with metadata and implement detection logic for braced vs implicit multimaps",
            "dependencies": [],
            "details": "In `src/bru_parser.zig`, implement:\n\n1. `Entry` struct (if not already in ir.zig, create parser-specific version):\n```zig\nconst Entry = struct {\n    annotations: []Annotation,\n    key: []const u8,\n    value: Value,\n    disabled: bool,\n    line: usize,\n};\n```\n\n2. Multimap detection helper:\n- `isBracedMultimap(tokenizer: *Tokenizer) bool` - peek for LBrace token\n- Returns true if next token is '{', false otherwise\n- Does not consume token, only peeks\n\n3. Initialize multimap entry ArrayList:\n- Use `std.ArrayList(Entry)` to preserve order and support duplicate keys\n- Allocate using provided ArenaAllocator\n\nThis establishes the data structure foundation for parseMultimap implementation.",
            "status": "pending",
            "testStrategy": "Unit tests: create Entry instances with various fields, test isBracedMultimap with '{' token (returns true), test with other tokens (returns false), verify peek doesn't consume tokens, test ArrayList initialization and append operations"
          },
          {
            "id": 2,
            "title": "Implement key parsing with colon validation and tilde prefix handling",
            "description": "Create function to parse multimap keys including unquoted format validation, colon requirement, and tilde prefix for disabled entries",
            "dependencies": [
              1
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`parseMultimapKey(tokenizer: *Tokenizer, allocator: std.mem.Allocator) !struct { key: []const u8, disabled: bool }`:\n\n1. Check for tilde prefix (~):\n   - Peek at current token\n   - If token is '~', consume it and set disabled = true\n   - Otherwise disabled = false\n\n2. Parse key identifier:\n   - Consume next token (must be unquoted identifier)\n   - Key format: no spaces, letters/digits/underscores/hyphens\n   - Extract lexeme as key string\n   - Allocate copy using allocator\n\n3. Validate colon:\n   - Consume next token\n   - Must be Colon token\n   - Return ParseError.MissingColon if not colon\n\n4. Return struct with key and disabled flag\n\nExamples:\n- 'name:' → { key: \"name\", disabled: false }\n- '~disabled-key:' → { key: \"disabled-key\", disabled: true }\n- 'no-colon' → ParseError.MissingColon",
            "status": "pending",
            "testStrategy": "Unit tests: parse regular keys (name:, my_key:, key-123:), parse tilde-prefixed keys (~disabled:), verify disabled flag correctly set, error on missing colon, error on keys with spaces, verify token consumption sequence, test with allocator for memory safety"
          },
          {
            "id": 3,
            "title": "Implement empty value detection and value parsing delegation",
            "description": "Create logic to detect empty values after colons and delegate to appropriate value parsers for nested structures",
            "dependencies": [
              2
            ],
            "details": "In `src/bru_parser.zig`, implement value parsing logic within parseMultimap:\n\n1. `isEmptyValue(tokenizer: *Tokenizer) bool`:\n   - After consuming colon, peek at next token\n   - If Newline or RBrace (end of entry), value is empty → return true\n   - Otherwise return false\n\n2. Value parsing delegation:\n   - If empty value → return Value.string(\"\") (empty string, NOT null per spec)\n   - Peek next token to determine value type:\n     - LBrace '{' → recursive call to parseMultimap() for nested multimap\n     - LBracket '[' → call parseArray() (dependency on Task 7)\n     - TripleQuote → call parseMultistring() (separate task)\n     - Other tokens → call parsePrimitive() (uses Task 4 parsers)\n\n3. Handle recursive nesting:\n   - Values can be nested multimaps: `outer: { inner: value }`\n   - Values can be arrays: `items: [1, 2, 3]`\n   - Properly track nesting level for brace matching\n\nThis enables flexible value types in multimap entries.",
            "status": "pending",
            "testStrategy": "Unit tests: detect empty values (key: followed by newline), detect non-empty values, test delegation to each value parser type (primitive, multimap, array, multistring), test recursive nested multimaps (2-3 levels deep), verify empty value returns empty string not null, test nesting level tracking"
          },
          {
            "id": 4,
            "title": "Implement annotation collection and entry assembly",
            "description": "Create logic to collect annotations on lines before keys and assemble complete Entry objects with all metadata",
            "dependencies": [
              3
            ],
            "details": "In `src/bru_parser.zig`, implement annotation handling within parseMultimap:\n\n1. Annotation collection before each entry:\n   - Initialize empty annotations ArrayList\n   - While next token is Annotation token:\n     - Call parseAnnotation() (from Task 5)\n     - Append result to annotations list\n     - Consume Newline after annotation\n   - Annotations must be on line immediately before key\n   - Multiple annotations allowed\n\n2. Entry assembly:\n   - After parsing key (with disabled flag) and value:\n   - Create Entry struct:\n     ```zig\n     const entry = Entry{\n         .annotations = annotations.toOwnedSlice(),\n         .key = parsed_key,\n         .value = parsed_value,\n         .disabled = is_disabled,\n         .line = tokenizer.current_line,\n     };\n     ```\n   - Append entry to multimap entries ArrayList\n   - Clear annotations for next entry\n\n3. Preserve duplicate keys:\n   - ArrayList preserves insertion order\n   - No deduplication - multiple entries with same key allowed\n   - Example: `name: Alice` then `name: Bob` → both stored in order\n\nThis completes entry construction with full metadata.",
            "status": "pending",
            "testStrategy": "Unit tests: parse entries with single annotation (@disabled), parse with multiple annotations, parse entries without annotations, verify annotation order preserved, test duplicate keys (same key multiple times), verify entry order matches source order, test disabled entries with tilde prefix, verify line number tracking, test combined annotations + disabled flag"
          },
          {
            "id": 5,
            "title": "Implement main parseMultimap function with brace matching and top-level implicit handling",
            "description": "Create complete parseMultimap function orchestrating all components with proper brace nesting, comment handling, and top-level document support",
            "dependencies": [
              4
            ],
            "details": "In `src/bru_parser.zig`, implement:\n\n`pub fn parseMultimap(tokenizer: *Tokenizer, allocator: std.mem.Allocator, is_top_level: bool) !Value`:\n\n1. Opening brace handling:\n   - If NOT is_top_level:\n     - Consume LBrace token (expect '{')\n     - Increment nesting_level\n   - If is_top_level:\n     - No opening brace (implicit multimap for document root)\n     - nesting_level starts at 0\n\n2. Main parsing loop:\n   - Initialize entries ArrayList\n   - Loop until closing condition:\n     - Skip Comment tokens (call parseComment from Task 9, discard unless --keep-comments)\n     - Skip Newline tokens (entry separator)\n     - If RBrace and nesting_level > 0 → break (end of braced multimap)\n     - If EOF and is_top_level → break (end of document)\n     - Otherwise parse entry (annotations + key + value)\n     - Append entry to ArrayList\n\n3. Closing brace handling:\n   - If NOT is_top_level:\n     - Consume RBrace token (expect '}')\n     - Decrement nesting_level\n     - Return ParseError.UnclosedBlock if RBrace missing\n   - If is_top_level:\n     - No closing brace expected\n     - Must end at EOF\n\n4. Return Value:\n   - Convert ArrayList to owned slice\n   - Return Value.multimap(entries.toOwnedSlice())\n\n5. Nesting level tracking:\n   - Track current brace depth for nested multimaps\n   - Ensure balanced braces\n   - Error on mismatched braces\n\nTop-level usage: `parseMultimap(tokenizer, allocator, true)` for document root",
            "status": "pending",
            "testStrategy": "Integration tests: parse simple braced multimap {key: value}, parse top-level implicit multimap (no braces), parse nested multimaps {outer: {inner: value}}, parse multimap with duplicate keys, parse with disabled entries (~key:), parse with annotations (@disabled key:), parse with comments (skip them), test empty multimap {}, test multimap with empty values (key:), error on unclosed braces, error on extra closing braces, test 3-level nesting, verify entry order preservation, test combined features (annotations + disabled + nested + duplicates)"
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement comment handler",
        "description": "Handle Bru comments with position validation and optional preservation for YAML output",
        "details": "Add to `src/bru_parser.zig`:\n- `parseComment() !?[]const u8` - returns comment text or null\n- # must be at start of line (preceded only by whitespace)\n- # after a value on same line = SYNTAX ERROR\n- Comment text: everything after # to end of line\n- Store comments in IR with line association when --keep-comments flag set\n- Comments not part of Value types, stored separately in parse context\n- Position validation: track if we've seen a value on current line\nComments are metadata, not part of the data model.",
        "testStrategy": "Unit tests: valid comments, inline # as syntax error, comment preservation mode, comment stripping mode, position tracking",
        "priority": "low",
        "dependencies": [
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Comment storage structure in parse context",
            "description": "Create a dedicated structure to store comments separately from the IR data model with line number associations",
            "dependencies": [],
            "details": "Add to `src/bru_parser.zig`:\n- `Comment` struct with fields: text ([]const u8), line (usize), column (usize)\n- `CommentStore` struct with ArrayList(Comment) for managing comment collection\n- Methods: init(allocator), deinit(), add(text, line, column)\n- Integration with parser context to hold CommentStore instance\n- Comments stored only when --keep-comments flag is enabled\n- Memory management: arena allocator for comment text, owned by parser\nComments are metadata, not part of Value types in the IR.",
            "status": "pending",
            "testStrategy": "Unit tests: create Comment instances, test CommentStore init/deinit lifecycle, verify add() method, test with std.testing.allocator for leak detection, verify line/column tracking accuracy"
          },
          {
            "id": 2,
            "title": "Implement parseComment function with basic extraction",
            "description": "Create the core comment parsing function that extracts comment text from lines starting with #",
            "dependencies": [
              1
            ],
            "details": "Add to `src/bru_parser.zig`:\n- `parseComment() !?[]const u8` - returns comment text or null\n- Detect # character at start of tokenization\n- Extract all text after # to end of line (excluding newline)\n- Trim leading whitespace after # (e.g., '# comment' → 'comment')\n- Return owned slice allocated from parser arena\n- Return null if current position is not a comment\n- Handle edge cases: empty comment (just #), # at EOF, multiple consecutive #\nBasic extraction without position validation yet.",
            "status": "pending",
            "testStrategy": "Unit tests: parse comment with text, parse empty comment (#), parse comment at EOF, parse # with leading whitespace in comment text, verify returned text excludes # and newline, verify allocation from parser arena, test null return when not at comment"
          },
          {
            "id": 3,
            "title": "Implement position validation for comment placement",
            "description": "Add validation logic to ensure # only appears at line start and detect inline # as syntax error",
            "dependencies": [
              2
            ],
            "details": "Add to `src/bru_parser.zig`:\n- Track parser state flag: seen_value_on_line (bool)\n- Set seen_value_on_line = true when parsing any value (string, number, multimap, etc.)\n- Reset seen_value_on_line = false on newline tokens\n- In parseComment: check if seen_value_on_line is true\n- If true: return error.UnexpectedComment with diagnostic info (# after value on same line)\n- # must be preceded only by whitespace (check column == 1 or only whitespace before)\n- Validate position before extracting comment text\nEnforces Bru comment rules strictly.",
            "status": "pending",
            "testStrategy": "Unit tests: valid comment at line start (column 1), valid comment after whitespace, error when # appears after value on same line (key: value # comment), error with inline comments in arrays, verify seen_value_on_line flag tracking, test with nested structures, verify error message includes line/column info"
          },
          {
            "id": 4,
            "title": "Integrate comment handling into main parser flow",
            "description": "Wire comment detection and storage into the main tokenization and parsing loop",
            "dependencies": [
              3
            ],
            "details": "Modify `src/bru_parser.zig`:\n- In main parse loop: check for # token before processing other tokens\n- Call parseComment() when # detected\n- Store result in CommentStore only if --keep-comments flag enabled\n- Associate comment with current line number from tokenizer\n- Skip comment storage if flag disabled (parse but discard)\n- Comments consumed during parsing do not affect IR structure\n- Ensure comments between entries are preserved with correct line associations\n- Handle comments before first entry, between entries, after last entry\nComments flow through parser but don't affect data model.",
            "status": "pending",
            "testStrategy": "Integration tests: parse file with comments throughout, verify comments stored when flag enabled, verify comments discarded when flag disabled, test comment positioning (before entries, between, after), verify IR structure unchanged by comments, test files with mixed comments and data, memory leak detection"
          },
          {
            "id": 5,
            "title": "Add command-line flag support and comment output integration",
            "description": "Implement --keep-comments CLI flag and prepare comment data for YAML emitter integration",
            "dependencies": [
              4
            ],
            "details": "Add to `src/main.zig`:\n- Add --keep-comments boolean flag to CLI argument parser\n- Pass flag to parser initialization\n- Store parsed comments in BruDocument metadata or separate structure\n- Create CommentMap or similar to associate comments with source lines\n- Add getComments() accessor to retrieve stored comments\n- Provide comment position hints for YAML emitter (which line, before which key)\n- Document flag in help text: '--keep-comments: Preserve comments in YAML output'\n- Default: false (strip comments)\nEnables optional comment preservation workflow.",
            "status": "pending",
            "testStrategy": "Integration tests: run with --keep-comments flag, run without flag (default), verify flag parsing, test comment retrieval from parsed document, verify comment-line associations, test with YAML emitter stub (verify comments accessible), test help text includes flag documentation"
          }
        ]
      },
      {
        "id": 10,
        "title": "Implement complete Bru parser with top-level orchestration",
        "description": "Integrate all parsing components into a complete Bru document parser",
        "details": "Complete `src/bru_parser.zig`:\n- `parse(allocator: Allocator, source: []const u8, file_path: []const u8) !BruDocument`\n- Initialize ArenaAllocator for document lifetime\n- Create tokenizer from source\n- Parse top-level implicit multimap\n- Collect and attach errors with file path and line numbers\n- Handle EOF correctly\n- Validate document structure: no orphaned annotations, balanced braces/brackets\n- Return structured parse errors, not generic failures\n- Include parse context: file path, line/column tracking\n- Option to preserve comments in IR for --keep-comments\nEntry point for all parsing operations.",
        "testStrategy": "Integration tests with complete .bru files from fixtures, error reporting verification, memory leak detection with testing.allocator",
        "priority": "high",
        "dependencies": [
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create bru_parser.zig file with parse function signature and document structure",
            "description": "Set up the main parser file with the public parse API, BruDocument struct, and basic scaffolding for the complete parsing pipeline",
            "dependencies": [],
            "details": "Create `src/bru_parser.zig` with:\n- Import statements: std, Allocator, ArenaAllocator\n- Define `BruDocument` struct: arena (ArenaAllocator), entries ([]Entry), file_path ([]const u8), comments (if --keep-comments flag)\n- Define `ParseError` struct with file_path, line, column, message fields\n- Define `parse(allocator: Allocator, source: []const u8, file_path: []const u8) !BruDocument` function signature\n- Add init/deinit methods for BruDocument using ArenaAllocator\n- Document lifecycle: caller owns source, parser owns all parsed data via arena\nThis establishes the entry point and data structures for complete Bru document parsing.",
            "status": "pending",
            "testStrategy": "Unit tests: verify BruDocument init creates arena allocator, test deinit cleanup with std.testing.allocator, verify file_path is stored correctly, test empty document creation"
          },
          {
            "id": 2,
            "title": "Implement tokenizer integration with CRLF normalization and EOF handling",
            "description": "Initialize the tokenizer from source input with line ending normalization and proper end-of-file detection",
            "dependencies": [
              1
            ],
            "details": "Add to `src/bru_parser.zig` parse function:\n- Create local mutable copy of source for CRLF→LF normalization (use arena allocator)\n- Initialize Tokenizer struct with normalized source: `Tokenizer.init(normalized_source)`\n- Store tokenizer state in parse context\n- Implement EOF detection: track when tokenizer returns null or Eof token\n- Verify tokenizer can peek without consuming for lookahead parsing\n- Handle tokenizer errors and wrap them in ParseError with file context\nEnsures clean, normalized token stream for parsing with proper boundary handling.",
            "status": "pending",
            "testStrategy": "Unit tests: verify CRLF normalization (\\r\\n → \\n), test with LF-only input (unchanged), verify EOF detection after last token, test peek vs next behavior, test with empty source, verify memory allocation uses arena"
          },
          {
            "id": 3,
            "title": "Implement top-level implicit multimap parsing orchestration",
            "description": "Call parseMultimap to parse the top-level document structure as an implicit (brace-less) multimap",
            "dependencies": [
              2
            ],
            "details": "Add to parse function after tokenizer initialization:\n- Call `parseMultimap(tokenizer, allocator, true)` where true indicates top-level implicit mode\n- Store returned entries in BruDocument.entries field\n- Handle parsing errors from parseMultimap and augment with file_path context\n- Verify parseMultimap consumes tokens until EOF\n- Support nested braced multimaps within top-level entries\n- Preserve entry order from source\n- Track annotations and disabled entries via parseMultimap\nThis orchestrates the main parsing work by delegating to the multimap parser.",
            "status": "pending",
            "testStrategy": "Integration tests: parse simple top-level multimap (meta, headers blocks), test with nested braced multimaps, verify entry order matches source, test with annotations, verify EOF is reached after parsing"
          },
          {
            "id": 4,
            "title": "Implement error collection with file path and line/column tracking",
            "description": "Create structured error reporting that captures parse failures with precise source location information",
            "dependencies": [
              3
            ],
            "details": "Add error handling throughout parse function:\n- Create `ParseErrorList` struct with ArrayList(ParseError)\n- Wrap all parser calls in error handling blocks\n- On error, create ParseError with: file_path, current line, current column, error message\n- Collect multiple errors when possible (don't fail on first error)\n- For fatal errors (memory allocation), return immediately\n- Include context in error messages: 'Expected colon after key at file.bru:5:12'\n- Attach error list to BruDocument or return as error union\n- Preserve line/column info from TokenWithPos\nProvides actionable error messages for debugging invalid Bru files.",
            "status": "pending",
            "testStrategy": "Unit tests: trigger parse error and verify file_path in error, verify line/column accuracy, test error message formatting, test multiple error collection, verify fatal errors abort immediately, integration test with intentionally broken .bru files"
          },
          {
            "id": 5,
            "title": "Implement document validation for orphaned annotations and balanced delimiters",
            "description": "Validate the parsed document structure for syntax correctness including annotation attachment and delimiter matching",
            "dependencies": [
              4
            ],
            "details": "Add validation pass after parseMultimap:\n- Verify all annotations are attached to entries (no orphaned @annotations)\n- Check all braces are balanced (each { has matching })\n- Check all brackets are balanced (each [ has matching ])\n- Validate no stray comment symbols after values on same line\n- Verify tilde prefix only appears on keys with colons\n- Return validation errors through ParseErrorList\n- Include option to preserve comments in IR via --keep-comments flag parsing\n- Store comments with line associations in separate CommentStore if flag enabled\n- Final return: populated BruDocument with all entries, or detailed ParseError\nEnsures document structure integrity before transformation phase.",
            "status": "pending",
            "testStrategy": "Unit tests: valid document passes, orphaned annotation triggers error, unbalanced braces/brackets trigger errors, inline comment after value triggers error, valid comments preserved when --keep-comments enabled, verify validation error messages include locations"
          }
        ]
      },
      {
        "id": 11,
        "title": "Create OpenCollection model types",
        "description": "Define type-safe Zig structs representing the OpenCollection YAML schema",
        "details": "Create `src/opencollection.zig`:\n- `OpenCollectionRequest` struct with: info, http, runtime, settings, docs\n- `Info` struct: name, type, seq, tags[]\n- `Http` struct: method, url, headers[], params[], body, auth\n- `Header` struct: name, value, enabled\n- `Param` struct: name, value, type (query/path), enabled\n- `Body` tagged union: Json, Xml, Text, FormUrlEncoded, MultipartForm, GraphQL with type and data fields\n- `Auth` tagged union: Bearer, Basic, OAuth2, AwsV4, Digest with type-specific fields\n- `Runtime` struct: scripts[], assertions[], vars[]\n- `Script` struct: type (before-request/after-response/tests), code\n- `Assertion` struct: expression, operator, value\n- `Var` struct: name, value, type (before-request/after-response)\nAll strings as []const u8, arrays as slices.",
        "testStrategy": "Unit tests for struct initialization, optional fields, tagged union handling, memory layout verification",
        "priority": "high",
        "dependencies": [
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define core OpenCollection structs (OpenCollectionRequest, Info, Settings, Docs)",
            "description": "Create src/opencollection.zig with the top-level OpenCollectionRequest struct and supporting Info, Settings, and Docs structs",
            "dependencies": [],
            "details": "Create src/opencollection.zig file with:\n- OpenCollectionRequest struct containing: info (Info), http (Http), runtime (?Runtime), settings (?Settings), docs (?Docs) - runtime/settings/docs are optional\n- Info struct: name ([]const u8), type ([]const u8), seq (?usize), tags (?[][]const u8) - seq and tags optional\n- Settings struct: placeholder for future settings fields (can be empty struct initially)\n- Docs struct: description (?[]const u8) and other documentation fields as optional\n- Use Zig's optional types (?) for nullable fields\n- All string fields as []const u8 (Zig byte slices)\n- Array fields as slices of appropriate types\n- Add pub before all structs to make them accessible from other modules\n- Follow Zig naming conventions: PascalCase for types, snake_case for fields",
            "status": "pending",
            "testStrategy": "Unit tests verifying: struct initialization with all fields, struct initialization with only required fields (optionals as null), field access, memory layout size using @sizeOf, default values for optional fields"
          },
          {
            "id": 2,
            "title": "Define HTTP request structs (Http, Header, Param)",
            "description": "Add Http struct with headers, query params, path params, and basic request configuration to opencollection.zig",
            "dependencies": [
              1
            ],
            "details": "Add to src/opencollection.zig:\n- Http struct containing: method ([]const u8), url ([]const u8), headers (?[]Header), params (?[]Param), body (?Body), auth (?Auth) - headers/params/body/auth optional\n- Header struct: name ([]const u8), value ([]const u8), enabled (bool) with enabled defaulting to true\n- Param struct: name ([]const u8), value ([]const u8), type (ParamType enum), enabled (bool)\n- ParamType enum: query, path (represents whether param is query string or path variable)\n- Method should support standard HTTP verbs: GET, POST, PUT, DELETE, PATCH, HEAD, OPTIONS\n- All optional array fields should be nullable slices\n- Ensure proper alignment for efficient memory layout\n- Consider adding helper methods if needed for common operations",
            "status": "pending",
            "testStrategy": "Unit tests for: Http struct with all HTTP methods, Header with enabled/disabled states, Param with query/path types, optional field handling (null headers/params), array slice initialization and iteration"
          },
          {
            "id": 3,
            "title": "Define Body tagged union with content type variants",
            "description": "Implement Body as a Zig tagged union supporting different content types (JSON, XML, Text, FormUrlEncoded, MultipartForm, GraphQL)",
            "dependencies": [
              1
            ],
            "details": "Add to src/opencollection.zig:\n- Body tagged union with variants:\n  - Json: { content_type: []const u8, data: []const u8 }\n  - Xml: { content_type: []const u8, data: []const u8 }\n  - Text: { content_type: []const u8, data: []const u8 }\n  - FormUrlEncoded: { fields: []FormField }\n  - MultipartForm: { parts: []MultipartPart }\n  - GraphQL: { query: []const u8, variables: ?[]const u8, operation_name: ?[]const u8 }\n- FormField struct: name ([]const u8), value ([]const u8)\n- MultipartPart struct: name ([]const u8), filename (?[]const u8), content_type (?[]const u8), data ([]const u8)\n- Use Zig tagged union syntax: 'const Body = union(enum) { ... }'\n- Each variant should be self-documenting with clear field names\n- Consider memory layout and padding for efficient storage",
            "status": "pending",
            "testStrategy": "Unit tests for: each Body variant initialization, tagged union pattern matching with switch statements, accessing variant-specific fields, FormField and MultipartPart struct creation, GraphQL with/without optional variables"
          },
          {
            "id": 4,
            "title": "Define Auth tagged union with authentication method variants",
            "description": "Implement Auth as a Zig tagged union supporting different authentication schemes (Bearer, Basic, OAuth2, AWS Signature V4, Digest)",
            "dependencies": [
              1
            ],
            "details": "Add to src/opencollection.zig:\n- Auth tagged union with variants:\n  - Bearer: { token: []const u8, prefix: ?[]const u8 } - prefix defaults to 'Bearer'\n  - Basic: { username: []const u8, password: []const u8 }\n  - OAuth2: { access_token: []const u8, token_type: ?[]const u8, refresh_token: ?[]const u8 }\n  - AwsV4: { access_key: []const u8, secret_key: []const u8, region: []const u8, service: []const u8, session_token: ?[]const u8 }\n  - Digest: { username: []const u8, password: []const u8, realm: ?[]const u8, nonce: ?[]const u8, algorithm: ?[]const u8 }\n- Use Zig tagged union syntax with enum for type discrimination\n- All sensitive fields (passwords, tokens, keys) should be []const u8\n- Optional fields use ? prefix (prefix, token_type, refresh_token, session_token, realm, nonce, algorithm)\n- Follow security best practices for sensitive data handling in memory",
            "status": "pending",
            "testStrategy": "Unit tests for: each Auth variant initialization, switch-based pattern matching on auth type, accessing variant-specific credentials, optional field handling in OAuth2/AwsV4/Digest, memory safety for sensitive data fields"
          },
          {
            "id": 5,
            "title": "Define Runtime structs (Runtime, Script, Assertion, Var)",
            "description": "Add Runtime configuration structs for pre/post request scripts, test assertions, and runtime variables to opencollection.zig",
            "dependencies": [
              1
            ],
            "details": "Add to src/opencollection.zig:\n- Runtime struct containing: scripts (?[]Script), assertions (?[]Assertion), vars (?[]Var) - all optional\n- Script struct: type (ScriptType enum), code ([]const u8), enabled (bool)\n- ScriptType enum: before_request, after_response, tests (represents when script executes)\n- Assertion struct: expression ([]const u8), operator (AssertionOperator enum), value ([]const u8), enabled (bool)\n- AssertionOperator enum: equals, not_equals, contains, not_contains, greater_than, less_than, matches_regex, exists, is_null\n- Var struct: name ([]const u8), value ([]const u8), type (VarType enum), enabled (bool)\n- VarType enum: before_request, after_response (represents variable scope/lifetime)\n- All boolean enabled fields should default to true for usability\n- Scripts should support JavaScript code as []const u8\n- Assertions should be flexible enough for various test conditions",
            "status": "pending",
            "testStrategy": "Unit tests for: Runtime with all optional fields, Script with each ScriptType, Assertion with each AssertionOperator, Var with each VarType, enabled/disabled states for scripts/assertions/vars, empty Runtime (all fields null)"
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement IR to OpenCollection transformer for meta block",
        "description": "Transform Bru meta block to OpenCollection info section",
        "details": "Create `src/transformer.zig`:\n- `transformMeta(meta: Value) !Info`\n- Extract name, type, seq from multimap\n- Parse tags array if present\n- Handle missing fields with defaults\n- Validate type field (must be 'http' or similar valid types)\n- seq should be parsed as integer\n- Preserve @description annotation if present\nMap to info.name, info.type, info.seq, info.tags.",
        "testStrategy": "Unit tests with various meta block configurations, missing fields, invalid types, tags array handling",
        "priority": "high",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformer.zig file with module structure and imports",
            "description": "Set up the transformer module with necessary imports and public API surface for transforming IR to OpenCollection types",
            "dependencies": [],
            "details": "Create `src/transformer.zig` file with:\n- Import std library\n- Import IR types from `src/ir.zig`: Value, Entry, Annotation, BruDocument\n- Import OpenCollection types from `src/opencollection.zig`: OpenCollectionRequest, Info, Http, etc.\n- Import std.mem.Allocator for memory management\n- Set up pub const for transformMeta function signature\n- Add module-level documentation comments explaining the transformer's role\n- Define error set: TransformError with variants like InvalidType, MissingRequiredField, InvalidSeqNumber\n- Use standard Zig module conventions with pub fn for public API",
            "status": "pending",
            "testStrategy": "Verify module compiles, imports resolve correctly, test with zig build to ensure no circular dependencies, verify error set is accessible"
          },
          {
            "id": 2,
            "title": "Implement transformMeta function signature and basic structure",
            "description": "Create the main transformMeta function that accepts a meta Value (multimap) and returns an Info struct with error handling",
            "dependencies": [
              1
            ],
            "details": "In `src/transformer.zig`, implement:\n```zig\npub fn transformMeta(allocator: std.mem.Allocator, meta: Value) !Info {\n    // Validate meta is a multimap\n    if (meta != .multimap) {\n        return error.InvalidMetaBlock;\n    }\n    const entries = meta.multimap;\n    \n    // Initialize result with defaults\n    var info = Info{\n        .name = \"\",\n        .type = \"http\",  // Default type\n        .seq = null,\n        .tags = null,\n    };\n    \n    // Parse entries (to be implemented in next subtasks)\n    \n    return info;\n}\n```\nFunction takes allocator for potential string duplication and meta Value. Returns Info or error. Validates input is multimap type using Zig tagged union pattern matching.",
            "status": "pending",
            "testStrategy": "Unit test: call with non-multimap Value (should error), call with empty multimap (should return defaults), verify Info struct initialization with default values"
          },
          {
            "id": 3,
            "title": "Extract and validate required meta fields (name, type)",
            "description": "Parse the meta multimap entries to extract name and type fields with validation for required fields and type values",
            "dependencies": [
              2
            ],
            "details": "Extend transformMeta in `src/transformer.zig`:\n- Iterate through meta.multimap entries\n- For each entry, match entry.key:\n  - \"name\": extract string value, validate non-empty, assign to info.name\n  - \"type\": extract string value, validate against allowed types (\"http\", \"graphql\", etc.), assign to info.type\n- Type validation: check value is one of: \"http\", \"graphql\", \"websocket\", \"grpc\"\n- Error if name is missing or empty string\n- Error if type is invalid (not in allowed list)\n- Handle entry.value being different Value variants (must be .string)\n- Skip disabled entries (entry.disabled == true)\n- Use allocator.dupe() to copy strings if needed for Info lifetime",
            "status": "pending",
            "testStrategy": "Unit tests: valid name and type, missing name (error), invalid type value (error), type validation with each allowed type, disabled name entry (should error), non-string value for name/type (error)"
          },
          {
            "id": 4,
            "title": "Parse optional meta fields (seq, tags) with type conversion",
            "description": "Extract and parse optional seq field as integer and tags field as string array from meta block entries",
            "dependencies": [
              3
            ],
            "details": "Extend transformMeta in `src/transformer.zig`:\n- Continue entry iteration to handle:\n  - \"seq\": parse as integer from Value\n    - If value is .number: parse string to usize using std.fmt.parseInt\n    - If value is .string: attempt parse as integer\n    - Error on parse failure or negative numbers\n    - Assign to info.seq (nullable usize)\n  - \"tags\": extract array of strings\n    - Validate value is .array\n    - Allocate [][]const u8 slice with allocator\n    - Iterate array, extract each string element\n    - Error if array contains non-string values\n    - Assign to info.tags (nullable string array)\n- Both fields are optional (null if not present)\n- Handle edge cases: empty tags array (valid), seq of 0 (valid)",
            "status": "pending",
            "testStrategy": "Unit tests: seq as number string, seq as integer value, seq parsing errors (negative, non-numeric), tags as string array, empty tags array, tags with non-string elements (error), both fields null (valid), seq=0 (valid)"
          },
          {
            "id": 5,
            "title": "Handle @description annotation and finalize Info struct",
            "description": "Check for @description annotation on meta block entries and preserve it in the Info struct, then validate final result",
            "dependencies": [
              4
            ],
            "details": "Complete transformMeta in `src/transformer.zig`:\n- For each entry, check entry.annotations array\n- Look for annotation.name == \"description\"\n- If found:\n  - Extract first arg from annotation.args (should be string Value)\n  - Store in Info struct (may need to extend Info with description field or handle via separate mechanism)\n  - Validate args length (should have exactly 1 argument)\n- If @description present on multiple fields, last one wins (or error on conflict)\n- Final validation before return:\n  - Ensure name is non-empty\n  - Ensure type is valid\n  - Verify allocated memory is managed correctly\n- Return complete Info struct\n- Add comprehensive error messages with context (which field, what went wrong)",
            "status": "pending",
            "testStrategy": "Unit tests: meta with @description annotation, @description with multiple args (error), @description with non-string arg (error), multiple @description annotations, complete meta block with all fields (name, type, seq, tags, @description), verify error messages include context"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement IR to OpenCollection transformer for HTTP method and URL",
        "description": "Transform Bru HTTP verb blocks (get, post, put, etc.) to OpenCollection http.method and http.url",
        "details": "Add to `src/transformer.zig`:\n- `transformHttpMethod(entries: []Entry) !struct { method: []const u8, url: []const u8 }`\n- Recognize method blocks: get, post, put, patch, delete, head, options\n- Extract URL from block value (usually a multimap with url: field or direct string)\n- Uppercase method name: get → GET\n- Handle both formats: `get { url: https://... }` and `get: https://...`\n- Error on multiple method blocks (contradictory)\n- Error on missing method block\nMap to http.method and http.url.",
        "testStrategy": "Unit tests for all HTTP verbs, URL extraction from different formats, error on multiple methods, error on missing method",
        "priority": "high",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add transformHttpMethod function signature and basic structure to transformer.zig",
            "description": "Create the transformHttpMethod function that accepts IR entries and returns method and URL with error handling for HTTP verb detection",
            "dependencies": [],
            "details": "In `src/transformer.zig`, add after transformMeta:\n```zig\npub fn transformHttpMethod(allocator: std.mem.Allocator, entries: []Entry) !struct { method: []const u8, url: []const u8 } {\n    var found_method: ?[]const u8 = null;\n    var found_url: ?[]const u8 = null;\n    \n    // Will iterate entries to find HTTP verb blocks\n    \n    if (found_method == null) {\n        return error.MissingHttpMethod;\n    }\n    \n    return .{\n        .method = found_method.?,\n        .url = found_url orelse \"\",\n    };\n}\n```\nFunction signature returns anonymous struct with method and url fields. Takes allocator for potential string operations and entries slice from BruDocument top-level multimap. Will be called by orchestration function with all top-level entries to extract HTTP method block.",
            "status": "pending",
            "testStrategy": "Unit test: call with empty entries (should error MissingHttpMethod), verify return type is struct with method and url fields, test error handling compilation"
          },
          {
            "id": 2,
            "title": "Implement HTTP verb block recognition for all supported methods",
            "description": "Iterate through entries to detect HTTP method blocks (get, post, put, patch, delete, head, options) and validate only one is present",
            "dependencies": [
              1
            ],
            "details": "Extend transformHttpMethod in `src/transformer.zig`:\n- Define constant array of valid HTTP verbs: const http_verbs = [_][]const u8{ \"get\", \"post\", \"put\", \"patch\", \"delete\", \"head\", \"options\" };\n- Iterate through entries parameter\n- For each entry, check if entry.key matches any verb in http_verbs (case-insensitive using std.ascii.eqlIgnoreCase)\n- Skip disabled entries (entry.disabled == true)\n- If match found:\n  - Check if found_method is already set → return error.MultipleHttpMethods\n  - Store matched verb in found_method\n  - Store entry.value for URL extraction in next subtask\n- After iteration, verify exactly one method was found\nEnsures single, unambiguous HTTP method per request.",
            "status": "pending",
            "testStrategy": "Unit tests: each HTTP verb individually (get, post, put, patch, delete, head, options), multiple method blocks (error), disabled method entry (skip it), case sensitivity (GET vs get), no method block (error)"
          },
          {
            "id": 3,
            "title": "Extract URL from method block value with format detection",
            "description": "Parse the method block value to extract URL supporting both multimap format with url field and direct string format",
            "dependencies": [
              2
            ],
            "details": "Extend transformHttpMethod in `src/transformer.zig`:\n- After finding method entry, examine entry.value (Value type)\n- Handle two formats:\n  1. Multimap: `get { url: https://example.com }`\n     - Verify value is .multimap variant\n     - Iterate multimap entries looking for key == \"url\"\n     - Extract url entry.value as string\n  2. Direct string: `get: https://example.com`\n     - Verify value is .string variant\n     - Use string directly as URL\n- Store extracted URL in found_url\n- Error if value is neither multimap nor string: return error.InvalidMethodBlockFormat\n- Error if multimap format but no url field found: return error.MissingUrlInMethodBlock\n- Handle empty URL as valid (some requests may not have URL set)\nSupports both Bru syntax variations for HTTP method blocks.",
            "status": "pending",
            "testStrategy": "Unit tests: multimap format with url field, direct string format, multimap without url field (error), invalid value type like number/array (error), empty string URL (valid), url field with non-string value (error)"
          },
          {
            "id": 4,
            "title": "Uppercase HTTP method name for OpenCollection format compliance",
            "description": "Convert the detected HTTP method from lowercase Bru format to uppercase OpenCollection format (get → GET)",
            "dependencies": [
              3
            ],
            "details": "Extend transformHttpMethod in `src/transformer.zig`:\n- After extracting method name from entry.key, uppercase it\n- Use std.ascii.upperString or manual conversion:\n  ```zig\n  var method_upper = try allocator.alloc(u8, found_method.len);\n  _ = std.ascii.upperString(method_upper, found_method);\n  ```\n- Store uppercased method for return value\n- OpenCollection spec requires uppercase methods: GET, POST, PUT, PATCH, DELETE, HEAD, OPTIONS\n- Allocator is used to create owned uppercase string\n- Original entry.key remains lowercase (from Bru)\nEnsures OpenCollection YAML has proper method casing.",
            "status": "pending",
            "testStrategy": "Unit tests: verify get becomes GET, post becomes POST, all verbs uppercased correctly, test memory allocation with testing.allocator, verify returned string is owned by caller"
          },
          {
            "id": 5,
            "title": "Add comprehensive error handling and edge case validation",
            "description": "Finalize transformHttpMethod with complete error handling, validation messages, and integration with transformer error types",
            "dependencies": [
              4
            ],
            "details": "Complete transformHttpMethod in `src/transformer.zig`:\n- Add to TransformError set: MissingHttpMethod, MultipleHttpMethods, InvalidMethodBlockFormat, MissingUrlInMethodBlock\n- Enhance error messages with context: which methods were found, what format was expected\n- Validate URL is non-empty string (warn if empty but don't error)\n- Handle annotation support: check for @disabled on method entry\n- Test all code paths: missing method, duplicate methods, both URL formats\n- Return final struct with uppercased method and extracted URL\n- Document function with comments explaining both supported formats\n- Add examples in comments showing get { url: ... } and get: ... formats\nCompletes the HTTP method and URL transformation with robust error handling.",
            "status": "pending",
            "testStrategy": "Unit tests: complete valid transformations for all verbs and both formats, all error conditions (missing method, multiple methods, invalid format, missing url), @disabled annotation handling, error message content verification, integration test with realistic Entry arrays from parsed Bru documents"
          }
        ]
      },
      {
        "id": 14,
        "title": "Implement IR to OpenCollection transformer for headers",
        "description": "Transform Bru headers block to OpenCollection http.headers array with disabled entry support",
        "details": "Add to `src/transformer.zig`:\n- `transformHeaders(headers: Value) ![]Header`\n- Parse headers multimap entries\n- Each entry → {name, value, enabled}\n- Handle @disabled annotation → enabled: false\n- Handle tilde prefix (~Header:) → enabled: false\n- Default enabled: true\n- Preserve header order\n- Handle duplicate header names (allowed in HTTP)\n- Empty values allowed\nMap to http.headers[].",
        "testStrategy": "Unit tests: basic headers, disabled headers (both annotation and tilde), duplicate headers, empty values, header order preservation",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformHeaders function signature and input validation",
            "description": "Set up the transformHeaders function in src/transformer.zig with proper signature, allocator handling, and input validation for the headers Value parameter",
            "dependencies": [],
            "details": "Add to src/transformer.zig:\n- Function signature: `pub fn transformHeaders(allocator: Allocator, headers: Value) ![]Header`\n- Validate input is Value.multimap type, return error.InvalidHeadersBlock if not\n- Handle null/empty headers gracefully: return empty slice &[_]Header{}\n- Set up ArrayList(Header) for building result array\n- Add error type TransformError with InvalidHeadersBlock variant\n- Import Header type from opencollection.zig\nThis establishes the foundation for header transformation with proper error handling.",
            "status": "pending",
            "testStrategy": "Unit tests: call with Value.multimap (should succeed), call with Value.string (should error InvalidHeadersBlock), call with empty multimap (should return empty slice), verify allocator usage"
          },
          {
            "id": 2,
            "title": "Implement basic header entry parsing from multimap",
            "description": "Parse each Entry in the headers multimap and extract header name and value fields into Header structs with default enabled state",
            "dependencies": [
              1
            ],
            "details": "In transformHeaders function:\n- Iterate through headers.multimap entries using for loop\n- For each Entry: extract key (header name) and value\n- Validate value is primitive type (string, number, bool converted to string, null → empty string)\n- Create Header struct: {name: entry.key, value: valueAsString(entry.value), enabled: true}\n- Append to ArrayList(Header)\n- Preserve iteration order (important for HTTP header order)\n- Helper function: valueAsString(v: Value) ![]const u8 to convert primitives to string\n- For Value.multimap or Value.array in header value: return error.InvalidHeaderValue\nBasic transformation without annotation handling.",
            "status": "pending",
            "testStrategy": "Unit tests: simple headers (Content-Type: application/json), numeric values converted to strings, boolean values to strings, null values to empty strings, multimap value triggers error, array value triggers error, verify order preservation"
          },
          {
            "id": 3,
            "title": "Handle @disabled annotation for header entries",
            "description": "Detect and process @disabled annotations on header entries to set enabled field to false",
            "dependencies": [
              2
            ],
            "details": "Extend header entry processing:\n- Check if Entry has annotations (entry.annotations array)\n- Search annotations for @disabled: loop through entry.annotations, check if annotation.name == \"disabled\"\n- If @disabled found: set Header.enabled = false\n- Otherwise: keep Header.enabled = true (default from subtask 2)\n- @disabled takes precedence over tilde prefix (will be handled in next subtask)\n- Validate @disabled has no arguments: if annotation.args.len > 0, log warning but still process\n- Multiple @disabled annotations on same entry: treat same as single @disabled\nAnnotation-based disabling mechanism.",
            "status": "pending",
            "testStrategy": "Unit tests: header with @disabled annotation (enabled=false), header without annotation (enabled=true), header with @disabled and arguments (warning logged, enabled=false), multiple @disabled on same header, mixed enabled/disabled headers in same block"
          },
          {
            "id": 4,
            "title": "Handle tilde prefix (~) for disabled headers",
            "description": "Detect tilde prefix in header names and set enabled to false, removing tilde from final name",
            "dependencies": [
              3
            ],
            "details": "Extend header name processing:\n- Before creating Header struct, check if entry.key starts with '~'\n- If tilde found: remove it from name (use entry.key[1..]), set enabled = false\n- If both @disabled annotation AND tilde prefix: @disabled takes precedence (already false), remove tilde from name\n- Preserve rest of header name exactly (case-sensitive, preserve special chars)\n- Empty name after tilde (~:) should error: error.EmptyHeaderName\n- Only leading tilde is special, tildes elsewhere in name are literal\nProvides alternative syntax for disabling headers.",
            "status": "pending",
            "testStrategy": "Unit tests: ~Content-Type header (name=Content-Type, enabled=false), header with both ~prefix and @disabled, empty name after tilde (error), tilde in middle of name (literal character), mixed regular and tilde-prefixed headers"
          },
          {
            "id": 5,
            "title": "Handle duplicate header names and empty values",
            "description": "Ensure duplicate header names are preserved as separate entries and empty header values are allowed per HTTP spec",
            "dependencies": [
              4
            ],
            "details": "Finalize header handling:\n- Do NOT deduplicate headers with same name - HTTP allows multiple headers with same name (e.g., multiple Set-Cookie)\n- Each Entry in multimap becomes separate Header in output array, even if names match\n- Empty values are valid: empty string \"\" is acceptable header value\n- Whitespace-only values: preserve as-is (don't trim)\n- Return final slice: return ArrayList.toOwnedSlice() to transfer ownership to caller\n- Memory: all strings should reference arena-allocated memory from original parse\n- Final validation: ensure all Header structs have non-null name and value fields\nCompletes header transformation with edge case handling.",
            "status": "pending",
            "testStrategy": "Unit tests: duplicate header names (verify both present in output), empty header value (value=\"\"), whitespace-only value preserved, order preservation with duplicates, realistic HTTP headers (Set-Cookie, Accept, etc.), memory ownership verification with testing.allocator"
          }
        ]
      },
      {
        "id": 15,
        "title": "Implement IR to OpenCollection transformer for query and path params",
        "description": "Transform Bru params:query and params:path blocks to OpenCollection http.params array",
        "details": "Add to `src/transformer.zig`:\n- `transformParams(query: ?Value, path: ?Value) ![]Param`\n- Parse params:query multimap → type: query\n- Parse params:path multimap → type: path\n- Each entry → {name, value, type, enabled}\n- Handle @disabled, @description, @enum annotations\n- Combine both param types into single array\n- Preserve order within each type\n- Validate enum values if @enum annotation present\nMap to http.params[] with type discriminator.",
        "testStrategy": "Unit tests: query params, path params, both types, annotations (@disabled, @enum, @description), validation",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformParams function signature and input validation",
            "description": "Set up the transformParams function in src/transformer.zig with proper signature, allocator handling, and input validation for optional query and path Value parameters",
            "dependencies": [],
            "details": "Add to `src/transformer.zig`:\n- Function signature: `pub fn transformParams(allocator: std.mem.Allocator, query: ?Value, path: ?Value) ![]Param`\n- Early return empty slice if both query and path are null\n- Validate query is Value.multimap if present, else error.InvalidQueryParamsBlock\n- Validate path is Value.multimap if present, else error.InvalidPathParamsBlock\n- Import necessary types: Value, Entry from ir.zig; Param, ParamType from opencollection.zig\n- Set up ArrayList(Param) for building result\n- Defer deinit and return toOwnedSlice() at end\nEstablish robust function foundation with proper type checking.",
            "status": "pending",
            "testStrategy": "Unit tests: call with both null (returns empty slice), call with query only, call with path only, call with both, call with non-multimap Value for query/path (should error), verify allocator usage and memory cleanup"
          },
          {
            "id": 2,
            "title": "Implement query params parsing and conversion to Param structs",
            "description": "Parse params:query multimap entries and convert each to Param with type=query, handling enabled/disabled state",
            "dependencies": [
              1
            ],
            "details": "Add parsing logic in transformParams for query parameters:\n- Iterate over query.multimap entries\n- For each entry: extract name from entry.key, value from entry.value\n- Convert value to string (handle Value.string, Value.number as string, error on complex types)\n- Create Param: { name: entry.key, value: value_str, type: .query, enabled: !entry.disabled }\n- Check entry.disabled flag OR tilde prefix (~) in key → enabled: false\n- Default enabled: true if neither disabled flag nor tilde present\n- Append to params ArrayList\n- Preserve order of entries as they appear in multimap\nHandles basic query param transformation.",
            "status": "pending",
            "testStrategy": "Unit tests: parse simple query params, parse with Value.number values, handle disabled flag, handle tilde prefix, verify enabled defaults to true, verify order preservation, error on non-primitive values (multimap/array)"
          },
          {
            "id": 3,
            "title": "Implement path params parsing and conversion to Param structs",
            "description": "Parse params:path multimap entries and convert each to Param with type=path, handling enabled/disabled state",
            "dependencies": [
              1
            ],
            "details": "Add parsing logic in transformParams for path parameters (similar to query):\n- Iterate over path.multimap entries after query params processed\n- For each entry: extract name from entry.key, value from entry.value\n- Convert value to string (handle Value.string, Value.number as string, error on complex types)\n- Create Param: { name: entry.key, value: value_str, type: .path, enabled: !entry.disabled }\n- Check entry.disabled flag OR tilde prefix (~) → enabled: false\n- Default enabled: true\n- Append to params ArrayList\n- Preserve order within path params, append after all query params\nHandles path param transformation with type discriminator.",
            "status": "pending",
            "testStrategy": "Unit tests: parse simple path params, parse with Value.number values, handle disabled flag, handle tilde prefix, verify type=.path set correctly, verify order preservation, test combining query and path params (query first, then path)"
          },
          {
            "id": 4,
            "title": "Implement annotation handling for params (@disabled, @description, @enum)",
            "description": "Parse and process @disabled, @description, and @enum annotations on param entries, with enum validation",
            "dependencies": [
              2,
              3
            ],
            "details": "Enhance param parsing to handle annotations:\n- For each entry, check entry.annotations array\n- @disabled: set enabled = false (redundant with entry.disabled flag, but support both)\n- @description: currently store annotation but not in Param struct (for future extension or metadata)\n- @enum: extract enum allowed values from annotation args (array of strings)\n  - Validate param value is in enum list\n  - If validation fails: return error.InvalidEnumValue with diagnostic message\n- Annotation processing happens before creating Param struct\n- Multiple annotations can exist on single entry\n- Unknown annotations: ignore with warning (log but don't fail)\nAdds validation and metadata handling.",
            "status": "pending",
            "testStrategy": "Unit tests: @disabled annotation sets enabled=false, @description annotation preserved (if stored), @enum validation passes with valid value, @enum validation fails with invalid value (error), multiple annotations on single param, unknown annotations ignored, no annotations (baseline case)"
          },
          {
            "id": 5,
            "title": "Finalize transformParams with error handling and integration",
            "description": "Complete transformParams with comprehensive error handling, edge cases, and proper integration with transformer error types",
            "dependencies": [
              4
            ],
            "details": "Finalize transformParams implementation:\n- Add comprehensive error handling: InvalidQueryParamsBlock, InvalidPathParamsBlock, InvalidEnumValue, InvalidParamValue\n- Handle edge cases: empty query/path multimaps (valid, return empty for that type), duplicate param names (allowed, don't deduplicate)\n- Handle special characters in param values (URL encoding preservation)\n- Add helper function valueToString(value: Value) ![]const u8 for consistent value conversion\n- Document function with doc comments explaining parameters, return value, errors\n- Verify memory safety: all allocations use provided allocator, cleanup on error paths\n- Integration: ensure Param slice matches opencollection.zig struct definition\nProduction-ready with full error coverage.",
            "status": "pending",
            "testStrategy": "Unit tests: empty multimaps, duplicate param names, special characters in values, memory leak detection with testing.allocator, error path cleanup, integration test with complete params:query and params:path blocks from IR, verify output matches opencollection.zig Param array format"
          }
        ]
      },
      {
        "id": 16,
        "title": "Implement IR to OpenCollection transformer for request body",
        "description": "Transform Bru body blocks (json, xml, text, form-urlencoded, multipart-form, graphql) to OpenCollection http.body",
        "details": "Add to `src/transformer.zig`:\n- `transformBody(body: Value) !?Body`\n- Recognize body types from block name: body:json, body:xml, body:text, etc.\n- body:json → Body.Json with multistring content\n- body:xml → Body.Xml with multistring content\n- body:text → Body.Text with multistring content\n- body:form-urlencoded → Body.FormUrlEncoded with array of {name, value}\n- body:multipart-form → Body.MultipartForm handling @file() and @content-type() annotations\n- body:graphql → Body.GraphQL with query and variables fields\n- Extract data from nested multimap structure\n- Handle missing body (return null)\n- Error on multiple body blocks\nMap to http.body with appropriate type.",
        "testStrategy": "Unit tests for each body type, multistring content, form data, multipart with file annotations, graphql structure, error on multiple bodies",
        "priority": "high",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement transformBody function signature and body block extraction",
            "description": "Create the transformBody function that extracts body-related blocks from the IR multimap and validates there's at most one body block",
            "dependencies": [],
            "details": "In src/transformer.zig, implement `transformBody(allocator: Allocator, doc: BruDocument) !?Body` function. Extract all blocks matching pattern 'body:*' from the top-level multimap. Validate that exactly zero or one body block exists - return error if multiple body blocks found. Return null if no body block present. Extract the body type from block name (e.g., 'body:json' → 'json'). This function serves as the entry point for all body transformation logic.",
            "status": "pending",
            "testStrategy": "Unit tests: no body block returns null, single body:json block succeeds, multiple body blocks returns error, verify block name parsing (body:json, body:xml, etc.), test with invalid block names"
          },
          {
            "id": 2,
            "title": "Implement simple body type transformations (json, xml, text)",
            "description": "Transform body:json, body:xml, and body:text blocks to their corresponding Body union variants with multistring content extraction",
            "dependencies": [
              1
            ],
            "details": "Add transformation logic for the three simple body types that contain multistring content. For body:json → Body.Json, body:xml → Body.Xml, body:text → Body.Text. Extract the multistring content from the block value (which should be a Value.Multistring or Value.String). Handle both triple-quoted multistrings and simple strings. Store the content as a string in the respective Body variant. Error if the value type is not string/multistring.",
            "status": "pending",
            "testStrategy": "Unit tests: body:json with multistring content, body:xml with triple-quoted string, body:text with simple string, error on non-string values (numbers, arrays, multimaps), verify content preservation including whitespace and newlines"
          },
          {
            "id": 3,
            "title": "Implement form-urlencoded body transformation",
            "description": "Transform body:form-urlencoded blocks to Body.FormUrlEncoded with array of name-value pairs",
            "dependencies": [
              1
            ],
            "details": "Add transformation for body:form-urlencoded → Body.FormUrlEncoded. Parse the body block value as a multimap where each entry represents a form field. Extract entries as {name: string, value: string} pairs. Handle @disabled annotations on form fields by including/excluding them. Convert multimap entries to FormField array (struct with name and value fields). Preserve entry order from source. Error if value is not a multimap structure.",
            "status": "pending",
            "testStrategy": "Unit tests: simple form with 2-3 fields, form with @disabled fields, empty form (empty array), form field with empty value, form with special characters in names/values, error on non-multimap body value"
          },
          {
            "id": 4,
            "title": "Implement multipart-form body transformation with file annotations",
            "description": "Transform body:multipart-form blocks to Body.MultipartForm handling @file() and @content-type() annotations",
            "dependencies": [
              1
            ],
            "details": "Add transformation for body:multipart-form → Body.MultipartForm. Parse body block value as multimap. For each entry, check for @file() annotation to determine if it's a file part or text part. Extract @content-type() annotation if present. Create MultipartPart structs with: name (from key), value (from entry value or file path from @file()), contentType (from @content-type() or null), isFile (boolean based on @file() presence). Handle @disabled annotations. Store as array of parts preserving order.",
            "status": "pending",
            "testStrategy": "Unit tests: text parts only, file parts with @file(path), parts with @content-type(), mixed text and file parts, @disabled parts excluded, file part with both @file and @content-type, error on invalid annotation arguments"
          },
          {
            "id": 5,
            "title": "Implement GraphQL body transformation with query and variables",
            "description": "Transform body:graphql blocks to Body.GraphQL with query string and optional variables object",
            "dependencies": [
              1
            ],
            "details": "Add transformation for body:graphql → Body.GraphQL. Parse body block value as multimap with 'query' and optional 'variables' fields. Extract 'query' field as multistring (GraphQL query text). Extract 'variables' field as multimap/object (JSON variables for query). Convert variables multimap to appropriate structure for OpenCollection (likely JSON string or structured object). Query is required, variables is optional (null if absent). Error if query field missing or non-string, error if variables is not multimap/object structure.",
            "status": "pending",
            "testStrategy": "Unit tests: query only (no variables), query with variables object, query with empty variables, multistring query with newlines, error on missing query field, error on non-string query, error on invalid variables type, verify variables JSON structure"
          }
        ]
      },
      {
        "id": 17,
        "title": "Implement IR to OpenCollection transformer for authentication",
        "description": "Transform Bru auth blocks (bearer, basic, oauth2, awsv4, digest) to OpenCollection http.auth",
        "details": "Add to `src/transformer.zig`:\n- `transformAuth(auth: Value) !?Auth`\n- Recognize auth types: auth:bearer, auth:basic, auth:oauth2, auth:awsv4, auth:digest\n- auth:bearer → Auth.Bearer with token field\n- auth:basic → Auth.Basic with username, password\n- auth:oauth2 → Auth.OAuth2 with grant type, client_id, client_secret, scope, etc.\n- auth:awsv4 → Auth.AwsV4 with accessKeyId, secretAccessKey, region, service\n- auth:digest → Auth.Digest with username, password\n- Extract fields from nested multimap\n- Handle missing auth (return null)\n- Error on multiple auth blocks\nMap to http.auth with appropriate type.",
        "testStrategy": "Unit tests for each auth type, field extraction, OAuth2 grant types, AWS Sig V4 fields, error on multiple auth blocks",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformAuth function signature and auth block extraction",
            "description": "Set up the transformAuth function in src/transformer.zig with proper signature and logic to extract auth blocks from the IR multimap, validating at most one auth block exists",
            "dependencies": [],
            "details": "In src/transformer.zig, implement `pub fn transformAuth(allocator: std.mem.Allocator, doc: BruDocument) !?Auth` function. Extract all blocks matching pattern 'auth:*' from the top-level multimap (auth:bearer, auth:basic, auth:oauth2, auth:awsv4, auth:digest). Validate that exactly zero or one auth block exists - return error.MultipleAuthBlocks if multiple found. Return null if no auth block present. Extract the auth type from block name (e.g., 'auth:bearer' → 'bearer'). Import necessary types: Value, Entry from ir.zig; Auth from opencollection.zig. This function serves as the entry point for all authentication transformation logic.",
            "status": "pending",
            "testStrategy": "Unit tests: no auth block returns null, single auth:bearer block succeeds, multiple auth blocks returns error.MultipleAuthBlocks, verify block name parsing (auth:bearer, auth:basic, auth:oauth2, auth:awsv4, auth:digest), test with invalid block names, test case-sensitivity"
          },
          {
            "id": 2,
            "title": "Implement Bearer and Basic authentication transformations",
            "description": "Transform auth:bearer and auth:basic blocks to their corresponding Auth union variants with field extraction from nested multimaps",
            "dependencies": [
              1
            ],
            "details": "Add transformation logic for Bearer and Basic auth types:\n\n1. auth:bearer → Auth.Bearer:\n   - Parse block value as Value.multimap\n   - Extract 'token' field from multimap entries (required)\n   - Extract optional 'prefix' field (defaults to 'Bearer' if missing)\n   - Return error.MissingRequiredField if token not found\n   - Return Auth{ .Bearer = .{ .token = token_str, .prefix = prefix_str } }\n\n2. auth:basic → Auth.Basic:\n   - Parse block value as Value.multimap\n   - Extract 'username' field (required)\n   - Extract 'password' field (required)\n   - Return error.MissingRequiredField if either missing\n   - Return Auth{ .Basic = .{ .username = username_str, .password = password_str } }\n\nBoth should validate the block value is a multimap and convert field values from Value to string slices.",
            "status": "pending",
            "testStrategy": "Unit tests: auth:bearer with token only, auth:bearer with token and custom prefix, auth:bearer missing token (error), auth:basic with username and password, auth:basic missing username (error), auth:basic missing password (error), verify string field extraction from Value types (String, Number as string), error on non-multimap auth blocks"
          },
          {
            "id": 3,
            "title": "Implement OAuth2 authentication transformation with grant type handling",
            "description": "Transform auth:oauth2 blocks to Auth.OAuth2 variant, extracting grant type, client credentials, scopes, and token fields",
            "dependencies": [
              1
            ],
            "details": "Add transformation for auth:oauth2 → Auth.OAuth2:\n- Parse block value as Value.multimap\n- Extract required fields: 'grant_type' (authorization_code, client_credentials, password, refresh_token), 'client_id'\n- Extract 'client_secret' (required for most grant types)\n- Extract optional fields: 'scope', 'access_token', 'token_type', 'refresh_token', 'redirect_uri', 'username', 'password' (for password grant)\n- Note: The opencollection.zig Auth.OAuth2 structure from task 11 has access_token, token_type, refresh_token fields. Adapt transformation to match actual structure, mapping grant_type/client_id/client_secret appropriately\n- Handle grant type validation: ensure grant_type is one of the valid OAuth2 grant types\n- Return error.MissingRequiredField for missing client_id\n- Convert all fields to string slices, handling Value.String and Value.Number\n- Return Auth{ .OAuth2 = .{ .access_token = ..., .token_type = ..., .refresh_token = ... } }",
            "status": "pending",
            "testStrategy": "Unit tests: OAuth2 with access_token only, OAuth2 with all fields populated, OAuth2 with optional fields as null, OAuth2 missing access_token (error if required), verify optional field handling (token_type, refresh_token), test with different grant types if stored, string conversion from various Value types"
          },
          {
            "id": 4,
            "title": "Implement AWS Signature V4 authentication transformation",
            "description": "Transform auth:awsv4 blocks to Auth.AwsV4 variant, extracting AWS credentials, region, service, and optional session token",
            "dependencies": [
              1
            ],
            "details": "Add transformation for auth:awsv4 → Auth.AwsV4:\n- Parse block value as Value.multimap\n- Extract required fields: 'accessKeyId' or 'access_key_id' (handle both naming conventions), 'secretAccessKey' or 'secret_access_key', 'region', 'service'\n- Extract optional field: 'sessionToken' or 'session_token'\n- Map to Auth.AwsV4 structure: { access_key, secret_key, region, service, session_token }\n- Return error.MissingRequiredField if any required field missing\n- Validate region format (basic validation: non-empty string, matches pattern like us-east-1)\n- Validate service format (non-empty string, common services: s3, ec2, lambda, etc.)\n- Handle both camelCase (from Bru) and snake_case field names for compatibility\n- Convert all fields to string slices from Value types\n- Return Auth{ .AwsV4 = .{ .access_key = ..., .secret_key = ..., .region = ..., .service = ..., .session_token = ... } }",
            "status": "pending",
            "testStrategy": "Unit tests: AWS v4 with all required fields (accessKeyId, secretAccessKey, region, service), AWS v4 with optional sessionToken, AWS v4 missing accessKeyId (error), AWS v4 missing region (error), test both camelCase and snake_case field naming, validate region format (us-east-1, eu-west-2), validate service names (s3, ec2, dynamodb), sessionToken as null when not provided"
          },
          {
            "id": 5,
            "title": "Implement Digest authentication transformation and finalize transformAuth",
            "description": "Transform auth:digest blocks to Auth.Digest variant and complete transformAuth with comprehensive error handling and integration",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Complete transformAuth implementation:\n\n1. Add auth:digest → Auth.Digest transformation:\n   - Parse block value as Value.multimap\n   - Extract required fields: 'username', 'password'\n   - Extract optional fields: 'realm', 'nonce', 'algorithm' (MD5, MD5-sess, SHA-256, etc.)\n   - Return error.MissingRequiredField if username or password missing\n   - Return Auth{ .Digest = .{ .username = ..., .password = ..., .realm = ..., .nonce = ..., .algorithm = ... } }\n\n2. Add central dispatch logic in transformAuth:\n   - Switch on auth type string (bearer/basic/oauth2/awsv4/digest)\n   - Call appropriate transform function for each type\n   - Return error.UnsupportedAuthType for unknown auth types\n\n3. Add helper function valueToString(value: Value) ![]const u8 for consistent Value to string conversion across all auth types\n\n4. Handle @disabled annotation: check if auth block entry has @disabled annotation, if so return null instead of Auth\n\n5. Add comprehensive error handling and documentation\n\n6. Verify memory safety: all string references point to arena-allocated memory from original parse",
            "status": "pending",
            "testStrategy": "Unit tests: Digest with username and password only, Digest with all optional fields (realm, nonce, algorithm), Digest missing password (error), test algorithm values (MD5, SHA-256), test @disabled annotation returns null, integration test with all 5 auth types, unknown auth type error, valueToString helper with String/Number/other types, memory verification with testing.allocator, verify null return for missing/disabled auth"
          }
        ]
      },
      {
        "id": 18,
        "title": "Implement IR to OpenCollection transformer for scripts and tests",
        "description": "Transform Bru script blocks (pre-request, post-response, tests) to OpenCollection runtime.scripts array",
        "details": "Add to `src/transformer.zig`:\n- `transformScripts(scripts: []Entry) ![]Script`\n- script:pre-request → Script with type: before-request\n- script:post-response → Script with type: after-response\n- tests → Script with type: tests\n- Extract multistring code content\n- Preserve code exactly (no escaping needed, will use YAML block scalar)\n- Multiple script blocks of same type → multiple Script entries\n- Handle missing scripts (return empty array)\nMap to runtime.scripts[].",
        "testStrategy": "Unit tests for each script type, multistring code extraction, multiple scripts, empty scripts array",
        "priority": "medium",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define transformScripts function signature and error handling",
            "description": "Create the main transformScripts function in transformer.zig that accepts script entries from IR and returns an OpenCollection Script array",
            "dependencies": [],
            "details": "Add to `src/transformer.zig`:\n- `pub fn transformScripts(allocator: std.mem.Allocator, scripts: []Entry) ![]Script`\n- Function accepts allocator for memory management and scripts slice from IR\n- Returns owned slice of Script structs or TransformError\n- Add error variants to TransformError: InvalidScriptType, InvalidScriptContent, MissingScriptCode\n- Initialize ArrayList(Script) for collecting transformed scripts\n- Set up iteration over input Entry array\n- Return owned slice from ArrayList using toOwnedSlice()\n- Handle empty input array (return empty slice, not error)",
            "status": "pending",
            "testStrategy": "Unit tests: call with empty scripts array (should return empty slice), verify function signature compiles, test error set composition, verify ArrayList initialization with std.testing.allocator for leak detection"
          },
          {
            "id": 2,
            "title": "Implement script type mapping from Bru to OpenCollection",
            "description": "Map Bru script block names (script:pre-request, script:post-response, tests) to OpenCollection ScriptType enum values",
            "dependencies": [
              1
            ],
            "details": "Add logic to transformScripts in `src/transformer.zig`:\n- Check entry.key for script type identification\n- \"script:pre-request\" → ScriptType.before_request\n- \"script:post-response\" → ScriptType.after_response  \n- \"tests\" → ScriptType.tests\n- Use string comparison with std.mem.eql(u8, entry.key, \"script:pre-request\")\n- Return InvalidScriptType error if key doesn't match known script types\n- Store detected script type in local variable for Script struct initialization\n- Handle case-sensitive matching (Bru format is case-sensitive)\n- Skip entries that don't match script patterns (they belong to other transformers)",
            "status": "pending",
            "testStrategy": "Unit tests: test each script type mapping (pre-request, post-response, tests), test unknown script type returns error, test case sensitivity, verify ScriptType enum values are correctly assigned"
          },
          {
            "id": 3,
            "title": "Extract multistring code content from script entries",
            "description": "Parse the Entry value to extract JavaScript/test code from multistring blocks, preserving code exactly as written",
            "dependencies": [
              2
            ],
            "details": "Add to transformScripts in `src/transformer.zig`:\n- Validate entry.value is Value.multistring type using tagged union pattern matching\n- Extract code string: `const code = entry.value.multistring;`\n- Return MissingScriptCode error if multistring is empty or null\n- Do NOT modify code content (no escaping, no trimming beyond what parser already did)\n- Code will be emitted as YAML block scalar (|) by emitter, so preserve exactly\n- Handle multistring indentation already normalized by parser\n- Store code as []const u8 slice pointing to arena-allocated memory\n- No need to duplicate string if already in arena allocator",
            "status": "pending",
            "testStrategy": "Unit tests: extract code from multistring Value, test with JavaScript code containing special characters, test with empty code (should error), verify code content preservation (compare input vs output byte-for-byte), test multiline code blocks"
          },
          {
            "id": 4,
            "title": "Handle @disabled annotations and create Script structs",
            "description": "Check for @disabled annotation on script entries and construct Script structs with appropriate enabled field values",
            "dependencies": [
              3
            ],
            "details": "Add to transformScripts in `src/transformer.zig`:\n- Check entry.disabled field (boolean set by IR parser based on @disabled annotation)\n- Create Script struct: Script{ .type = script_type, .code = code, .enabled = !entry.disabled }\n- If entry.disabled is true, set enabled to false in Script struct\n- If entry.disabled is false, set enabled to true (default state)\n- Append Script struct to ArrayList using try scripts_list.append(script)\n- Handle memory allocation errors from ArrayList.append\n- Preserve entry order from IR (scripts execute in order defined)\n- Store all annotation metadata if needed for future use",
            "status": "pending",
            "testStrategy": "Unit tests: create Script with enabled=true (no @disabled), create Script with enabled=false (@disabled present), verify struct field values, test ArrayList append operations, verify script ordering preserved"
          },
          {
            "id": 5,
            "title": "Handle multiple scripts of same type and return final array",
            "description": "Support multiple script blocks of the same type (e.g., multiple pre-request scripts) and return the complete transformed Script array",
            "dependencies": [
              4
            ],
            "details": "Complete transformScripts in `src/transformer.zig`:\n- Continue iterating through all entries in scripts array\n- Allow multiple entries with same script type (Bru multimap allows duplicates)\n- Each duplicate creates separate Script entry in output array\n- Example: two \"script:pre-request\" blocks → two Script entries with type=before_request\n- After processing all entries, convert ArrayList to owned slice: `return scripts_list.toOwnedSlice();`\n- Handle case where no valid script entries found (return empty slice []Script{})\n- Add comprehensive function documentation explaining behavior with duplicates\n- Ensure proper error propagation using Zig error unions (try keyword)\n- Memory owned by caller after return (arena allocator pattern)",
            "status": "pending",
            "testStrategy": "Integration tests: transform single script, transform multiple different script types, transform duplicate script types (verify multiple Script entries created), transform with mix of enabled/disabled scripts, test with empty input, verify memory ownership and no leaks with std.testing.allocator, test complete .bru file with all script types"
          }
        ]
      },
      {
        "id": 19,
        "title": "Implement IR to OpenCollection transformer for assertions",
        "description": "Transform Bru assert block to OpenCollection runtime.assertions array",
        "details": "Add to `src/transformer.zig`:\n- `transformAssertions(assert: Value) ![]Assertion`\n- Parse assert multimap entries\n- Each entry → {expression, operator, value}\n- Expression from key, operator and value from entry value parsing\n- Common operators: eq, ne, gt, lt, contains, matches\n- Handle @disabled annotations\n- Preserve assertion order\n- Handle missing assert block (return empty array)\nMap to runtime.assertions[].",
        "testStrategy": "Unit tests for various assertion operators, expression parsing, disabled assertions, missing assertions",
        "priority": "low",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformAssertions function signature and input validation",
            "description": "Set up the transformAssertions function in src/transformer.zig with proper signature, allocator handling, and validation for the assert Value parameter",
            "dependencies": [],
            "details": "In src/transformer.zig, implement `transformAssertions(allocator: Allocator, assert: ?Value) ![]Assertion` function. Handle null input (missing assert block) by returning empty slice. Validate that assert Value is a multimap type - return error.InvalidAssertBlock if not. Initialize ArrayList(Assertion) for building result array. Return owned slice at the end using toOwnedSlice(). This function serves as the entry point for assertion transformation.",
            "status": "pending",
            "testStrategy": "Unit tests: call with null returns empty slice, call with Value.multimap succeeds, call with Value.string errors with InvalidAssertBlock, call with empty multimap returns empty slice, verify allocator usage with std.testing.allocator for leak detection"
          },
          {
            "id": 2,
            "title": "Implement assertion operator parsing from entry value",
            "description": "Parse the assertion entry value to extract operator and expected value, mapping Bru operator syntax to OpenCollection AssertionOperator enum",
            "dependencies": [
              1
            ],
            "details": "Create helper function `parseAssertionOperator(value_str: []const u8) !struct { operator: AssertionOperator, value: []const u8 }`. Parse common Bru assertion formats:\n- 'eq VALUE' → equals\n- 'ne VALUE' → not_equals\n- 'gt VALUE' → greater_than\n- 'lt VALUE' → less_than\n- 'contains VALUE' → contains\n- 'matches REGEX' → matches_regex\n- Handle whitespace around operator and value. Return error.InvalidAssertionOperator for unknown operators. Split on first whitespace to separate operator from value. Trim leading/trailing whitespace from value part.",
            "status": "pending",
            "testStrategy": "Unit tests: test each operator mapping (eq, ne, gt, lt, contains, matches), test with extra whitespace, test unknown operator returns error, test value extraction for various formats, test empty value (should be valid), verify operator case sensitivity"
          },
          {
            "id": 3,
            "title": "Extract expression from assertion entry key",
            "description": "Extract the assertion expression (left-hand side) from the multimap entry key, preserving the exact expression string",
            "dependencies": [
              1
            ],
            "details": "The expression is the key portion of each assert multimap entry. For entry with key 'res.status' and value 'eq 200', extract 'res.status' as the expression. No transformation needed - preserve the key string exactly. Handle common expression patterns like:\n- res.status (response status)\n- res.body.field (response body field access)\n- res.headers.ContentType (header access)\n- Any valid JavaScript-like expression\nStore expression as-is for OpenCollection Assertion struct.",
            "status": "pending",
            "testStrategy": "Unit tests: extract simple expressions (res.status), extract nested expressions (res.body.user.id), preserve dots and brackets in expressions, verify no trimming or transformation occurs, test expressions with special characters"
          },
          {
            "id": 4,
            "title": "Handle @disabled annotation for assertions",
            "description": "Detect and handle @disabled annotations on assert entries, setting the enabled field appropriately in the Assertion struct",
            "dependencies": [
              1
            ],
            "details": "For each Entry in the assert multimap, check the annotations slice for @disabled annotation. If found, set enabled=false in resulting Assertion struct. Otherwise, set enabled=true (default). Handle both:\n- Explicit @disabled annotation in Entry.annotations\n- Entry.disabled boolean flag (set by parser for tilde prefix)\nThe Assertion struct has an enabled boolean field - use it to track assertion state. Disabled assertions are preserved in output but marked as disabled.",
            "status": "pending",
            "testStrategy": "Unit tests: assertion without @disabled (enabled=true), assertion with @disabled annotation (enabled=false), assertion with Entry.disabled=true (enabled=false), assertion with both annotation and disabled flag, verify default enabled state is true"
          },
          {
            "id": 5,
            "title": "Integrate assertion transformation and preserve order",
            "description": "Complete transformAssertions by integrating operator parsing, expression extraction, and disabled handling while preserving assertion order from the IR",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "In transformAssertions function, iterate through assert multimap entries in order. For each Entry:\n1. Extract expression from Entry.key\n2. Parse Entry.value (as string) using parseAssertionOperator to get operator and value\n3. Check Entry.disabled flag and Entry.annotations for @disabled\n4. Construct Assertion struct with all fields: expression, operator, value, enabled\n5. Append to ArrayList\nPreserve entry order - assertions should appear in YAML output in same order as Bru input. Handle edge cases: empty assert block, malformed entry values, non-string values in entries (error). Return owned slice from ArrayList.",
            "status": "pending",
            "testStrategy": "Integration tests: complete assert block with multiple assertions, mixed enabled/disabled assertions, various operators, verify order preservation, test error handling for malformed entries, test with std.testing.allocator for memory safety, test realistic assert blocks from Bru examples"
          }
        ]
      },
      {
        "id": 20,
        "title": "Implement IR to OpenCollection transformer for runtime variables",
        "description": "Transform Bru vars blocks (pre-request, post-response) to OpenCollection runtime.vars array",
        "details": "Add to `src/transformer.zig`:\n- `transformVars(vars: []Entry) ![]Var`\n- vars:pre-request → Var with type: before-request\n- vars:post-response → Var with type: after-response\n- Parse multimap entries into {name, value, type}\n- Handle both formats\n- Preserve variable order\n- Handle missing vars (return empty array)\nMap to runtime.vars[].",
        "testStrategy": "Unit tests for both var types, variable parsing, missing vars",
        "priority": "low",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformVars function signature and input validation",
            "description": "Set up the transformVars function in src/transformer.zig with proper signature, allocator handling, and extraction of vars blocks from the IR multimap",
            "dependencies": [],
            "details": "In src/transformer.zig, implement `transformVars(allocator: Allocator, doc: BruDocument) ![]Var` function. Extract blocks matching pattern 'vars:pre-request' and 'vars:post-response' from the top-level multimap in BruDocument. Validate input: ensure each vars block contains a multimap Value type (not other types like string, array). Return empty slice if no vars blocks found (this is valid - not all requests have runtime variables). Set up ArrayList(Var) with allocator to collect variables from both blocks. This function serves as the entry point for runtime variable transformation.",
            "status": "pending",
            "testStrategy": "Unit tests: transformVars with no vars blocks returns empty array, transformVars with invalid value type returns error, verify allocator usage with std.testing.allocator for leak detection"
          },
          {
            "id": 2,
            "title": "Implement pre-request variables transformation logic",
            "description": "Transform vars:pre-request block entries into Var structs with type set to before_request, parsing name-value pairs from the multimap",
            "dependencies": [
              1
            ],
            "details": "In transformVars function, locate 'vars:pre-request' block from multimap. If found, extract the Value which should be a multimap of []Entry. Iterate through each Entry in the multimap. For each Entry: extract key as variable name ([]const u8), extract value which should be a String variant (convert to []const u8), create Var struct with {name: key, value: extracted_value, type: VarType.before_request, enabled: true}. Check for @disabled annotation on Entry - if present, set enabled: false. Append each Var to the ArrayList. Preserve the order of variables as they appear in the source multimap.",
            "status": "pending",
            "testStrategy": "Unit tests: transform single pre-request variable, transform multiple pre-request variables, verify order preservation, handle @disabled annotation correctly (enabled: false), verify VarType is before_request, handle empty vars:pre-request block"
          },
          {
            "id": 3,
            "title": "Implement post-response variables transformation logic",
            "description": "Transform vars:post-response block entries into Var structs with type set to after_response, handling the same parsing logic as pre-request variables",
            "dependencies": [
              1
            ],
            "details": "In transformVars function, locate 'vars:post-response' block from multimap. If found, extract the Value which should be a multimap of []Entry. Iterate through each Entry in the multimap. For each Entry: extract key as variable name ([]const u8), extract value which should be a String variant (convert to []const u8), create Var struct with {name: key, value: extracted_value, type: VarType.after_response, enabled: true}. Check for @disabled annotation on Entry - if present, set enabled: false. Append each Var to the ArrayList. Preserve the order of variables as they appear in the source multimap.",
            "status": "pending",
            "testStrategy": "Unit tests: transform single post-response variable, transform multiple post-response variables, verify order preservation, handle @disabled annotation correctly (enabled: false), verify VarType is after_response, handle empty vars:post-response block"
          },
          {
            "id": 4,
            "title": "Handle tilde prefix for disabled variables",
            "description": "Implement support for tilde-prefixed variable names (~varname:) which should result in enabled: false, similar to header and other block handling patterns",
            "dependencies": [
              2,
              3
            ],
            "details": "In both pre-request and post-response variable transformation logic, check if Entry key starts with '~' prefix. If key starts with '~': strip the tilde from the name (slice key[1..] to get actual variable name without prefix), set enabled: false in the resulting Var struct. This mirrors the pattern used in transformHeaders where ~Header: results in enabled: false. The tilde prefix is an alternative to @disabled annotation for marking variables as disabled. Both mechanisms should be supported and produce the same result (enabled: false).",
            "status": "pending",
            "testStrategy": "Unit tests: tilde-prefixed pre-request variable (~var:) results in enabled: false, tilde-prefixed post-response variable results in enabled: false, verify tilde is stripped from variable name, test combination of tilde prefix AND @disabled annotation (both should result in enabled: false), verify non-prefixed variables remain enabled: true"
          },
          {
            "id": 5,
            "title": "Finalize transformVars with error handling and integration",
            "description": "Complete transformVars with comprehensive error handling for type mismatches, value extraction failures, and return the final []Var slice from ArrayList",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add robust error handling throughout transformVars: if vars block Value is not a multimap, return TransformError.InvalidFieldValue with diagnostic info. If variable value is not a String variant, return error with context about which variable failed. Handle allocator failures gracefully. After processing both vars:pre-request and vars:post-response blocks, call toOwnedSlice() on ArrayList to return final []Var array. If both blocks are missing, return allocator.alloc(Var, 0) for empty slice. Add integration with the main transform orchestration function so runtime.vars field is populated. Document function behavior in comments: purpose, parameters, return value, possible errors.",
            "status": "pending",
            "testStrategy": "Unit tests: error on non-multimap vars block, error on non-string variable value, successful transformation with both pre-request and post-response variables, verify memory ownership with toOwnedSlice, integration test within complete transformation pipeline (mock BruDocument with vars blocks → verify OpenCollectionRequest.runtime.vars populated correctly)"
          }
        ]
      },
      {
        "id": 21,
        "title": "Implement IR to OpenCollection transformer for docs",
        "description": "Transform Bru docs block to OpenCollection docs field",
        "details": "Add to `src/transformer.zig`:\n- `transformDocs(docs: Value) !?[]const u8`\n- Extract multistring content\n- Preserve markdown formatting exactly\n- Handle missing docs (return null)\n- No transformation of content needed\nMap to docs field as YAML block scalar.",
        "testStrategy": "Unit tests: markdown content preservation, missing docs, multiline docs",
        "priority": "low",
        "dependencies": [
          10,
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformDocs function signature in transformer.zig",
            "description": "Set up the transformDocs function with proper signature, parameter types, and return type for handling Bru docs block transformation",
            "dependencies": [],
            "details": "In `src/transformer.zig`, add the transformDocs function:\n- Function signature: `pub fn transformDocs(docs: Value) !?[]const u8`\n- Takes a Value parameter (expected to be Value.multistring or Value.null)\n- Returns optional string (!?[]const u8) - null if no docs, string content if present\n- Error union for handling invalid input types\n- Add function-level documentation comment explaining it extracts docs from Bru IR\n- Place this function with other transform functions in the module\n- No allocator parameter needed since we return existing slice from IR",
            "status": "pending",
            "testStrategy": "Unit tests: verify function compiles, accepts Value parameter, returns correct type (!?[]const u8), test with null Value returns null without error"
          },
          {
            "id": 2,
            "title": "Implement input validation for docs Value type",
            "description": "Add validation logic to ensure the docs parameter is either null or multistring type, rejecting other Value types with appropriate errors",
            "dependencies": [
              1
            ],
            "details": "Inside transformDocs function body:\n- Use switch statement on docs parameter to discriminate tagged union\n- Case Value.null: return null immediately (no docs present)\n- Case Value.multistring: proceed to content extraction (next subtask)\n- All other cases (bool, number, string, multimap, array): return error.InvalidDocsBlock\n- Add error.InvalidDocsBlock to TransformError set if not already present\n- This validates that Bru parser correctly identified docs as multistring block\n- Error case indicates malformed IR structure",
            "status": "pending",
            "testStrategy": "Unit tests: pass Value.null (should return null), pass Value.multistring (should proceed), pass Value.string (should error InvalidDocsBlock), pass Value.multimap (should error), pass Value.array (should error), verify error type"
          },
          {
            "id": 3,
            "title": "Extract multistring content preserving exact formatting",
            "description": "Implement extraction of the multistring content from the Value.multistring variant, preserving all markdown formatting including whitespace, newlines, and special characters",
            "dependencies": [
              2
            ],
            "details": "In the Value.multistring case of transformDocs:\n- Access the []const u8 payload from Value.multistring variant\n- No content transformation needed - preserve markdown exactly as parsed\n- Preserve all whitespace: leading/trailing spaces, indentation, blank lines\n- Preserve special characters: #, *, -, >, etc. (markdown syntax)\n- Preserve line endings (already normalized to LF by tokenizer)\n- Return the slice directly - no allocation needed since IR owns the memory\n- The slice points to memory in ArenaAllocator from parse phase\n- Content lifetime tied to BruDocument lifetime",
            "status": "pending",
            "testStrategy": "Unit tests: simple markdown text, markdown with headers (#, ##), markdown with lists (-, *), markdown with code blocks (```), multiline content with blank lines, leading/trailing whitespace preservation, verify no content modification, verify returned slice matches input exactly"
          },
          {
            "id": 4,
            "title": "Handle empty multistring content correctly",
            "description": "Add special handling for empty or whitespace-only multistring content to determine whether to return null or empty string based on OpenCollection semantics",
            "dependencies": [
              3
            ],
            "details": "After extracting multistring content:\n- Check if content is empty string (length == 0)\n- Check if content is whitespace-only using std.mem.trim\n- Decision: empty/whitespace-only should return null (no meaningful docs)\n- This prevents emitting empty docs field in YAML output\n- Alternative: return empty string and let YAML emitter decide\n- Recommended: return null for empty content (cleaner YAML)\n- Add std.mem import if not present for trim function\n- Use std.ascii.isWhitespace for whitespace detection",
            "status": "pending",
            "testStrategy": "Unit tests: empty string multistring (should return null), whitespace-only multistring with spaces (should return null), whitespace-only with tabs and newlines (should return null), single non-whitespace character (should return content), verify std.mem.trim usage"
          },
          {
            "id": 5,
            "title": "Add integration with YAML emitter block scalar mapping",
            "description": "Document and verify that the returned docs content will be correctly mapped to YAML block scalar (|-) format by the YAML emitter, ensuring proper markdown rendering",
            "dependencies": [
              4
            ],
            "details": "Complete transformDocs integration:\n- Add function documentation explaining YAML emitter will use block scalar (|-)\n- Block scalar (|-) preserves literal content without interpretation\n- The | indicates block scalar, - strips final newline\n- YAML emitter task (Task 31) will handle actual formatting\n- transformDocs returns raw content, emitter handles YAML syntax\n- Add example in documentation showing Bru docs block → OpenCollection YAML\n- Example: docs {{{ markdown }}} → docs: |- \\n  markdown\n- Verify integration point: OpenCollectionRequest.docs field receives this value\n- Confirm null docs results in omitted docs field in final YAML",
            "status": "pending",
            "testStrategy": "Integration test: create complete BruDocument with docs multistring, call transformDocs, verify returned content matches IR, verify null case propagates correctly, document integration with Task 31 (YAML emitter), verify OpenCollectionRequest struct can hold returned value"
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement annotation resolution in transformer",
        "description": "Handle Bru annotations mapping to OpenCollection fields and metadata",
        "details": "Add to `src/transformer.zig`:\n- `resolveAnnotations(annotations: []Annotation, entry: *Entry) !void`\n- @disabled → enabled: false on entry\n- @description(...) → description field on entry\n- @enum(...) → enum array on entry\n- @file or @file(...) → file indicator in multipart bodies\n- @content-type(...) → content-type field in multipart entries\n- Unknown annotations → preserve in annotations array with name and args\n- Apply annotations to appropriate OpenCollection fields\n- Validate annotation arguments (primitives only)\nCentralized annotation handling for all transformer functions.",
        "testStrategy": "Unit tests for each annotation type, unknown annotation preservation, argument validation, annotation application to different entry types",
        "priority": "medium",
        "dependencies": [
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define annotation resolution function signature and core data structures",
            "description": "Create the resolveAnnotations function with proper error handling and allocator management in src/transformer.zig",
            "dependencies": [],
            "details": "Implement `resolveAnnotations(allocator: Allocator, annotations: []Annotation, entry: *Entry) !void` function signature. Set up proper error types for annotation validation failures. Define internal helper structures for tracking applied annotations and validation state. Ensure allocator is used for any dynamic memory needed during resolution (e.g., copying annotation arguments, building arrays).",
            "status": "pending",
            "testStrategy": "Unit test: function accepts valid annotation slice and entry pointer, returns without error for empty annotations array, allocator tracking shows no leaks"
          },
          {
            "id": 2,
            "title": "Implement standard annotation handlers (@disabled, @description, @enum)",
            "description": "Add logic to recognize and apply @disabled, @description, and @enum annotations to Entry fields",
            "dependencies": [
              1
            ],
            "details": "Create switch/if-else logic to match annotation names. For @disabled: set entry.enabled = false (no arguments expected, error if arguments provided). For @description(text): validate single string argument, copy to entry.description field. For @enum(values...): validate array of primitive arguments, allocate and populate entry.enum array. Implement argument type validation ensuring only primitives (strings, numbers, booleans) are accepted. Return descriptive errors for invalid argument types or counts.",
            "status": "pending",
            "testStrategy": "Unit tests: @disabled sets enabled=false, @description(\"text\") populates description field, @enum(\"a\", \"b\", \"c\") creates enum array, invalid arguments return errors, missing required arguments fail validation"
          },
          {
            "id": 3,
            "title": "Implement multipart body annotations (@file, @content-type)",
            "description": "Handle @file and @content-type annotations specific to multipart request bodies",
            "dependencies": [
              1
            ],
            "details": "For @file or @file(filename): set file indicator flag/field in entry (if entry is multipart body type). Validate that @file is only applied to appropriate entry types (multipart bodies). For @content-type(mime): validate single string argument representing MIME type, set content-type field on multipart entry. Implement entry type checking - return error or warning if annotation applied to incompatible entry type. Handle optional filename argument for @file annotation.",
            "status": "pending",
            "testStrategy": "Unit tests: @file sets file indicator on multipart entry, @file(\"image.png\") sets filename, @content-type(\"image/jpeg\") sets content-type field, applying @file to non-multipart entry returns error, invalid MIME types are caught"
          },
          {
            "id": 4,
            "title": "Implement unknown annotation preservation mechanism",
            "description": "Create logic to preserve unrecognized annotations in entry.annotations array for future extensibility",
            "dependencies": [
              1
            ],
            "details": "After checking all known annotation types, if annotation name doesn't match any standard type, preserve it in entry.annotations array. Allocate space for Annotation struct containing name and args fields. Copy annotation name string and arguments array (primitive values only - validate this). Ensure preserved annotations maintain their original argument order and types. This allows forward compatibility and custom annotations to pass through without data loss.",
            "status": "pending",
            "testStrategy": "Unit tests: unknown @custom annotation preserved in entry.annotations, annotation name and arguments copied correctly, multiple unknown annotations all preserved, memory properly allocated and freed"
          },
          {
            "id": 5,
            "title": "Add comprehensive annotation argument validation and error reporting",
            "description": "Implement robust validation for annotation arguments with clear error messages",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create validateAnnotationArgs helper function that checks: argument count matches expected count for each annotation type, all arguments are primitives (no nested objects/arrays), string arguments are valid UTF-8, numeric arguments are in valid ranges. Generate clear error messages including annotation name, expected vs actual argument count/types, line number from Annotation position info. Handle edge cases: empty argument lists, null values, whitespace-only strings. Return specific error types (ArgumentCountError, ArgumentTypeError, InvalidValueError) for better error handling upstream.",
            "status": "pending",
            "testStrategy": "Unit tests: too many arguments rejected, too few arguments rejected, non-primitive arguments rejected, clear error messages include annotation name and position, edge cases (empty strings, null, zero) handled correctly, all error paths tested"
          }
        ]
      },
      {
        "id": 23,
        "title": "Implement complete transformer orchestration",
        "description": "Integrate all transformation functions into complete Bru to OpenCollection converter",
        "details": "Complete `src/transformer.zig`:\n- `transform(allocator: Allocator, doc: BruDocument) !OpenCollectionRequest`\n- Orchestrate all transform functions\n- Extract blocks by name from top-level multimap\n- Call transformMeta, transformHttpMethod, transformHeaders, etc.\n- Assemble final OpenCollectionRequest struct\n- Validate no contradictory blocks (multiple methods, bodies, auths)\n- Handle missing optional blocks gracefully\n- Preserve unknown blocks as warnings (log but don't fail)\n- Return structured transform errors\nEntry point for all transformation operations.",
        "testStrategy": "Integration tests with complete BruDocument inputs, validate all sections populated correctly, error handling for contradictions, unknown block handling",
        "priority": "high",
        "dependencies": [
          22
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create transformer.zig with main transform function signature and error types",
            "description": "Set up the transformer module structure with the main transform function and comprehensive error handling",
            "dependencies": [],
            "details": "Create src/transformer.zig with:\n- Import statements for std.mem.Allocator, BruDocument from bru_parser.zig, OpenCollectionRequest from opencollection.zig\n- Define TransformError error set including: DuplicateMethod, DuplicateBody, DuplicateAuth, InvalidBlock, MissingRequiredField, AllocationFailed\n- Implement transform(allocator: Allocator, doc: BruDocument) !OpenCollectionRequest function signature\n- Add internal helper extractBlock(doc: BruDocument, name: []const u8) ?Value for extracting named blocks from top-level multimap\n- Set up basic structure for accumulating warnings for unknown blocks",
            "status": "pending",
            "testStrategy": "Unit test: verify function signature compiles, extractBlock helper returns correct values for known/unknown block names, error types are accessible, allocator parameter accepted"
          },
          {
            "id": 2,
            "title": "Implement block extraction and validation logic for required and optional blocks",
            "description": "Extract all known Bru blocks from BruDocument and validate for contradictory configurations",
            "dependencies": [
              1
            ],
            "details": "In transform function, extract blocks using extractBlock helper:\n- Required: meta block (error if missing)\n- HTTP method blocks: get, post, put, patch, delete, head, options (validate exactly one present)\n- Optional blocks: headers, params:query, params:path, body:json, body:xml, body:text, body:form-urlencoded, body:multipart-form, body:graphql, auth:bearer, auth:basic, auth:oauth2, auth:awsv4, auth:digest, script:pre-request, script:post-response, tests, assert, vars:pre-request, vars:post-response, docs\n- Validate contradictions: multiple HTTP methods → DuplicateMethod error, multiple body blocks → DuplicateBody error, multiple auth blocks → DuplicateAuth error\n- Identify unknown blocks (blocks not in known list) and collect for warnings",
            "status": "pending",
            "testStrategy": "Unit tests: missing meta block returns error, multiple HTTP methods returns DuplicateMethod, multiple bodies returns DuplicateBody, multiple auth returns DuplicateAuth, unknown blocks collected correctly, valid configuration passes validation"
          },
          {
            "id": 3,
            "title": "Orchestrate all transformer functions for required and HTTP-related sections",
            "description": "Call transformMeta, transformHttpMethod, transformHeaders, and transformParams to populate OpenCollectionRequest",
            "dependencies": [
              2
            ],
            "details": "Call transformer functions in sequence:\n- transformMeta(meta_block) → populate info field (name, type, seq, tags)\n- transformHttpMethod(doc.entries) → populate http.method and http.url (scans all entries for HTTP verb blocks)\n- transformHeaders(headers_block) if present → populate http.headers array, else empty array\n- transformParams(query_block, path_block) → populate http.params array (handles both types)\n- Store results in OpenCollectionRequest struct fields\n- Propagate errors from transform functions up to caller\n- Handle null/missing optional blocks gracefully (empty arrays or null fields as appropriate)",
            "status": "pending",
            "testStrategy": "Integration tests: complete BruDocument with all HTTP sections transforms correctly, missing optional sections result in empty arrays, errors from transform functions propagate correctly, verify http section fully populated with method, url, headers, params"
          },
          {
            "id": 4,
            "title": "Orchestrate transformer functions for body, auth, and runtime sections",
            "description": "Call transformBody, transformAuth, transformScripts, transformAssertions, transformVars, and transformDocs",
            "dependencies": [
              3
            ],
            "details": "Continue orchestration:\n- transformBody(body_block) if any body:* block present → populate http.body (handles all body types: json, xml, text, form-urlencoded, multipart-form, graphql)\n- transformAuth(auth_block) if any auth:* block present → populate http.auth (handles all auth types: bearer, basic, oauth2, awsv4, digest)\n- transformScripts(doc.entries) → populate runtime.scripts array (extracts all script:* blocks)\n- transformAssertions(assert_block) if present → populate runtime.assertions array\n- transformVars(doc.entries) → populate runtime.vars array (extracts vars:pre-request and vars:post-response)\n- transformDocs(docs_block) if present → populate docs field, else null\n- Handle null returns appropriately (null for optional fields, empty arrays for collections)",
            "status": "pending",
            "testStrategy": "Integration tests: BruDocument with body transforms correctly for all body types, auth transforms for all auth types, scripts/assertions/vars populate runtime correctly, docs field populated or null, complete request with all sections validates correctly"
          },
          {
            "id": 5,
            "title": "Implement unknown block warning mechanism and final OpenCollectionRequest assembly",
            "description": "Handle unknown blocks with warnings and assemble the complete OpenCollectionRequest struct",
            "dependencies": [
              4
            ],
            "details": "Complete the transform function:\n- For each unknown block collected in step 2, log warning using std.log.warn with format: \"Unknown block '{s}' at line {d} - preserving but skipping transformation\"\n- Warnings should not cause transformation to fail, only inform user\n- Assemble final OpenCollectionRequest struct with all populated fields: info (from transformMeta), http (method, url, headers, params, body, auth), runtime (scripts, assertions, vars), docs\n- Return the complete OpenCollectionRequest\n- Ensure all allocated memory uses the provided allocator\n- Add function-level documentation comment explaining the complete transformation flow",
            "status": "pending",
            "testStrategy": "Integration tests: unknown blocks logged as warnings but don't fail transformation, complete BruDocument with all sections transforms to valid OpenCollectionRequest, unknown blocks with known blocks handled correctly, verify log output contains unknown block warnings, final struct has all expected fields populated, memory allocation uses provided allocator correctly"
          }
        ]
      },
      {
        "id": 24,
        "title": "Implement YAML emitter core infrastructure",
        "description": "Create foundational YAML emission utilities for indentation, quoting, and scalar handling",
        "details": "Create `src/yaml_emitter.zig`:\n- `YamlEmitter` struct with: writer (std.io.Writer), indent_level (usize), buffer (ArrayList(u8))\n- `writeIndent() !void` - emit current indentation\n- `writeKey(key: []const u8) !void` - emit key with colon\n- `writeString(s: []const u8) !void` - quote if needed (contains :, #, {, }, [, ], quotes, starts with space)\n- `writeBlockScalar(content: []const u8) !void` - emit |- for multi-line content\n- `writeArray(comptime T: type, items: []const T, writeFn: fn(T) !void) !void` - generic array emission\n- `increaseIndent() void` and `decreaseIndent() void`\n- Quote detection: YAML special chars, leading/trailing spaces\nIndentation: 2 spaces per level.",
        "testStrategy": "Unit tests for indentation, string quoting logic, block scalar formatting, array emission, special character handling",
        "priority": "high",
        "dependencies": [
          11
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create YamlEmitter struct with core fields and initialization",
            "description": "Define the YamlEmitter struct with writer, indent_level, and buffer fields, and implement init/deinit methods",
            "dependencies": [],
            "details": "Create src/yaml_emitter.zig file. Define YamlEmitter struct containing: writer field (type: anytype for generic writer), indent_level field (usize, tracks current indentation depth), buffer field (std.ArrayList(u8) for buffering output). Implement init(allocator: std.mem.Allocator, writer: anytype) to initialize struct. Implement deinit() to free buffer memory. Set initial indent_level to 0. This provides the foundation for all emission operations.",
            "status": "pending",
            "testStrategy": "Unit test: create emitter with test allocator, verify initialization, test deinit with memory leak detection using std.testing.allocator"
          },
          {
            "id": 2,
            "title": "Implement indentation control methods",
            "description": "Create writeIndent, increaseIndent, and decreaseIndent methods for managing YAML indentation levels",
            "dependencies": [
              1
            ],
            "details": "In src/yaml_emitter.zig, implement writeIndent() !void to emit (indent_level * 2) spaces to writer (2 spaces per indentation level as per spec). Implement increaseIndent() void to increment indent_level by 1. Implement decreaseIndent() void to decrement indent_level by 1 (with safety check to prevent underflow). These methods control the hierarchical structure of YAML output.",
            "status": "pending",
            "testStrategy": "Unit tests: verify writeIndent outputs correct number of spaces at levels 0,1,2,3; test increaseIndent/decreaseIndent modify indent_level correctly; test underflow protection"
          },
          {
            "id": 3,
            "title": "Implement string quoting and key emission methods",
            "description": "Create writeString method with YAML special character detection and writeKey for key-value pair emission",
            "dependencies": [
              1,
              2
            ],
            "details": "In src/yaml_emitter.zig, implement writeString(s: []const u8) !void with quoting logic: check if string contains YAML special chars (:, #, {, }, [, ], quotes, leading/trailing spaces). If special chars detected, emit double-quoted string with proper escaping. Otherwise emit plain string. Implement writeKey(key: []const u8) !void to emit key with colon separator (format: 'key: '). Use writeIndent before key emission for proper alignment.",
            "status": "pending",
            "testStrategy": "Unit tests: plain strings without quoting, strings with colons/braces/brackets requiring quotes, strings with leading/trailing spaces, empty strings, quote escaping in quoted strings, writeKey formatting"
          },
          {
            "id": 4,
            "title": "Implement block scalar and array emission methods",
            "description": "Create writeBlockScalar for multi-line content and writeArray for generic array handling",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "In src/yaml_emitter.zig, implement writeBlockScalar(content: []const u8) !void to emit block scalar with |- indicator for literal block style (preserves newlines, strips final newline). Split content by newlines, emit each line with proper indentation. Implement writeArray(comptime T: type, items: []const T, writeFn: fn(*YamlEmitter, T) anyerror!void) !void for generic array emission: iterate items, emit each with '- ' prefix and proper indentation, call writeFn for each item's value serialization.",
            "status": "pending",
            "testStrategy": "Unit tests: single-line block scalar, multi-line block scalar with proper line breaks, empty block scalar, array of strings, array of integers, empty array, nested structure emission via writeFn callback"
          },
          {
            "id": 5,
            "title": "Add comprehensive integration tests and documentation",
            "description": "Create integration tests combining all YamlEmitter methods and add module documentation",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "In src/yaml_emitter.zig, add module-level documentation comment explaining YamlEmitter purpose and usage patterns. Create integration test blocks using std.testing that combine multiple methods: test emitting a complete YAML document with nested keys, arrays, block scalars, and proper indentation. Test edge cases: deeply nested structures, mixed content types, special characters throughout. Verify output matches expected YAML format. This ensures all components work together correctly for real-world usage.",
            "status": "pending",
            "testStrategy": "Integration tests: emit complete YAML documents with headers/body/auth sections mimicking OpenCollection format, verify indentation consistency across nesting levels, test round-trip parse compatibility if YAML parser available"
          }
        ]
      },
      {
        "id": 25,
        "title": "Implement YAML emitter for info section",
        "description": "Emit OpenCollection info section as YAML",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitInfo(info: Info) !void`\n- Write 'info:' key\n- Indent and emit name, type, seq fields\n- Emit tags array if present\n- Handle optional fields (skip if null)\n- Order: name, type, seq, tags\nFormat:\n```yaml\ninfo:\n  name: Create User\n  type: http\n  seq: 1\n  tags:\n    - sanity\n```",
        "testStrategy": "Unit tests: complete info, missing tags, field order, indentation",
        "priority": "medium",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emitInfo function signature and structure",
            "description": "Define the emitInfo method in YamlEmitter for emitting the info section with proper error handling",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, add method to YamlEmitter struct:\n```zig\npub fn emitInfo(self: *YamlEmitter, info: Info) !void {\n    // Write 'info:' key at current indentation level\n    try self.writeIndent();\n    try self.writer.writeAll(\"info:\\n\");\n    \n    // Increase indentation for nested fields\n    self.increaseIndent();\n    defer self.decreaseIndent();\n    \n    // Field emission to be implemented in next subtasks\n}\n```\nFunction takes Info struct from opencollection.zig (contains: name []const u8, type []const u8, seq ?usize, tags ?[][]const u8). Uses existing writeIndent method for proper alignment. Establishes defer pattern to ensure indentation is restored even if errors occur during field emission.",
            "status": "pending",
            "testStrategy": "Unit test: call emitInfo with minimal Info struct, verify 'info:' key emitted, verify indentation increased/decreased correctly, test with std.testing.allocator and std.io.fixedBufferStream for output capture"
          },
          {
            "id": 2,
            "title": "Emit required info fields (name, type)",
            "description": "Implement emission of the required name and type fields with proper indentation and YAML string quoting",
            "dependencies": [
              1
            ],
            "details": "Within emitInfo function in src/yaml_emitter.zig, add after increaseIndent():\n```zig\n// Emit name field (required)\ntry self.writeIndent();\ntry self.writeKey(\"name\");\ntry self.writeString(info.name);\ntry self.writer.writeAll(\"\\n\");\n\n// Emit type field (required, usually 'http')\ntry self.writeIndent();\ntry self.writeKey(\"type\");\ntry self.writeString(info.type);\ntry self.writer.writeAll(\"\\n\");\n```\nUses existing writeKey and writeString methods from task 24. writeString handles YAML special character quoting automatically. Order is important: name first, then type. Fields are always present (not optional in Info struct).",
            "status": "pending",
            "testStrategy": "Unit tests: emit info with plain strings (no quotes needed), emit with special YAML chars in name requiring quotes (colons, braces), verify field order (name before type), verify proper indentation (2 spaces)"
          },
          {
            "id": 3,
            "title": "Emit optional seq field with integer formatting",
            "description": "Handle the optional seq field, emitting it only when present with proper integer-to-string conversion",
            "dependencies": [
              2
            ],
            "details": "In emitInfo function after type field emission, add:\n```zig\n// Emit seq field (optional usize)\nif (info.seq) |seq_value| {\n    try self.writeIndent();\n    try self.writeKey(\"seq\");\n    // Convert usize to string for output\n    var buf: [20]u8 = undefined;\n    const seq_str = try std.fmt.bufPrint(&buf, \"{d}\", .{seq_value});\n    try self.writer.writeAll(seq_str);\n    try self.writer.writeAll(\"\\n\");\n}\n```\nseq is optional (?usize in Info struct), only emit if not null. Use Zig optional unwrap syntax with if-capture. Convert integer to decimal string using std.fmt.bufPrint. Buffer size 20 bytes sufficient for usize (max ~20 digits). No quotes needed for integer values in YAML.",
            "status": "pending",
            "testStrategy": "Unit tests: info with seq present (value: 1, 42, 999), info with seq=null (field not emitted), verify integer formatting (no leading zeros, decimal output), verify field appears after type field"
          },
          {
            "id": 4,
            "title": "Emit optional tags array field",
            "description": "Handle the optional tags array, emitting each tag as a YAML list item with proper indentation",
            "dependencies": [
              3
            ],
            "details": "In emitInfo function after seq field handling, add:\n```zig\n// Emit tags field (optional array of strings)\nif (info.tags) |tags_list| {\n    if (tags_list.len > 0) {\n        try self.writeIndent();\n        try self.writer.writeAll(\"tags:\\n\");\n        self.increaseIndent();\n        defer self.decreaseIndent();\n        \n        for (tags_list) |tag| {\n            try self.writeIndent();\n            try self.writer.writeAll(\"- \");\n            try self.writeString(tag);\n            try self.writer.writeAll(\"\\n\");\n        }\n    }\n}\n```\ntags is optional (?[][]const u8 in Info struct). Only emit if not null AND non-empty array. Use YAML array syntax with '- ' prefix for each item. Nested indentation: tags key at info level, items indented one more level. writeString handles quoting for tag values with special chars.",
            "status": "pending",
            "testStrategy": "Unit tests: info with single tag ['sanity'], multiple tags ['sanity', 'regression'], empty tags array (not emitted), tags=null (not emitted), tags with special chars requiring quotes, verify array indentation (3 levels: root, info fields, tag items)"
          },
          {
            "id": 5,
            "title": "Add comprehensive integration tests for complete info section",
            "description": "Create integration tests combining all info fields with various combinations and verify complete YAML output format",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "In src/yaml_emitter.zig test block, add integration tests:\n```zig\ntest \"emitInfo complete with all fields\" {\n    // Test with name, type, seq=1, tags=['sanity', 'smoke']\n    // Verify exact output matches expected format from spec\n}\n\ntest \"emitInfo minimal (name and type only)\" {\n    // Test with only required fields, seq=null, tags=null\n    // Verify optional fields not present in output\n}\n\ntest \"emitInfo field ordering\" {\n    // Verify output order: name, type, seq, tags\n    // Parse output lines to check sequence\n}\n\ntest \"emitInfo indentation consistency\" {\n    // Verify all fields at same indent level (2 spaces)\n    // Verify tags array items indented further (4 spaces)\n}\n```\nTests should use std.testing.allocator, capture output to buffer, compare against expected YAML strings. Verify complete format matches example in task description.",
            "status": "pending",
            "testStrategy": "Integration tests covering: complete info with all fields, minimal info (required only), various seq values, various tag combinations, special characters in all string fields, verify output is valid YAML that matches OpenCollection spec format"
          }
        ]
      },
      {
        "id": 26,
        "title": "Implement YAML emitter for http section",
        "description": "Emit OpenCollection http section including method, url, headers, params, body, auth",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitHttp(http: Http) !void`\n- Write 'http:' key\n- Emit method and url fields\n- Call emitHeaders, emitParams, emitBody, emitAuth as needed\n- Skip optional sections if null\n- Order: method, url, headers, params, body, auth\n- Proper indentation for nested structures",
        "testStrategy": "Unit tests: minimal http (method+url only), full http with all sections, optional section skipping",
        "priority": "high",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emitHttp function signature and http key emission",
            "description": "Implement the main emitHttp function in yaml_emitter.zig that accepts an Http struct and writes the 'http:' key with proper indentation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, add the emitHttp function:\n```zig\npub fn emitHttp(self: *YamlEmitter, http: Http) !void {\n    try self.writeIndent();\n    try self.writer.writeAll(\"http:\\n\");\n    self.increaseIndent();\n    defer self.decreaseIndent();\n    \n    // Method and URL emission will be added in next subtask\n}\n```\nThis function takes an Http struct from opencollection.zig (containing method, url, headers, params, body, auth fields). It writes the 'http:' key at the current indentation level, then increases indentation for nested content. The defer ensures indentation is restored after all http fields are emitted. This establishes the foundation for emitting all HTTP-related configuration.",
            "status": "pending",
            "testStrategy": "Unit test: call emitHttp with minimal Http struct (only method+url), verify 'http:' key written with correct indentation, verify indent level increases/decreases properly, test with different initial indent levels"
          },
          {
            "id": 2,
            "title": "Emit required HTTP method and URL fields",
            "description": "Add method and url field emission to emitHttp, handling string quoting as needed for URL values",
            "dependencies": [
              1
            ],
            "details": "Extend emitHttp in src/yaml_emitter.zig to emit method and url fields:\n```zig\n// After increaseIndent() in emitHttp:\ntry self.writeIndent();\ntry self.writeKey(\"method\");\ntry self.writeString(http.method);\ntry self.writer.writeAll(\"\\n\");\n\ntry self.writeIndent();\ntry self.writeKey(\"url\");\ntry self.writeString(http.url);\ntry self.writer.writeAll(\"\\n\");\n```\nMethod is emitted first (GET, POST, PUT, DELETE, etc. as uppercase strings). URL follows on next line. Both use writeString which applies quoting if the value contains YAML special characters (e.g., colons in http://, query params with &, etc.). These are required fields per OpenCollection spec, so they're always emitted. Field order follows spec: method before url.",
            "status": "pending",
            "testStrategy": "Unit tests: emit method (GET, POST, etc.), emit simple URL without special chars, emit URL with query params (requires quoting), emit URL with fragments, verify field order (method before url), verify proper indentation for both fields"
          },
          {
            "id": 3,
            "title": "Add conditional emission for headers and params arrays",
            "description": "Call emitHeaders and emitParams helper functions when http.headers and http.params are non-null, skipping these sections if null",
            "dependencies": [
              2
            ],
            "details": "Extend emitHttp in src/yaml_emitter.zig to conditionally emit headers and params:\n```zig\n// After url emission:\nif (http.headers) |headers| {\n    if (headers.len > 0) {\n        try self.emitHeaders(headers);\n    }\n}\n\nif (http.params) |params| {\n    if (params.len > 0) {\n        try self.emitParams(params);\n    }\n}\n```\nThese functions (emitHeaders and emitParams) are implemented in task 27 but are called here. Headers and params are optional fields on Http struct (type: ?[]Header and ?[]Param). Only emit if non-null and non-empty. This maintains clean YAML output without empty array sections. Order follows spec: headers before params, both after url and before body.",
            "status": "pending",
            "testStrategy": "Unit tests: http with null headers (should skip headers section), http with empty headers array (should skip), http with headers (should call emitHeaders), same for params, verify emitHeaders/emitParams are called with correct arguments, verify field order in output"
          },
          {
            "id": 4,
            "title": "Add conditional emission for request body",
            "description": "Call emitBody helper function when http.body is non-null to emit the request body with type-specific formatting",
            "dependencies": [
              3
            ],
            "details": "Extend emitHttp in src/yaml_emitter.zig to conditionally emit body:\n```zig\n// After params emission:\nif (http.body) |body| {\n    try self.emitBody(body);\n}\n```\nThe emitBody function (implemented in task 28) handles the Body tagged union which can be Json, Xml, Text, FormUrlEncoded, MultipartForm, or GraphQL. Body is optional on Http struct (type: ?Body). Only emit if non-null. Body section appears after params and before auth per OpenCollection spec. The emitBody function will handle the complexity of different body types (block scalars for text content, arrays for form data, etc.).",
            "status": "pending",
            "testStrategy": "Unit tests: http with null body (should skip body section), http with Json body (should call emitBody), http with each body type variant, verify emitBody called with correct Body value, verify field order (body after params, before auth)"
          },
          {
            "id": 5,
            "title": "Add conditional emission for authentication",
            "description": "Call emitAuth helper function when http.auth is non-null to emit authentication configuration with type-specific fields",
            "dependencies": [
              4
            ],
            "details": "Complete emitHttp in src/yaml_emitter.zig by conditionally emitting auth:\n```zig\n// After body emission (before defer decreaseIndent):\nif (http.auth) |auth| {\n    try self.emitAuth(auth);\n}\n```\nThe emitAuth function (implemented in task 29) handles the Auth tagged union which can be Bearer, Basic, OAuth2, AwsV4, or Digest. Auth is optional on Http struct (type: ?Auth). Only emit if non-null. Auth is the last field in the http section per OpenCollection spec. This completes the emitHttp implementation with all six possible subsections: method (required), url (required), headers (optional), params (optional), body (optional), auth (optional). The defer decreaseIndent() ensures proper indentation restoration after all fields are emitted.",
            "status": "pending",
            "testStrategy": "Unit tests: http with null auth (should skip auth section), http with Bearer auth (should call emitAuth), http with each auth type variant, verify emitAuth called with correct Auth value, verify complete field order (method, url, headers, params, body, auth), integration test with full Http struct containing all sections, verify indentation decreases after all fields"
          }
        ]
      },
      {
        "id": 27,
        "title": "Implement YAML emitter for headers, params, and arrays",
        "description": "Emit OpenCollection arrays for headers and params with enabled field handling",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitHeaders(headers: []Header) !void`\n- `emitParams(params: []Param) !void`\n- Array format: each item on new line with '- '\n- Header: name, value, enabled (skip if true)\n- Param: name, value, type, enabled (skip if true)\n- Quote values with special YAML chars\n- Preserve array order\nFormat:\n```yaml\nheaders:\n  - name: Content-Type\n    value: application/json\n  - name: X-Debug\n    value: \"true\"\n    enabled: false\n```",
        "testStrategy": "Unit tests: basic arrays, disabled entries, special characters in values, empty arrays",
        "priority": "medium",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement emitHeaders function signature and 'headers:' key emission",
            "description": "Create the emitHeaders function in yaml_emitter.zig that accepts a Header slice and writes the 'headers:' key with proper indentation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, add the emitHeaders function:\n- Function signature: `fn emitHeaders(self: *YamlEmitter, headers: []Header) !void`\n- First, write the 'headers:' key using writeKey() at current indentation level\n- Increase indentation level for array items\n- The function will be completed in subsequent subtasks to emit individual header items\n- This establishes the structure for the headers array section\n- Follow the same pattern used in emitHttp and emitInfo functions\n- Ensure proper error propagation with ! return type",
            "status": "pending",
            "testStrategy": "Unit test: call emitHeaders with empty Header slice, verify 'headers:' key written with correct indentation, verify indent level increases properly, test with different initial indent levels, use std.testing.allocator and std.io.fixedBufferStream for output capture"
          },
          {
            "id": 2,
            "title": "Implement Header array item emission with name and value fields",
            "description": "Add logic to iterate through headers array and emit each header as YAML array item with name and value fields",
            "dependencies": [
              1
            ],
            "details": "In src/yaml_emitter.zig, extend emitHeaders function:\n- Iterate through headers slice using for loop: `for (headers) |header|`\n- For each header, emit array item marker '- ' using writeIndent() followed by '- '\n- Increase indentation for header fields\n- Emit 'name' field using writeKey() and writeString() with header.name value\n- Emit 'value' field using writeKey() and writeString() with header.value\n- Apply quoting logic from writeString() for values with special YAML characters (colons, braces, quotes, etc.)\n- Decrease indentation after each header item\n- Preserve array order as provided in headers slice\n- Handle edge case of empty headers array gracefully",
            "status": "pending",
            "testStrategy": "Unit tests: emit single header with simple name/value, emit multiple headers preserving order, emit header with special YAML chars in value requiring quotes (e.g., value: 'application/json'), emit header with colon in value, verify proper indentation for array items (2 spaces per level), verify '- ' array marker placement, test empty headers array"
          },
          {
            "id": 3,
            "title": "Add conditional 'enabled' field emission for disabled headers",
            "description": "Extend Header emission to conditionally include the enabled field only when it is false, skipping it when true",
            "dependencies": [
              2
            ],
            "details": "In src/yaml_emitter.zig, modify emitHeaders header iteration logic:\n- After emitting name and value fields for each header\n- Check if header.enabled is false: `if (!header.enabled)`\n- Only when enabled is false, emit 'enabled: false' field using writeKey() and writeBool()\n- When enabled is true (the default/common case), skip emitting the enabled field entirely\n- This reduces YAML verbosity by omitting default values\n- Ensure enabled field comes after name and value fields\n- Maintain consistent indentation for all three fields\n- Follow OpenCollection spec format for disabled headers",
            "status": "pending",
            "testStrategy": "Unit tests: header with enabled=true (should NOT emit enabled field, verify output omits it), header with enabled=false (should emit 'enabled: false'), multiple headers with mixed enabled states, verify field order (name, value, enabled), verify output matches expected format from task description example with 'X-Debug' header"
          },
          {
            "id": 4,
            "title": "Implement emitParams function with name, value, type, and enabled fields",
            "description": "Create emitParams function following the same pattern as emitHeaders but including the additional 'type' field for parameters",
            "dependencies": [
              3
            ],
            "details": "In src/yaml_emitter.zig, implement emitParams function:\n- Function signature: `fn emitParams(self: *YamlEmitter, params: []Param) !void`\n- Write 'params:' key using writeKey()\n- Increase indentation for array items\n- Iterate through params slice: `for (params) |param|`\n- For each param, emit '- ' array marker\n- Emit 'name' field with param.name\n- Emit 'value' field with param.value (with quoting for special chars)\n- Emit 'type' field with param.type (ParamType enum: 'query' or 'path')\n- Conditionally emit 'enabled: false' field only when param.enabled is false\n- Skip enabled field when true (default)\n- Field order: name, value, type, enabled\n- Preserve params array order\n- Decrease indentation after completion",
            "status": "pending",
            "testStrategy": "Unit tests: emit single param with type='query', emit param with type='path', emit multiple params with mixed types, emit param with enabled=false (verify 'enabled: false' appears), emit param with enabled=true (verify enabled field omitted), verify field order (name, value, type, enabled), test special chars in param values requiring quotes, test empty params array, verify '- ' array markers and indentation match headers pattern"
          },
          {
            "id": 5,
            "title": "Add comprehensive integration tests for headers and params emission",
            "description": "Create integration tests verifying complete headers and params emission with edge cases, special characters, and real-world examples",
            "dependencies": [
              4
            ],
            "details": "In src/yaml_emitter.zig or dedicated test file, implement comprehensive tests:\n- Test headers with special characters in values: quotes, colons, braces, brackets, leading/trailing spaces\n- Test params with all ParamType variants (query and path)\n- Test mixed enabled/disabled entries in same array\n- Test empty arrays for both headers and params\n- Test realistic examples matching OpenCollection spec format\n- Verify output YAML is syntactically valid (can be parsed)\n- Test preservation of array order for both headers and params\n- Test quoting behavior: values without special chars unquoted, values with special chars quoted\n- Integration test combining headers and params in same emitter context\n- Verify indentation consistency across all nested structures\n- Test with example from task description (Content-Type header and X-Debug disabled header)",
            "status": "pending",
            "testStrategy": "Integration tests: complete headers array with 5+ items including disabled entries, complete params array with query and path types, headers with special chars (':', '#', '{', '}', '[', ']'), params with boolean-like string values requiring quotes, empty headers and params arrays, verify output matches exact format from task description example, parse emitted YAML to verify syntactic correctness"
          }
        ]
      },
      {
        "id": 28,
        "title": "Implement YAML emitter for request body",
        "description": "Emit OpenCollection http.body with type-specific formatting using block scalars for content",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitBody(body: Body) !void`\n- Switch on body type tag\n- Json/Xml/Text: emit type and data as block scalar (|-)\n- FormUrlEncoded: emit type and array of {name, value}\n- MultipartForm: emit type and array with file/content-type handling\n- GraphQL: emit type, query, and variables\n- Block scalars preserve formatting, no escaping needed\nFormat:\n```yaml\nbody:\n  type: json\n  data: |-\n    {\n      \"email\": \"user@example.com\"\n    }\n```",
        "testStrategy": "Unit tests for each body type, block scalar formatting, multipart file handling, graphql structure",
        "priority": "high",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emitBody function signature and body key emission",
            "description": "Implement the main emitBody function in yaml_emitter.zig that accepts a Body tagged union and writes the 'body:' key with proper indentation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, create emitBody function with signature: pub fn emitBody(self: *YamlEmitter, body: Body) !void. First emit 'body:' key using writeKey or writeIndent + direct write. Increase indent level for nested fields. The function will switch on body type tag in subsequent subtasks. Set up basic error handling using Zig's error union return type (!void). This establishes the entry point for all body type emission.",
            "status": "pending",
            "testStrategy": "Unit test: call emitBody with any Body variant, verify 'body:' key written with correct indentation, verify indent level increases properly, test with different initial indent levels using std.testing.allocator and std.io.fixedBufferStream for output capture"
          },
          {
            "id": 2,
            "title": "Implement emission for Json, Xml, and Text body types using block scalars",
            "description": "Add switch cases for Body.Json, Body.Xml, and Body.Text that emit type field and data content as block scalar with |- indicator",
            "dependencies": [
              1
            ],
            "details": "In emitBody function, add switch statement on body tag. For Json, Xml, Text variants: emit 'type:' field with value ('json', 'xml', or 'text'), then emit 'data:' field using block scalar format (|-). Use writeKey for 'type' and 'data', writeString for type value, and writeBlockScalar for data content. Block scalar (|-) preserves exact formatting without escaping, strips trailing newlines. Order: type first, then data. Each variant extracts data string from Body.Json.data, Body.Xml.data, or Body.Text.data respectively.",
            "status": "pending",
            "testStrategy": "Unit tests: emit Json body with JSON object content, emit Xml body with XML markup, emit Text body with plain text, verify block scalar formatting (|- indicator present), verify multi-line content preserved exactly, verify no escaping occurs, verify type and data field order, test with empty data strings, test with data containing special YAML chars (should not be escaped in block scalar)"
          },
          {
            "id": 3,
            "title": "Implement emission for FormUrlEncoded body type with field arrays",
            "description": "Add switch case for Body.FormUrlEncoded that emits type and fields array with name-value pairs",
            "dependencies": [
              1
            ],
            "details": "In emitBody switch, add FormUrlEncoded case. Emit 'type: form-urlencoded' field. Then emit 'fields:' array where each item has 'name' and 'value' fields. Use array emission pattern from emitHeaders/emitParams: iterate Body.FormUrlEncoded.fields slice ([]FormField), emit '- ' array marker, increase indent, emit name and value for each FormField struct, decrease indent. Preserve field order from source array. Format:\n```\nbody:\n  type: form-urlencoded\n  fields:\n    - name: email\n      value: user@example.com\n```",
            "status": "pending",
            "testStrategy": "Unit tests: emit FormUrlEncoded with single field, emit with multiple fields preserving order, emit with empty fields array, verify type field value is 'form-urlencoded', verify array '- ' markers and indentation match headers/params pattern, verify name/value field order within each array item, test with special chars in field names/values requiring quotes"
          },
          {
            "id": 4,
            "title": "Implement emission for MultipartForm body type with file handling",
            "description": "Add switch case for Body.MultipartForm that emits type and parts array with optional filename and content_type fields",
            "dependencies": [
              1
            ],
            "details": "In emitBody switch, add MultipartForm case. Emit 'type: multipart-form'. Then emit 'parts:' array iterating Body.MultipartForm.parts slice ([]MultipartPart). For each part: emit required 'name' and 'data' fields, emit optional 'filename' and 'content_type' fields only if non-null (check ?[]const u8). Use block scalar (|-) for data field if it contains multi-line content. Order within each part: name, filename (if present), content_type (if present), data. Handle both text and file parts based on presence of filename field.",
            "status": "pending",
            "testStrategy": "Unit tests: emit MultipartForm with text part (name+data only), emit with file part (name+filename+content_type+data), emit with mixed text and file parts, emit with null filename/content_type (verify fields omitted), verify type field value is 'multipart-form', verify optional field handling, verify data uses block scalar for multi-line content, verify field order within parts, test empty parts array"
          },
          {
            "id": 5,
            "title": "Implement emission for GraphQL body type with query and optional variables",
            "description": "Add switch case for Body.GraphQL that emits type, query (as block scalar), and optional variables/operation_name fields",
            "dependencies": [
              1
            ],
            "details": "In emitBody switch, add GraphQL case. Emit 'type: graphql' field. Emit 'query:' using block scalar (|-) for Body.GraphQL.query (typically multi-line GraphQL syntax). Emit 'variables:' field only if Body.GraphQL.variables is non-null (?[]const u8), using block scalar for JSON object. Emit 'operation_name:' only if Body.GraphQL.operation_name is non-null. Field order: type, query, variables (if present), operation_name (if present). After all body fields emitted, decrease indent level to return to original level. This completes all Body variant emissions.",
            "status": "pending",
            "testStrategy": "Unit tests: emit GraphQL with query only (no variables), emit with query and variables, emit with all fields including operation_name, verify type field value is 'graphql', verify query uses block scalar preserving GraphQL syntax, verify variables uses block scalar for JSON content, verify optional field handling (variables/operation_name omitted when null), verify field order, verify indent level properly decreased after emission, integration test with all body types to verify complete emitBody function"
          }
        ]
      },
      {
        "id": 29,
        "title": "Implement YAML emitter for authentication",
        "description": "Emit OpenCollection http.auth with type-specific field handling",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitAuth(auth: Auth) !void`\n- Switch on auth type tag\n- Bearer: emit type and token\n- Basic: emit type, username, password\n- OAuth2: emit type and all grant-specific fields\n- AwsV4: emit type, accessKeyId, secretAccessKey, region, service\n- Digest: emit type, username, password\n- Proper field ordering per OpenCollection spec\nFormat:\n```yaml\nauth:\n  type: bearer\n  token: {{authToken}}\n```",
        "testStrategy": "Unit tests for each auth type, field completeness, OAuth2 grant variations, AWS fields",
        "priority": "medium",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emitAuth function signature and auth section header",
            "description": "Define the emitAuth method in yaml_emitter.zig with proper signature and emit the 'auth:' key with indentation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, create function: pub fn emitAuth(self: *YamlEmitter, auth: Auth) !void. First emit 'auth:' key at current indent level using writeKey() or writeIndent() followed by writer.writeAll(\"auth:\\n\"). Increase indent level with increaseIndent() for nested auth fields. Set up error handling with Zig's error union return type (!void). This establishes the entry point for authentication emission before switching on auth type variants.",
            "status": "pending",
            "testStrategy": "Unit test: verify 'auth:' key emitted with correct indentation, verify indent_level increased by 1 after header emission, test with mock writer to capture output"
          },
          {
            "id": 2,
            "title": "Implement Bearer and Basic authentication emission",
            "description": "Add switch statement on Auth tagged union and implement emission for Bearer (type + token) and Basic (type + username + password) variants",
            "dependencies": [
              1
            ],
            "details": "In emitAuth function, add switch statement: switch (auth) { .Bearer => |bearer| { ... }, .Basic => |basic| { ... } }. For Bearer: emit 'type: bearer' then 'token: <token_value>' with proper indentation. For Basic: emit 'type: basic', then 'username: <username>', then 'password: <password>'. Use writeIndent() before each field, writeKey() for keys, and writeString() for values to handle YAML special characters. Fields order must match OpenCollection spec: type first, then credentials. Handle optional Bearer prefix field if present.",
            "status": "pending",
            "testStrategy": "Unit tests: Bearer with token value, Bearer with token containing special chars requiring quotes, Basic with username and password, Basic with values containing colons/quotes, verify field ordering (type before credentials), verify indentation is 2 spaces per level"
          },
          {
            "id": 3,
            "title": "Implement OAuth2 authentication emission with grant-specific fields",
            "description": "Add OAuth2 variant case to switch statement, emitting type and all OAuth2-specific fields including optional token_type and refresh_token",
            "dependencies": [
              1
            ],
            "details": "In emitAuth switch, add .OAuth2 => |oauth2| { ... }. Emit fields in order: 'type: oauth2', 'access_token: <value>', then optional fields only if non-null: 'token_type: <value>' (e.g., 'Bearer'), 'refresh_token: <value>'. Use if (oauth2.token_type) |tt| { ... } pattern for optional field checking. All fields should use writeIndent(), writeKey(), and writeString() for proper formatting. OAuth2 may have additional grant-specific fields (client_id, client_secret, scope) - check Auth union definition from Task 11 and emit all defined fields maintaining proper field order per OpenCollection spec.",
            "status": "pending",
            "testStrategy": "Unit tests: OAuth2 with only access_token, OAuth2 with all optional fields populated, OAuth2 with token_type but no refresh_token, verify optional fields skipped when null, verify field ordering matches spec, test with various grant types if supported"
          },
          {
            "id": 4,
            "title": "Implement AwsV4 authentication emission with AWS-specific fields",
            "description": "Add AwsV4 variant case emitting type, accessKeyId, secretAccessKey, region, service, and optional session_token",
            "dependencies": [
              1
            ],
            "details": "In emitAuth switch, add .AwsV4 => |aws| { ... }. Emit required fields in order: 'type: awsv4', 'accessKeyId: <value>' (or access_key based on Auth union field name from Task 11), 'secretAccessKey: <value>' (or secret_key), 'region: <value>', 'service: <value>'. Then emit optional 'session_token: <value>' only if non-null using if (aws.session_token) |st| { ... }. Verify exact field names from Auth.AwsV4 struct definition (may be camelCase or snake_case). Use proper indentation and string quoting for all values. AWS credentials are sensitive - ensure proper handling.",
            "status": "pending",
            "testStrategy": "Unit tests: AwsV4 with all required fields, AwsV4 with session_token, AwsV4 without session_token (should skip), verify field names match Auth struct definition, verify field ordering (type, keys, region, service, session_token), test with values containing special characters"
          },
          {
            "id": 5,
            "title": "Implement Digest authentication emission and finalize emitAuth",
            "description": "Add Digest variant case emitting type, username, password, and optional realm, nonce, algorithm fields, then decrease indent level to complete emitAuth function",
            "dependencies": [
              1
            ],
            "details": "In emitAuth switch, add .Digest => |digest| { ... }. Emit required fields: 'type: digest', 'username: <value>', 'password: <value>'. Then emit optional fields only if non-null: 'realm: <value>', 'nonce: <value>', 'algorithm: <value>' (e.g., 'MD5', 'SHA-256'). After switch statement completes (all auth types handled), call decreaseIndent() to restore previous indent level. Add comprehensive error handling for write operations using try. Ensure all auth type variants are covered - compiler will enforce exhaustive switching on tagged union. Add function documentation comment explaining supported auth types and field ordering requirements.",
            "status": "pending",
            "testStrategy": "Unit tests: Digest with username and password only, Digest with all optional fields, Digest with selective optional fields, verify optional fields skipped when null, verify field ordering, verify indent level decreased after emission, integration test calling emitAuth with each of 5 auth types (Bearer, Basic, OAuth2, AwsV4, Digest), verify complete YAML output format matches OpenCollection spec examples"
          }
        ]
      },
      {
        "id": 30,
        "title": "Implement YAML emitter for runtime section",
        "description": "Emit OpenCollection runtime section with scripts, assertions, and vars using block scalars for code",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitRuntime(runtime: Runtime) !void`\n- Emit scripts array: type and code (block scalar)\n- Emit assertions array: expression, operator, value\n- Emit vars array: name, value, type\n- Skip runtime section entirely if all arrays empty\n- Code in scripts uses |- block scalar\n- Order: scripts, assertions, vars\nFormat:\n```yaml\nruntime:\n  scripts:\n    - type: before-request\n      code: |-\n        bru.setVar('timestamp', Date.now());\n```",
        "testStrategy": "Unit tests: scripts with code, assertions, vars, empty runtime, code formatting preservation",
        "priority": "medium",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emitRuntime function signature and conditional runtime section emission",
            "description": "Define the emitRuntime method in yaml_emitter.zig with logic to check if runtime section should be emitted (skip if all arrays empty) and write the 'runtime:' key with proper indentation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, create function: pub fn emitRuntime(self: *YamlEmitter, runtime: Runtime) !void. First check if all Runtime fields are empty/null: if scripts is empty AND assertions is empty AND vars is empty, return early without emitting anything. This implements the requirement to skip runtime section entirely if all arrays empty. If at least one array has content, emit 'runtime:' key using writeKey() or writeIndent() + writer.writeAll(\"runtime:\\n\"). Increase indent level with increaseIndent() for nested runtime fields. Set up error handling with Zig's error union return type (!void).",
            "status": "pending",
            "testStrategy": "Unit tests: call emitRuntime with empty Runtime (all arrays empty) - verify no output emitted, call with runtime containing scripts only - verify 'runtime:' key written, call with runtime containing any non-empty array - verify header emitted, verify indent_level increased when section emitted, test with std.testing.allocator and std.io.fixedBufferStream for output capture"
          },
          {
            "id": 2,
            "title": "Implement scripts array emission with type and code block scalar",
            "description": "Add logic to emit the scripts array when non-empty, emitting each Script with type field and code as block scalar using |- indicator to preserve formatting",
            "dependencies": [
              1
            ],
            "details": "In emitRuntime function, after the runtime header, check if runtime.scripts is non-null and non-empty. If so, emit 'scripts:' key with array formatting. Iterate through runtime.scripts slice ([]Script). For each Script: emit '- ' array marker, increase indent, emit 'type:' field with Script.type value (map ScriptType enum to strings: before_request → 'before-request', after_response → 'after-response', test → 'test'), emit 'code:' field using block scalar format (|-) with writeBlockScalar() to preserve exact JavaScript code formatting without escaping. Conditionally emit 'enabled: false' field only if Script.enabled is false. Decrease indent after each script. Order within each script: type, code, enabled (if false).",
            "status": "pending",
            "testStrategy": "Unit tests: emit single script with before-request type and code, emit multiple scripts preserving order, emit script with multi-line JavaScript code (verify |- block scalar preserves formatting exactly), emit script with enabled=false (verify 'enabled: false' appears), emit script with enabled=true (verify enabled field omitted), verify ScriptType enum mapping (before_request → 'before-request', after_response → 'after-response', test → 'test'), verify code containing special YAML chars preserved in block scalar, verify array '- ' markers and indentation match task description example, test empty scripts array (should skip scripts section)"
          },
          {
            "id": 3,
            "title": "Implement assertions array emission with expression, operator, and value",
            "description": "Add logic to emit the assertions array when non-empty, emitting each Assertion with expression, operator, and value fields in the correct order",
            "dependencies": [
              1
            ],
            "details": "In emitRuntime function, after scripts section (if present), check if runtime.assertions is non-null and non-empty. If so, emit 'assertions:' key with array formatting. Iterate through runtime.assertions slice ([]Assertion). For each Assertion: emit '- ' array marker, increase indent, emit 'expression:' field with Assertion.expression string, emit 'operator:' field with Assertion.operator value (map AssertionOperator enum to strings: eq, ne, gt, lt, gte, lte, contains, matches, etc.), emit 'value:' field with Assertion.value string (use writeString() for proper YAML quoting if special chars present). Conditionally emit 'enabled: false' field only if Assertion.enabled is false. Decrease indent after each assertion. Order within each assertion: expression, operator, value, enabled (if false).",
            "status": "pending",
            "testStrategy": "Unit tests: emit single assertion with eq operator, emit multiple assertions preserving order, emit assertion for each AssertionOperator variant (eq, ne, gt, lt, contains, matches, etc.), emit assertion with enabled=false (verify 'enabled: false' appears), emit assertion with enabled=true (verify enabled field omitted), verify operator enum mapping matches OpenCollection spec, verify expression and value with special YAML chars are properly quoted, verify field order (expression, operator, value, enabled), test empty assertions array (should skip assertions section), verify array indentation and '- ' markers"
          },
          {
            "id": 4,
            "title": "Implement vars array emission with name, value, and type fields",
            "description": "Add logic to emit the vars array when non-empty, emitting each Var with name, value, and type fields in the correct order",
            "dependencies": [
              1
            ],
            "details": "In emitRuntime function, after assertions section (if present), check if runtime.vars is non-null and non-empty. If so, emit 'vars:' key with array formatting. Iterate through runtime.vars slice ([]Var). For each Var: emit '- ' array marker, increase indent, emit 'name:' field with Var.name string, emit 'value:' field with Var.value string (use writeString() for proper YAML quoting), emit 'type:' field with Var.type value (map VarType enum to strings: before_request → 'before-request', after_response → 'after-response'). Conditionally emit 'enabled: false' field only if Var.enabled is false. Decrease indent after each var. Order within each var: name, value, type, enabled (if false).",
            "status": "pending",
            "testStrategy": "Unit tests: emit single var with before-request type, emit multiple vars preserving order, emit var with after-response type, emit var with enabled=false (verify 'enabled: false' appears), emit var with enabled=true (verify enabled field omitted), verify VarType enum mapping (before_request → 'before-request', after_response → 'after-response'), verify name and value with special YAML chars are properly quoted, verify field order (name, value, type, enabled), test empty vars array (should skip vars section), verify array indentation and '- ' markers"
          },
          {
            "id": 5,
            "title": "Integrate runtime emission with correct field ordering and finalize indentation",
            "description": "Complete emitRuntime by ensuring correct field order (scripts, assertions, vars), proper indentation management, and integration testing with complete Runtime structs",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "In emitRuntime function, verify that the emission order is strictly: scripts first, then assertions, then vars (as specified in task requirements). After all runtime fields have been emitted, call decreaseIndent() to return to the original indent level before the runtime section. Ensure that empty arrays are skipped (no section emitted for empty scripts/assertions/vars arrays). Verify that if all three arrays are empty, the entire runtime section is skipped (early return logic from subtask 1). Add comprehensive error propagation for all writeKey, writeString, writeBlockScalar, and writeIndent calls.",
            "status": "pending",
            "testStrategy": "Integration tests: emit complete Runtime with all three arrays populated (verify order: scripts, assertions, vars), emit Runtime with only scripts (verify assertions and vars sections omitted), emit Runtime with only assertions, emit Runtime with only vars, emit Runtime with scripts and assertions but no vars, emit empty Runtime (all arrays empty - verify no output), verify indentation decreases properly after runtime section, verify block scalar formatting preserved for script code matching task description example format, test with nested runtime in complete OpenCollectionRequest emission, verify memory safety with std.testing.allocator"
          }
        ]
      },
      {
        "id": 31,
        "title": "Implement YAML emitter for docs and comments",
        "description": "Emit OpenCollection docs field and optionally preserve Bru comments as YAML comments",
        "details": "Add to `src/yaml_emitter.zig`:\n- `emitDocs(docs: ?[]const u8) !void`\n- Use block scalar (|-) for markdown content\n- Skip if docs is null\n- `emitComment(comment: []const u8) !void` for --keep-comments mode\n- YAML comment format: # followed by comment text\n- Comments emit before associated key\n- Preserve comment position hints from parse phase\nDocs is last section in output.",
        "testStrategy": "Unit tests: docs with markdown, missing docs, comment preservation, comment positioning",
        "priority": "low",
        "dependencies": [
          24
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement emitDocs function signature and docs key emission",
            "description": "Create the emitDocs function in yaml_emitter.zig that accepts an optional docs string and writes the 'docs:' key with block scalar notation",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, add the emitDocs function:\n```zig\npub fn emitDocs(self: *YamlEmitter, docs: ?[]const u8) !void {\n    // Early return if docs is null - skip section entirely\n    const docs_content = docs orelse return;\n    \n    // Write 'docs:' key at current indentation level\n    try self.writeIndent();\n    try self.writer.writeAll(\"docs: |-\\n\");\n    \n    // Increase indent for block content\n    self.increaseIndent();\n    defer self.decreaseIndent();\n    \n    // Content emission will be added in next subtask\n}\n```\nThe '|-' syntax indicates a literal block scalar that strips the final newline, which is appropriate for markdown content. This function handles the optional docs field by returning early if null, ensuring clean YAML output without empty docs sections. The docs section should appear last in the OpenCollection YAML output per the specification.",
            "status": "pending",
            "testStrategy": "Unit tests: call emitDocs with null (should produce no output), call emitDocs with empty string, call emitDocs with simple text, verify 'docs: |-' key written with correct indentation, verify indent level increases and decreases properly"
          },
          {
            "id": 2,
            "title": "Implement markdown content emission with proper block scalar formatting",
            "description": "Add logic to emit the markdown content as a YAML literal block scalar, preserving formatting and handling multi-line content correctly",
            "dependencies": [
              1
            ],
            "details": "Extend emitDocs in src/yaml_emitter.zig to handle markdown content:\n```zig\n// After increaseIndent() in emitDocs:\n// Split content by lines to emit each with proper indentation\nvar lines = std.mem.split(u8, docs_content, \"\\n\");\nwhile (lines.next()) |line| {\n    try self.writeIndent();\n    // Write line content as-is (block scalar preserves formatting)\n    try self.writer.writeAll(line);\n    try self.writer.writeAll(\"\\n\");\n}\n```\nThe literal block scalar (|-) preserves all formatting including blank lines, code blocks, and markdown syntax. Each line of the docs content must be indented according to YAML block scalar rules (2 spaces more than the 'docs:' key). Empty lines are preserved to maintain markdown structure (important for paragraphs, lists, code blocks). No escaping is needed within block scalars - markdown special characters like #, *, -, etc. are literal.",
            "status": "pending",
            "testStrategy": "Unit tests: single-line markdown, multi-line markdown with paragraphs, markdown with code blocks (triple backticks), markdown with lists and headings, markdown with blank lines between sections, verify indentation consistency, verify no character escaping occurs, verify trailing newlines handled correctly"
          },
          {
            "id": 3,
            "title": "Implement emitComment function for YAML comment emission",
            "description": "Create the emitComment function to emit Bru comments as YAML comments when --keep-comments mode is enabled",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, add the emitComment function:\n```zig\npub fn emitComment(self: *YamlEmitter, comment: []const u8) !void {\n    // Emit YAML comment at current indentation level\n    try self.writeIndent();\n    try self.writer.writeAll(\"# \");\n    try self.writer.writeAll(comment);\n    try self.writer.writeAll(\"\\n\");\n}\n```\nYAML comments start with '#' followed by a space (per YAML style guide). The comment text is emitted as-is without any escaping - YAML comments continue to end of line and don't require special handling. This function will be called before emitting associated keys to preserve the position hint from the parse phase (stored during parsing with --keep-comments flag). Comments are metadata and don't affect the structure, but they provide valuable context when converting from Bru format.",
            "status": "pending",
            "testStrategy": "Unit tests: emit single comment, emit comment with special characters, emit empty comment, verify '# ' prefix added, verify proper indentation, test comment at various indent levels, verify newline termination"
          },
          {
            "id": 4,
            "title": "Integrate comment position tracking with key emission",
            "description": "Add logic to emit comments before their associated keys using position hints preserved from parse phase",
            "dependencies": [
              3
            ],
            "details": "Modify key emission functions in src/yaml_emitter.zig to check for associated comments:\n```zig\n// Add optional comment parameter to writeKey or create wrapper:\npub fn writeKeyWithComment(self: *YamlEmitter, key: []const u8, comment: ?[]const u8) !void {\n    // Emit comment before key if present\n    if (comment) |c| {\n        try self.emitComment(c);\n    }\n    \n    // Then emit key as normal\n    try self.writeKey(key);\n}\n```\nComment position hints are stored during parsing (see task 9) with line associations. When emitting each key (like 'method:', 'url:', 'headers:', etc.), check if there's an associated comment from the parse phase. Comments should be emitted on the line immediately before their associated key to maintain the visual relationship. This requires the emitter to have access to the comment store (passed via ConvertOptions with keep_comments flag). The integration point is in all emit functions that write keys (emitHttp, emitHeaders, emitParams, etc.).",
            "status": "pending",
            "testStrategy": "Unit tests: emit key with associated comment, emit key without comment, verify comment appears on line before key, verify indentation matches key indentation, test with multiple keys and comments, integration test with full request showing comment preservation throughout structure"
          },
          {
            "id": 5,
            "title": "Add comprehensive tests for docs and comment edge cases",
            "description": "Create integration tests covering all docs and comment scenarios including complex markdown, missing docs, and comment positioning",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "In src/yaml_emitter.zig test block, add comprehensive tests:\n```zig\ntest \"emitDocs with complex markdown\" {\n    // Test with markdown containing: headings, lists, code blocks, links, bold/italic\n    // Verify all markdown preserved exactly including special chars (#, *, _, etc.)\n}\n\ntest \"emitDocs null handling\" {\n    // Test with null docs - should produce no output at all\n    // Verify no 'docs:' key appears in output\n}\n\ntest \"emitComment with special characters\" {\n    // Test comments containing: colons, quotes, braces, YAML special chars\n    // Verify all treated as literal text in comment\n}\n\ntest \"comment positioning integration\" {\n    // Test complete request with comments before various keys\n    // Verify each comment appears on line immediately before associated key\n    // Verify indentation matches key indentation\n}\n\ntest \"docs as last section\" {\n    // Test complete OpenCollectionRequest emission\n    // Verify docs section appears after info, http, runtime, settings\n    // Parse output to confirm section ordering\n}\n```\nTests should use std.testing.allocator, capture output to buffer, compare against expected YAML strings. Verify complete format matches OpenCollection spec.",
            "status": "pending",
            "testStrategy": "Integration tests covering: complex markdown preservation (code blocks, lists, headings, links), null docs (no output), empty docs, very long docs content, comment with special YAML characters, comment positioning before keys, multiple comments throughout document, docs appearing as final section in complete YAML output, verify YAML validity with block scalar syntax"
          }
        ]
      },
      {
        "id": 32,
        "title": "Implement complete YAML emitter orchestration",
        "description": "Integrate all YAML emission functions into complete OpenCollection to YAML converter",
        "details": "Complete `src/yaml_emitter.zig`:\n- `emit(allocator: Allocator, request: OpenCollectionRequest) ![]const u8`\n- Create buffered writer with ArrayList backend\n- Emit sections in order: info, http, runtime, settings (if present), docs\n- Blank line between major sections\n- Proper indentation throughout\n- Return owned slice from ArrayList\n- Handle write errors gracefully\n- Option to preserve comments from parse phase\nEntry point for all YAML emission.",
        "testStrategy": "Integration tests with complete OpenCollectionRequest, validate YAML syntax with external parser, round-trip testing",
        "priority": "high",
        "dependencies": [
          25,
          26,
          27,
          28,
          29,
          30,
          31
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create emit function signature and ArrayList backend",
            "description": "Define the main emit function with proper signature and set up buffered writer using ArrayList(u8) as backend for accumulating YAML output",
            "dependencies": [],
            "details": "In src/yaml_emitter.zig, implement the public emit function:\n\n```zig\npub fn emit(allocator: std.mem.Allocator, request: OpenCollectionRequest) ![]const u8 {\n    var output = std.ArrayList(u8).init(allocator);\n    errdefer output.deinit();\n    \n    var writer = output.writer();\n    // Subsequent subtasks will use this writer\n    \n    return output.toOwnedSlice();\n}\n```\n\nThis establishes the entry point for complete YAML emission. The ArrayList provides a growable buffer that accumulates all YAML output. Using toOwnedSlice() transfers ownership of the buffer to the caller, who becomes responsible for freeing it with allocator.free(). The errdefer ensures cleanup if any emission step fails.",
            "status": "pending",
            "testStrategy": "Unit test: call emit with minimal OpenCollectionRequest, verify function returns non-empty slice, verify memory ownership transfer works correctly, test with std.testing.allocator to detect leaks, verify errdefer triggers on artificial errors"
          },
          {
            "id": 2,
            "title": "Implement section emission orchestration in correct order",
            "description": "Call emitInfo, emitHttp, emitRuntime, emitSettings (if present), and emitDocs in the OpenCollection-specified order",
            "dependencies": [
              1
            ],
            "details": "Within the emit function from subtask 1, orchestrate all section emission functions:\n\n```zig\n// Always emit required sections\ntry emitInfo(&writer, request.info);\ntry writer.writeAll(\"\\n\");\ntry emitHttp(&writer, request.http);\n\n// Emit optional runtime section if present\nif (request.runtime) |runtime| {\n    try writer.writeAll(\"\\n\");\n    try emitRuntime(&writer, runtime);\n}\n\n// Emit optional settings if present\nif (request.settings) |settings| {\n    try writer.writeAll(\"\\n\");\n    try emitSettings(&writer, settings);\n}\n\n// Emit optional docs section if present (always last)\nif (request.docs) |docs| {\n    try writer.writeAll(\"\\n\");\n    try emitDocs(&writer, docs);\n}\n```\n\nThis follows the OpenCollection YAML structure specification: info and http are required and appear first, followed by optional runtime, settings, and docs sections. Each section is separated by a blank line for readability.",
            "status": "pending",
            "testStrategy": "Integration tests: emit complete request with all sections present, emit minimal request (info+http only), emit request with runtime but no settings, emit request with docs, verify section order in output, verify blank lines between sections, verify optional sections correctly skipped when null"
          },
          {
            "id": 3,
            "title": "Implement YamlEmitter struct wrapper for stateful emission",
            "description": "Create YamlEmitter struct to encapsulate writer and indentation state, refactor emit function to use it",
            "dependencies": [
              1
            ],
            "details": "The individual emission functions (emitInfo, emitHttp, etc.) from tasks 25-31 expect to be methods on a YamlEmitter struct that tracks indentation state. Create this struct in src/yaml_emitter.zig:\n\n```zig\nconst YamlEmitter = struct {\n    writer: std.ArrayList(u8).Writer,\n    indent_level: usize,\n    \n    fn init(writer: std.ArrayList(u8).Writer) YamlEmitter {\n        return .{ .writer = writer, .indent_level = 0 };\n    }\n    \n    fn increaseIndent(self: *YamlEmitter) void {\n        self.indent_level += 1;\n    }\n    \n    fn decreaseIndent(self: *YamlEmitter) void {\n        if (self.indent_level > 0) self.indent_level -= 1;\n    }\n    \n    fn writeIndent(self: *YamlEmitter) !void {\n        const spaces = self.indent_level * 2;\n        var i: usize = 0;\n        while (i < spaces) : (i += 1) {\n            try self.writer.writeByte(' ');\n        }\n    }\n};\n```\n\nRefactor the emit function to create a YamlEmitter instance and pass it to section emission functions as methods.",
            "status": "pending",
            "testStrategy": "Unit tests: verify YamlEmitter init, test increaseIndent/decreaseIndent modify indent_level correctly, test writeIndent outputs correct spaces at levels 0,1,2,3, test underflow protection in decreaseIndent, integration test with complete emission to verify indentation consistency throughout output"
          },
          {
            "id": 4,
            "title": "Add error handling and validation for write operations",
            "description": "Implement comprehensive error handling for write failures and validate section emission results",
            "dependencies": [
              2,
              3
            ],
            "details": "Enhance the emit function with robust error handling:\n\n```zig\npub fn emit(allocator: std.mem.Allocator, request: OpenCollectionRequest) ![]const u8 {\n    var output = std.ArrayList(u8).init(allocator);\n    errdefer output.deinit();\n    \n    var emitter = YamlEmitter.init(output.writer());\n    \n    // Wrap each emission in error handling context\n    emitter.emitInfo(request.info) catch |err| {\n        std.log.err(\"Failed to emit info section: {}\", .{err});\n        return err;\n    };\n    \n    try emitter.writer.writeAll(\"\\n\");\n    \n    emitter.emitHttp(request.http) catch |err| {\n        std.log.err(\"Failed to emit http section: {}\", .{err});\n        return err;\n    };\n    \n    // Similar error handling for optional sections\n    // ...\n    \n    // Validate output is not empty\n    if (output.items.len == 0) {\n        return error.EmptyOutput;\n    }\n    \n    return output.toOwnedSlice();\n}\n```\n\nThis ensures write errors are caught with diagnostic context, and validates the output meets basic requirements before returning.",
            "status": "pending",
            "testStrategy": "Unit tests: test emission with artificial write errors (mock writer that fails), verify error.EmptyOutput for degenerate input, verify error messages logged correctly, test error propagation from each section emission function, verify errdefer cleanup on failures"
          },
          {
            "id": 5,
            "title": "Add comment preservation support and integration tests",
            "description": "Implement optional comment preservation from parse phase and create comprehensive integration tests validating complete YAML output",
            "dependencies": [
              4
            ],
            "details": "Add comment preservation capability to the emit function:\n\n1. Extend OpenCollectionRequest to include optional comment metadata:\n```zig\n// In OpenCollectionRequest struct\ncomments: ?[]CommentInfo = null,\n```\n\n2. Pass comment context to YamlEmitter:\n```zig\nconst YamlEmitter = struct {\n    // ... existing fields\n    comments: ?[]CommentInfo,\n    \n    fn emitCommentIfPresent(self: *YamlEmitter, location: CommentLocation) !void {\n        if (self.comments) |comment_list| {\n            for (comment_list) |comment| {\n                if (comment.location == location) {\n                    try self.writeIndent();\n                    try self.writer.print(\"# {}\\n\", .{comment.text});\n                }\n            }\n        }\n    }\n};\n```\n\n3. Create integration tests:\n- Test complete request with all sections (info, http, runtime, settings, docs)\n- Validate YAML syntax using external parser (if available)\n- Round-trip testing: parse emitted YAML to verify correctness\n- Test comment preservation in --keep-comments mode\n- Test large requests with many headers/params/scripts\n- Verify proper indentation throughout complex nested structures\n- Test all body types, auth types, runtime scripts, etc.",
            "status": "pending",
            "testStrategy": "Integration tests: emit complete OpenCollectionRequest with all section types, validate output with YAML parser (std.yaml or external tool), round-trip test (emit -> parse -> compare), test comment preservation at various locations (before sections, before keys, inline where allowed), test edge cases (empty arrays, null optionals, special characters in strings), performance test with large requests (100+ headers), verify final output matches OpenCollection spec examples"
          }
        ]
      },
      {
        "id": 33,
        "title": "Implement filesystem walker for directory traversal",
        "description": "Create recursive directory walker to discover .bru files with optional recursion",
        "details": "Create `src/fs_walker.zig`:\n- `walkDirectory(allocator: Allocator, path: []const u8, recursive: bool) ![][]const u8`\n- Use std.fs.Dir.iterate() for directory traversal\n- Filter for .bru extension\n- Recursive: descend into subdirectories\n- Non-recursive: only top-level files\n- Return array of absolute file paths\n- Handle permissions errors gracefully\n- Skip hidden directories (. prefix) by default\n- Maintain relative path structure for output mapping\nUse ArrayList to accumulate paths.",
        "testStrategy": "Unit tests with test fixtures: flat directory, nested directories, recursive/non-recursive modes, permission errors, no .bru files",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create fs_walker.zig file structure and basic types",
            "description": "Set up the new fs_walker.zig module with necessary imports, type definitions, and function signatures",
            "dependencies": [],
            "details": "Create src/fs_walker.zig with:\n- Import std library (std.fs, std.mem, std.ArrayList, std.Allocator)\n- Define function signature: pub fn walkDirectory(allocator: Allocator, path: []const u8, recursive: bool) ![][]const u8\n- Add basic documentation comments explaining the function purpose\n- Set up error set for potential errors (AccessDenied, InvalidPath, etc.)\n- Prepare the structure for the main implementation\nThis establishes the foundation for the walker implementation.",
            "status": "pending",
            "testStrategy": "Verify file compiles without errors using zig build, ensure function signature is correct"
          },
          {
            "id": 2,
            "title": "Implement non-recursive directory iteration with .bru filtering",
            "description": "Add logic to scan a single directory level and collect .bru files",
            "dependencies": [
              1
            ],
            "details": "In walkDirectory function:\n- Open directory using std.fs.cwd().openDir(path, .{ .iterate = true })\n- Create ArrayList([]const u8) to accumulate file paths\n- Use dir.iterate() to get directory iterator\n- For each entry: check if entry.kind == .file\n- Filter files by checking if name ends with \".bru\" using std.mem.endsWith\n- For matching files, construct absolute path by joining directory path with filename\n- Duplicate path string using allocator.dupe() and append to ArrayList\n- Skip hidden directories (names starting with '.')\nImplements the core flat directory scanning functionality.",
            "status": "pending",
            "testStrategy": "Unit test with flat test directory containing .bru and non-.bru files, verify only .bru files returned, verify hidden files skipped"
          },
          {
            "id": 3,
            "title": "Add recursive subdirectory traversal support",
            "description": "Extend the walker to recursively descend into subdirectories when recursive=true",
            "dependencies": [
              2
            ],
            "details": "Enhance walkDirectory implementation:\n- When recursive=true and entry.kind == .directory, recursively call walkDirectory on subdirectory\n- Skip hidden directories (check if directory name starts with '.')\n- Use std.fs.path.join to construct subdirectory paths correctly\n- Merge results from recursive calls into main ArrayList\n- Maintain relative path structure by tracking current depth/path\n- Handle circular symlinks by checking entry.kind carefully\n- Ensure memory from recursive calls is properly managed\nEnables deep directory tree traversal for finding all .bru files.",
            "status": "pending",
            "testStrategy": "Unit tests with nested directory structures (3+ levels deep), verify all .bru files found in recursive mode, verify only top-level found in non-recursive mode, test hidden directory skipping"
          },
          {
            "id": 4,
            "title": "Implement error handling for filesystem operations",
            "description": "Add graceful error handling for permission errors, invalid paths, and I/O failures",
            "dependencies": [
              3
            ],
            "details": "Add comprehensive error handling:\n- Catch std.fs.Dir.OpenError for directory access failures\n- Handle permission denied errors: log warning and skip directory instead of failing entirely\n- Validate input path exists before attempting to open\n- Handle std.fs.File.OpenError for individual files\n- Return meaningful errors for non-existent paths or non-directory paths\n- Use error union return type: ![][]const u8\n- Add error context with std.debug.print for debugging\n- Ensure ArrayList.deinit() called on error paths to prevent leaks\nMakes the walker robust against real-world filesystem issues.",
            "status": "pending",
            "testStrategy": "Unit tests: non-existent directory, file instead of directory, permission-denied directory (mock), verify graceful degradation, ensure no memory leaks on error paths using std.testing.allocator"
          },
          {
            "id": 5,
            "title": "Add path normalization and result finalization",
            "description": "Convert accumulated paths to absolute form and return owned slice",
            "dependencies": [
              4
            ],
            "details": "Finalize the walker implementation:\n- Convert all collected paths to absolute paths using std.fs.cwd().realpathAlloc()\n- Ensure consistent path separators across platforms\n- Call ArrayList.toOwnedSlice() to transfer ownership to caller\n- Document that caller owns returned memory and must free with allocator.free()\n- Sort results alphabetically for deterministic output using std.mem.sort\n- Deduplicate any duplicate paths that might arise from symlinks\n- Add function-level documentation with usage examples\nProvides clean, usable API with proper memory ownership semantics.",
            "status": "pending",
            "testStrategy": "Integration test: verify returned paths are absolute, verify memory ownership transfer (no leaks), test path sorting, verify caller can successfully free memory, test with ArrayList pattern matching existing codebase (src/main.zig:12)"
          }
        ]
      },
      {
        "id": 34,
        "title": "Implement CLI argument parser",
        "description": "Parse command-line arguments for bru2oc CLI with flag handling and validation",
        "details": "Add to `src/main.zig`:\n- Use std.process.args() to get arguments\n- `CliArgs` struct: path, recursive, delete, output_dir, dry_run, verbose, keep_comments\n- Parse flags: -r/--recursive, -d/--delete, -o/--output, --dry-run, -v/--verbose, --keep-comments, -h/--help, --version\n- Positional argument: <path>\n- Validate: path is required, output_dir is optional\n- Handle combined short flags: -rd\n- Help text formatting with usage examples\n- Version string from build.zig.zon\n- Error on unknown flags\n- Return parsed CliArgs struct",
        "testStrategy": "Unit tests: various flag combinations, long/short flags, invalid flags, missing path, help/version",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define CliArgs struct and argument types",
            "description": "Create the data structure to hold parsed command-line arguments with all required fields and types",
            "dependencies": [],
            "details": "In src/main.zig, define the CliArgs struct with fields: path (?[]const u8), recursive (bool), delete (bool), output_dir (?[]const u8), dry_run (bool), verbose (bool), keep_comments (bool). Initialize default values: all bools to false, optional strings to null. Add a deinit() method if needed for cleanup.",
            "status": "pending",
            "testStrategy": "Unit test: verify struct initialization with default values, test field access"
          },
          {
            "id": 2,
            "title": "Implement basic argument iterator and flag recognition",
            "description": "Create argument parsing loop using std.process.args() with flag detection logic",
            "dependencies": [
              1
            ],
            "details": "Use std.process.argsAlloc() to get arguments. Implement parseArgs() function that iterates through args. Recognize long flags (--recursive, --delete, --output, --dry-run, --verbose, --keep-comments, --help, --version) and short flags (-r, -d, -o, -v, -h). Track positional argument for <path>. Handle unknown flags by returning error.UnknownFlag.",
            "status": "pending",
            "testStrategy": "Unit tests: parse single long flag, parse single short flag, detect unknown flag, parse positional argument"
          },
          {
            "id": 3,
            "title": "Add combined short flag parsing and flag value handling",
            "description": "Support combined short flags like -rd and handle flags that take values like -o/--output",
            "dependencies": [
              2
            ],
            "details": "Detect combined short flags (e.g., -rd, -rdv) and expand them into individual flags (-r -d or -r -d -v). For flags that take values (--output/-o), check if next argument exists and doesn't start with '-'. Store value in CliArgs.output_dir. Return error.MissingValue if value flag has no argument. Return error.InvalidValue for malformed inputs.",
            "status": "pending",
            "testStrategy": "Unit tests: parse -rd as recursive+delete, parse -o <path>, parse --output=<path>, error on -o without value, parse -rdv combined"
          },
          {
            "id": 4,
            "title": "Implement help text and version display",
            "description": "Create formatted help message with usage examples and version string from build.zig.zon",
            "dependencies": [
              1
            ],
            "details": "Define printHelp() function that outputs usage: 'bru2oc [options] <path>', list all flags with descriptions, show examples ('bru2oc -r ./bruno', 'bru2oc -rd -o ./output ./input.bru'). Define printVersion() that reads version from build.zig.zon (currently '0.0.0') and prints 'bru2oc version 0.0.0'. When --help/-h detected, call printHelp() and exit. When --version detected, call printVersion() and exit.",
            "status": "pending",
            "testStrategy": "Integration test: capture stdout for --help, verify help text format, capture --version output, verify version string"
          },
          {
            "id": 5,
            "title": "Add argument validation and error handling",
            "description": "Validate required arguments and handle all error cases with clear error messages",
            "dependencies": [
              2,
              3
            ],
            "details": "After parsing, validate that path is provided (error.PathRequired if missing). Validate output_dir is valid if provided. Return populated CliArgs struct on success. Define custom error types: ParseError with variants PathRequired, UnknownFlag, MissingValue, InvalidValue. For each error, print clear message to stderr before returning ('Error: path argument is required', 'Error: unknown flag --xyz', 'Error: --output requires a value').",
            "status": "pending",
            "testStrategy": "Unit tests: missing path returns error.PathRequired, unknown flag shows error message, all validation rules tested, verify error messages printed to stderr"
          }
        ]
      },
      {
        "id": 35,
        "title": "Implement file validation and path resolution",
        "description": "Validate input paths and resolve output paths with directory structure preservation",
        "details": "Add to `src/main.zig`:\n- `validatePath(path: []const u8) !PathInfo` where PathInfo = struct { absolute: []const u8, is_file: bool, is_dir: bool }\n- Use std.fs.cwd().statFile() to check existence\n- Convert to absolute path\n- Validate .bru extension for single files\n- Error if path doesn't exist\n- `resolveOutputPath(input: []const u8, output_dir: ?[]const u8, base_dir: []const u8) ![]const u8`\n- Preserve relative directory structure when using --output\n- Replace .bru extension with .yml\n- Create output directories as needed\nPath handling uses std.fs.path utilities.",
        "testStrategy": "Unit tests: file validation, directory validation, non-existent paths, output path resolution with/without --output flag, directory creation",
        "priority": "medium",
        "dependencies": [
          34
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define PathInfo struct and path validation function signature",
            "description": "Create the PathInfo struct to hold path metadata and define the validatePath function signature in src/main.zig",
            "dependencies": [],
            "details": "In `src/main.zig`, define:\n```zig\npub const PathInfo = struct {\n    absolute: []const u8,  // Absolute path to file/directory\n    is_file: bool,         // True if path points to a file\n    is_dir: bool,          // True if path points to a directory\n    \n    pub fn deinit(self: PathInfo, allocator: std.mem.Allocator) void {\n        allocator.free(self.absolute);\n    }\n};\n\npub fn validatePath(allocator: std.mem.Allocator, path: []const u8) !PathInfo\n```\nThis struct encapsulates all path metadata needed for validation. The absolute path is heap-allocated and must be freed by caller. The function signature uses error union (!PathInfo) to return validation errors.",
            "status": "pending",
            "testStrategy": "Unit tests verifying struct can be created and deinitialized, test function signature compiles with proper error union return type"
          },
          {
            "id": 2,
            "title": "Implement path existence checking and type detection",
            "description": "Implement core path validation logic using std.fs.cwd().statFile() and std.fs.openDirAbsolute() to check if paths exist and determine if they are files or directories",
            "dependencies": [
              1
            ],
            "details": "In validatePath function body:\n1. Use `std.fs.cwd().statFile(path)` to check file existence - returns error.FileNotFound if doesn't exist\n2. Check file kind with `stat.kind == .file` vs `.directory`\n3. For directories, use `std.fs.openDirAbsolute()` or `std.fs.cwd().openDir()` as fallback\n4. Populate is_file and is_dir booleans based on stat.kind\n5. Return appropriate error from error set if path doesn't exist (map to IoError.FileNotFound from errors.zig)\n\nExample:\n```zig\nconst stat = std.fs.cwd().statFile(path) catch |err| {\n    if (err == error.FileNotFound) return error.FileNotFound;\n    return err;\n};\nconst is_file = stat.kind == .file;\nconst is_dir = stat.kind == .directory;\n```",
            "status": "pending",
            "testStrategy": "Unit tests: test with existing file, existing directory, non-existent path, verify is_file and is_dir flags are set correctly, verify FileNotFound error is returned for missing paths"
          },
          {
            "id": 3,
            "title": "Implement absolute path conversion and .bru extension validation",
            "description": "Convert relative paths to absolute using std.fs.path utilities and validate .bru extension for single files",
            "dependencies": [
              2
            ],
            "details": "In validatePath function, add path normalization and extension checking:\n1. Use `std.fs.cwd().realpathAlloc(allocator, path)` to convert relative to absolute path\n2. This resolves symlinks and returns heap-allocated absolute path\n3. For files (is_file == true), validate .bru extension:\n   - Use `std.mem.endsWith(u8, absolute, \".bru\")` to check extension\n   - Return error.InvalidPath if file doesn't have .bru extension\n4. Directories don't need extension validation\n5. Store absolute path in PathInfo.absolute\n\nExample:\n```zig\nconst absolute = try std.fs.cwd().realpathAlloc(allocator, path);\nerrdefer allocator.free(absolute);\n\nif (is_file and !std.mem.endsWith(u8, absolute, \".bru\")) {\n    allocator.free(absolute);\n    return error.InvalidPath;\n}\n```",
            "status": "pending",
            "testStrategy": "Unit tests: test relative path conversion (\"./file.bru\" -> \"/full/path/file.bru\"), test .bru extension validation for files, verify directories bypass extension check, test with .txt file returns InvalidPath, verify memory properly freed on error paths using std.testing.allocator"
          },
          {
            "id": 4,
            "title": "Implement resolveOutputPath function with directory structure preservation",
            "description": "Create resolveOutputPath function that preserves relative directory structure when using --output flag and replaces .bru extension with .yml",
            "dependencies": [
              3
            ],
            "details": "In `src/main.zig`, implement:\n```zig\npub fn resolveOutputPath(\n    allocator: std.mem.Allocator,\n    input: []const u8,      // Input file path (absolute)\n    output_dir: ?[]const u8, // Optional output directory\n    base_dir: []const u8,   // Base directory to preserve structure from\n) ![]const u8\n```\nLogic:\n1. Replace .bru extension with .yml using std.mem.replacementSize and std.mem.replace\n2. If output_dir is null, replace extension in-place at input location\n3. If output_dir provided:\n   - Calculate relative path from base_dir to input using std.fs.path.relative\n   - Join output_dir with relative path using std.fs.path.join\n   - Preserve intermediate directories (e.g., base_dir=\"/project\", input=\"/project/api/users.bru\", output_dir=\"/out\" -> \"/out/api/users.yml\")\n4. Return heap-allocated output path",
            "status": "pending",
            "testStrategy": "Unit tests: test extension replacement (.bru -> .yml), test without output_dir (in-place), test with output_dir preserving structure, test nested directories (subdir/file.bru -> output/subdir/file.yml), verify memory allocated with std.testing.allocator"
          },
          {
            "id": 5,
            "title": "Implement output directory creation with error handling",
            "description": "Add logic to create output directory structure as needed using std.fs.makePath and integrate with resolveOutputPath",
            "dependencies": [
              4
            ],
            "details": "Enhance resolveOutputPath or add companion function createOutputDir:\n1. Extract directory path from resolved output path using `std.fs.path.dirname(output_path)`\n2. Use `std.fs.cwd().makePath(dir_path)` to create directory structure recursively\n3. makePath is idempotent - safe to call if directory exists\n4. Handle errors: PermissionDenied, DiskFull, etc. map to IoError.DirectoryCreationFailed\n5. Call before writing converted file\n\nExample:\n```zig\nif (std.fs.path.dirname(output_path)) |dir| {\n    std.fs.cwd().makePath(dir) catch |err| {\n        // Map to appropriate error\n        return error.DirectoryCreationFailed;\n    };\n}\n```\nIntegrate with file conversion workflow to ensure directories exist before write operations.",
            "status": "pending",
            "testStrategy": "Unit tests: test creating single directory, test creating nested directory structure (a/b/c), test with existing directory (should not error), test permission denied scenario, verify directories are actually created using std.fs.cwd().access()"
          }
        ]
      },
      {
        "id": 36,
        "title": "Implement single file conversion function",
        "description": "Core conversion logic for a single .bru file to .yml with verification",
        "details": "Add to `src/main.zig`:\n- `convertFile(allocator: Allocator, input_path: []const u8, output_path: []const u8, options: ConvertOptions) !ConversionResult`\n- ConvertOptions: keep_comments, verbose\n- ConversionResult: success (bool), error_msg (?[]const u8)\n- Read input file: std.fs.cwd().readFileAlloc()\n- Parse: call bru_parser.parse()\n- Transform: call transformer.transform()\n- Emit: call yaml_emitter.emit()\n- Write output: std.fs.cwd().createFile() and write\n- Verify: read back and basic YAML syntax check (colon presence, indentation sanity)\n- On error: preserve original, return detailed error\n- On success: return ConversionResult with success=true\nSingle-file conversion unit.",
        "testStrategy": "Unit tests with test fixtures: valid .bru files, invalid syntax, I/O errors, verification failure, verbose mode output",
        "priority": "high",
        "dependencies": [
          10,
          23,
          32,
          35
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define ConvertOptions and ConversionResult types",
            "description": "Create data structures for conversion function configuration and result reporting",
            "dependencies": [],
            "details": "In `src/main.zig`, define:\n\n1. ConvertOptions struct:\n```zig\npub const ConvertOptions = struct {\n    keep_comments: bool = false,\n    verbose: bool = false,\n};\n```\n\n2. ConversionResult struct:\n```zig\npub const ConversionResult = struct {\n    success: bool,\n    error_msg: ?[]const u8 = null,\n    \n    pub fn ok() ConversionResult {\n        return .{ .success = true };\n    }\n    \n    pub fn err(msg: []const u8) ConversionResult {\n        return .{ .success = false, .error_msg = msg };\n    }\n};\n```\n\nThese types provide the interface contract for the convertFile function, supporting comment preservation, verbose logging, and detailed error reporting.",
            "status": "pending",
            "testStrategy": "Unit tests for struct initialization, test ConversionResult.ok() and .err() helper methods, verify optional error_msg handling"
          },
          {
            "id": 2,
            "title": "Implement file reading with error handling",
            "description": "Read input .bru file content into memory with proper allocator usage and I/O error handling",
            "dependencies": [
              1
            ],
            "details": "In convertFile function implementation:\n\n1. Read input file using std.fs.cwd().readFileAlloc():\n```zig\nconst input_content = try std.fs.cwd().readFileAlloc(\n    allocator,\n    input_path,\n    std.math.maxInt(usize)\n);\ndefer allocator.free(input_content);\n```\n\n2. Handle I/O errors:\n- FileNotFound: return ConversionResult.err(\"Input file not found\")\n- AccessDenied: return ConversionResult.err(\"Permission denied reading input file\")\n- Other errors: return ConversionResult.err(\"Failed to read input file\")\n\n3. If verbose mode enabled, print: \"Reading: {s}\" with input_path\n\nUse errdefer for cleanup on error paths to prevent memory leaks.",
            "status": "pending",
            "testStrategy": "Unit tests: read valid file, handle non-existent file, handle permission errors, verify memory cleanup with testing allocator, test verbose output"
          },
          {
            "id": 3,
            "title": "Implement parse-transform-emit pipeline",
            "description": "Orchestrate calling bru_parser, transformer, and yaml_emitter modules to convert content",
            "dependencies": [
              2
            ],
            "details": "Chain the conversion pipeline:\n\n1. Parse the input content:\n```zig\nconst parsed_doc = try bru_parser.parse(allocator, input_content);\ndefer parsed_doc.deinit(allocator);\n```\n\n2. Transform to OpenCollection IR:\n```zig\nconst oc_request = try transformer.transform(allocator, parsed_doc);\ndefer oc_request.deinit(allocator);\n```\n\n3. Emit YAML:\n```zig\nconst yaml_output = try yaml_emitter.emit(allocator, oc_request, .{ .include_comments = options.keep_comments });\ndefer allocator.free(yaml_output);\n```\n\n4. Handle errors at each stage:\n- Parse errors: return ConversionResult.err(\"Parse failed: {error}\")\n- Transform errors: return ConversionResult.err(\"Transform failed: {error}\")\n- Emit errors: return ConversionResult.err(\"YAML emit failed: {error}\")\n\n5. If verbose, log each stage completion",
            "status": "pending",
            "testStrategy": "Unit tests with mock modules: test successful pipeline, test parse failure propagation, test transform failure, test emit failure, verify memory cleanup on each error path"
          },
          {
            "id": 4,
            "title": "Implement output file writing",
            "description": "Write YAML output to target file with atomic write and error handling",
            "dependencies": [
              3
            ],
            "details": "Write the converted YAML output:\n\n1. Create output file:\n```zig\nconst output_file = try std.fs.cwd().createFile(output_path, .{});\ndefer output_file.close();\n```\n\n2. Write content:\n```zig\ntry output_file.writeAll(yaml_output);\n```\n\n3. Handle write errors:\n- PathAlreadyExists with no truncate: return ConversionResult.err(\"Output file exists\")\n- AccessDenied: return ConversionResult.err(\"Permission denied writing output file\")\n- DiskQuota/NoSpaceLeft: return ConversionResult.err(\"Insufficient disk space\")\n- Other errors: return ConversionResult.err(\"Failed to write output file\")\n\n4. If verbose mode, print: \"Writing: {s}\"\n\n5. On error, preserve original input file (do not delete/modify)",
            "status": "pending",
            "testStrategy": "Unit tests: successful write, handle write permission errors, handle disk full, verify original file untouched on errors, test verbose output, verify file content matches yaml_output"
          },
          {
            "id": 5,
            "title": "Implement YAML verification and return result",
            "description": "Read back generated YAML file and perform basic syntax validation before returning success",
            "dependencies": [
              4
            ],
            "details": "Verify the written output:\n\n1. Read back the output file:\n```zig\nconst verify_content = try std.fs.cwd().readFileAlloc(\n    allocator,\n    output_path,\n    std.math.maxInt(usize)\n);\ndefer allocator.free(verify_content);\n```\n\n2. Basic YAML syntax checks:\n- Check for at least one colon (key-value separator)\n- Verify no tabs in indentation (YAML forbids tabs)\n- Check indentation is consistent (multiples of 2 spaces)\n- Verify content length > 0\n\n3. Validation failures:\n- Return ConversionResult.err(\"Verification failed: {reason}\")\n- On verification failure, optionally delete malformed output\n\n4. On success:\n- If verbose, print: \"Verified: {s}\"\n- Return ConversionResult.ok()\n\n5. This provides a safety check that the conversion actually produced valid YAML",
            "status": "pending",
            "testStrategy": "Unit tests: verify valid YAML passes, detect missing colons, detect tab characters, detect invalid indentation, verify error messages are descriptive, test verbose output"
          }
        ]
      },
      {
        "id": 37,
        "title": "Implement YAML verification function",
        "description": "Verify written .yml files are valid YAML before deleting originals",
        "details": "Add to `src/main.zig`:\n- `verifyYaml(file_path: []const u8) !bool`\n- Read .yml file content\n- Basic YAML validation: check for balanced indentation, key:value format, no syntax errors\n- Don't need full YAML parser, just syntax sanity check\n- Look for common YAML errors: tabs instead of spaces, unquoted special chars in wrong context, unbalanced brackets\n- Could use simple heuristics or integrate lightweight YAML validator\n- Return true if valid, false/error if invalid\n- Never delete original .bru if this returns false/error\nSafety check before deletion.",
        "testStrategy": "Unit tests: valid YAML, invalid YAML (syntax errors), empty files, malformed indentation",
        "priority": "medium",
        "dependencies": [
          32
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create verifyYaml function signature and file reading logic",
            "description": "Implement the basic function structure for verifyYaml with file reading capability using std.fs",
            "dependencies": [],
            "details": "Add to src/main.zig: Define function signature `pub fn verifyYaml(allocator: std.mem.Allocator, file_path: []const u8) !bool`. Use std.fs.cwd().openFile() to open the YAML file. Read file contents using file.readToEndAlloc() with allocator. Defer file.close() and defer allocator.free(content). Set up error handling for file I/O errors (FileNotFound, AccessDenied, etc.). Return true on successful read as placeholder. This establishes the foundation for YAML validation logic.",
            "status": "pending",
            "testStrategy": "Unit tests: test with valid file path, test with non-existent file (expect error), test with empty file (should succeed read), test with permission denied scenario, verify allocator properly frees memory on success and error paths using std.testing.allocator"
          },
          {
            "id": 2,
            "title": "Implement indentation validation checker",
            "description": "Add logic to verify YAML has consistent indentation using spaces only, no tabs, and balanced indent levels",
            "dependencies": [
              1
            ],
            "details": "In verifyYaml function after reading file content: Check for tab characters (ASCII 0x09) anywhere in content - if found, return error.YamlContainsTabs. Iterate through lines tracking indent level: count leading spaces on each line, verify indent changes are multiples of 2 (standard YAML indent). Track indent stack to ensure proper nesting: push on indent increase, pop on decrease. Verify stack balances to 0 at end. Lines with only whitespace or starting with # (comments) are ignored for indent validation. Return false if indentation is inconsistent or unbalanced.",
            "status": "pending",
            "testStrategy": "Unit tests: valid YAML with 2-space indent, YAML with tabs (should fail), YAML with inconsistent indent (3 spaces then 2 spaces - should fail), YAML with unbalanced indentation (orphaned deep indent - should fail), YAML with comments at various indent levels (should pass), empty lines (should be ignored)"
          },
          {
            "id": 3,
            "title": "Implement key-value format validation",
            "description": "Add validation for proper YAML key:value syntax including colon placement and value presence",
            "dependencies": [
              2
            ],
            "details": "After indentation check, validate key:value format: For each non-comment, non-empty line, check if it contains a colon. If colon present, verify format 'key: value' or 'key:' (for nested objects/arrays). Key must be non-empty and precede colon. Detect common errors: colon without key (line starts with ':'), multiple colons without quotes (ambiguous structure). For array items (lines starting with '- '), validate array syntax. Check for unquoted special characters in wrong context: unquoted '{', '}', '[', ']' at start of value (should be quoted). Return false if syntax violations detected.",
            "status": "pending",
            "testStrategy": "Unit tests: valid key:value pairs, nested objects with proper colons, array syntax with '- ' prefix, error on ':value' without key, error on 'key' without colon (non-nested context), error on unquoted braces/brackets at value start, quoted special chars (should pass), list items with values"
          },
          {
            "id": 4,
            "title": "Implement bracket and quote balance validation",
            "description": "Add validation to ensure brackets, braces, and quotes are properly balanced throughout the YAML content",
            "dependencies": [
              3
            ],
            "details": "Implement bracket/brace/quote balancing: Track opening and closing characters: '{' with '}', '[' with ']', single quotes ('), double quotes (\"). Use a stack or counter for each pair type. Parse through content character by character, respecting quote context (ignore brackets inside quoted strings). For quotes: track if inside single or double quoted string, handle escaped quotes (\\' and \\\"). For brackets/braces: increment on open, decrement on close, verify count returns to 0 at end. Return false if any imbalance detected (unclosed quote, unmatched bracket, etc.). Skip validation inside comments (lines starting with #).",
            "status": "pending",
            "testStrategy": "Unit tests: balanced quotes and brackets, unmatched opening brace (should fail), unmatched closing bracket (should fail), unclosed double quote (should fail), unclosed single quote (should fail), escaped quotes inside strings (should pass), nested brackets (should pass), brackets inside quoted strings (should pass and be ignored), comments with unbalanced chars (should be ignored)"
          },
          {
            "id": 5,
            "title": "Add comprehensive error handling and integration",
            "description": "Complete error handling, add detailed error messages, and integrate verifyYaml into conversion workflow with safety checks",
            "dependencies": [
              4
            ],
            "details": "Define error set in src/main.zig: YamlError with variants YamlContainsTabs, YamlInvalidIndentation, YamlInvalidSyntax, YamlUnbalancedBrackets, YamlUnbalancedQuotes. Update verifyYaml to return YamlError!bool instead of generic !bool. Add line number tracking to error returns for better diagnostics. Integrate into convertFile function: after writing YAML file, call verifyYaml on the output file path. If verification returns false or error, log error with file path and line number, DO NOT delete original .bru file, return error to user. Only proceed with deletion if verifyYaml returns true. Add verbose logging option to show 'YAML verification passed for: file.yml'.",
            "status": "pending",
            "testStrategy": "Integration tests: convertFile with valid YAML output (verification passes, .bru deleted), convertFile with invalid YAML output (verification fails, .bru preserved), error message includes file path and line number, verbose mode logs verification success, test all error variants (tabs, indentation, syntax, brackets, quotes), verify original file never deleted on verification failure"
          }
        ]
      },
      {
        "id": 38,
        "title": "Implement batch conversion with error aggregation",
        "description": "Convert multiple .bru files with per-file error handling and summary reporting",
        "details": "Add to `src/main.zig`:\n- `convertBatch(allocator: Allocator, files: [][]const u8, args: CliArgs) !BatchResult`\n- BatchResult: total, converted, failed, deleted, errors ([]ConversionError)\n- Iterate through files\n- Call convertFile for each\n- Collect errors but continue processing (fail-per-file)\n- If --delete and conversion successful and verification passed: delete original\n- Track counts: found, converted, failed, deleted\n- Return BatchResult with summary\n- Print progress in verbose mode\nBatch orchestration with error isolation.",
        "testStrategy": "Integration tests: multiple files, mix of success/failure, deletion mode, dry-run mode, verbose output",
        "priority": "high",
        "dependencies": [
          33,
          36,
          37
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define BatchResult struct and ConversionError types",
            "description": "Create data structures to track batch conversion statistics and per-file errors",
            "dependencies": [],
            "details": "In `src/main.zig`, define:\n\n```zig\npub const BatchResult = struct {\n    total: usize,\n    converted: usize,\n    failed: usize,\n    deleted: usize,\n    errors: []ConversionError,\n    \n    pub fn init(allocator: std.mem.Allocator) BatchResult {\n        return .{\n            .total = 0,\n            .converted = 0,\n            .failed = 0,\n            .deleted = 0,\n            .errors = &.{},\n        };\n    }\n    \n    pub fn deinit(self: *BatchResult, allocator: std.mem.Allocator) void {\n        allocator.free(self.errors);\n    }\n};\n\npub const ConversionError = struct {\n    file_path: []const u8,\n    error_message: []const u8,\n};\n```\n\nThese structures provide type-safe tracking of conversion outcomes and error details for each file in the batch.",
            "status": "pending",
            "testStrategy": "Unit tests: create BatchResult, verify initialization, test adding errors to array, verify deinit cleans up memory with std.testing.allocator"
          },
          {
            "id": 2,
            "title": "Implement convertBatch function signature and iteration",
            "description": "Create the main batch conversion function that iterates through input files",
            "dependencies": [
              1
            ],
            "details": "In `src/main.zig`, implement the convertBatch function:\n\n```zig\npub fn convertBatch(allocator: Allocator, files: [][]const u8, args: CliArgs) !BatchResult {\n    var result = BatchResult.init(allocator);\n    errdefer result.deinit(allocator);\n    \n    result.total = files.len;\n    var errors = std.ArrayList(ConversionError).init(allocator);\n    defer errors.deinit();\n    \n    for (files) |file_path| {\n        // Per-file processing in next subtask\n    }\n    \n    result.errors = try errors.toOwnedSlice();\n    return result;\n}\n```\n\nThis establishes the batch processing structure with proper memory management and error collection. Uses ArrayList for dynamic error accumulation.",
            "status": "pending",
            "testStrategy": "Unit tests: call convertBatch with empty file list, single file, multiple files, verify BatchResult.total equals input count, test memory cleanup with std.testing.allocator"
          },
          {
            "id": 3,
            "title": "Integrate convertFile calls with per-file error handling",
            "description": "Call convertFile for each input file and collect errors without stopping batch processing",
            "dependencies": [
              2
            ],
            "details": "Within the convertBatch for-loop, add per-file conversion with error isolation:\n\n```zig\nfor (files) |file_path| {\n    if (args.verbose) {\n        std.debug.print(\"Converting: {s}\\n\", .{file_path});\n    }\n    \n    // Call convertFile (from task 33)\n    convertFile(allocator, file_path, args) catch |err| {\n        // Capture error but continue processing\n        try errors.append(.{\n            .file_path = file_path,\n            .error_message = try std.fmt.allocPrint(allocator, \"{}\", .{err}),\n        });\n        result.failed += 1;\n        continue;\n    };\n    \n    result.converted += 1;\n    // Deletion logic in next subtask\n}\n```\n\nThis implements fail-per-file behavior where one file's error doesn't prevent processing remaining files.",
            "status": "pending",
            "testStrategy": "Integration tests: mix of valid and invalid .bru files, verify all files attempted, verify failed count matches errors, test verbose output, verify error messages captured correctly"
          },
          {
            "id": 4,
            "title": "Implement conditional file deletion after successful conversion",
            "description": "Delete original .bru files when --delete flag is set and conversion succeeds",
            "dependencies": [
              3
            ],
            "details": "After successful convertFile call, add deletion logic:\n\n```zig\nresult.converted += 1;\n\n// Delete original if --delete flag set and conversion succeeded\nif (args.delete) {\n    std.fs.cwd().deleteFile(file_path) catch |err| {\n        // Log deletion failure but don't fail the batch\n        if (args.verbose) {\n            std.debug.print(\"Warning: Failed to delete {s}: {}\\n\", .{file_path, err});\n        }\n        // Note: Don't increment result.deleted on failure\n        continue;\n    };\n    result.deleted += 1;\n    if (args.verbose) {\n        std.debug.print(\"Deleted: {s}\\n\", .{file_path});\n    }\n}\n```\n\nDeletion only occurs after verified successful conversion. Deletion failures are logged but don't fail the batch.",
            "status": "pending",
            "testStrategy": "Integration tests: --delete with successful conversions, verify files deleted, test deletion permission failures, dry-run mode prevents deletion, count deleted files correctly"
          },
          {
            "id": 5,
            "title": "Implement batch summary reporting and verbose progress output",
            "description": "Print comprehensive batch results including totals, success/failure counts, and error details",
            "dependencies": [
              4
            ],
            "details": "At the end of convertBatch, add summary output:\n\n```zig\n// Print summary\nconst stderr = std.io.getStdErr().writer();\ntry stderr.print(\"\\n=== Batch Conversion Summary ===\\n\", .{});\ntry stderr.print(\"Total files: {}\\n\", .{result.total});\ntry stderr.print(\"Converted: {}\\n\", .{result.converted});\ntry stderr.print(\"Failed: {}\\n\", .{result.failed});\nif (args.delete) {\n    try stderr.print(\"Deleted: {}\\n\", .{result.deleted});\n}\n\nif (result.errors.len > 0) {\n    try stderr.print(\"\\nErrors:\\n\", .{});\n    for (result.errors) |conversion_error| {\n        try stderr.print(\"  {s}: {s}\\n\", .{conversion_error.file_path, conversion_error.error_message});\n    }\n}\n```\n\nProvides clear feedback on batch operation outcome with detailed error information.",
            "status": "pending",
            "testStrategy": "Integration tests: verify summary printed to stderr, check counts match actual results, test error list formatting, verify verbose mode prints progress during conversion, test with no errors vs multiple errors"
          }
        ]
      },
      {
        "id": 39,
        "title": "Implement dry-run mode",
        "description": "Preview conversion actions without writing files or deleting originals",
        "details": "Add to `src/main.zig`:\n- Modify convertFile to accept dry_run flag\n- When dry_run=true: skip write operations, skip delete operations\n- Still perform parse and transform to detect errors\n- Print what would happen: 'Would convert: input.bru → output.yml'\n- Print 'Would delete: input.bru' if --delete specified\n- Return success/error as normal (errors still detected)\n- Summary shows what would be done\nSafe preview mode.",
        "testStrategy": "Unit tests: dry-run with valid files, dry-run with errors, verify no writes/deletes occur",
        "priority": "low",
        "dependencies": [
          36
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add dry_run boolean parameter to convertFile function signature",
            "description": "Modify the convertFile function in src/main.zig to accept a dry_run boolean flag parameter",
            "dependencies": [],
            "details": "Update the convertFile function signature in src/main.zig to include a dry_run parameter:\n- Change signature from `fn convertFile(allocator: Allocator, input_path: []const u8, output_path: []const u8, delete_original: bool)` to `fn convertFile(allocator: Allocator, input_path: []const u8, output_path: []const u8, delete_original: bool, dry_run: bool)`\n- This parameter will be threaded through the conversion pipeline to control whether actual file operations occur\n- The dry_run flag does not affect parsing or transformation logic, only I/O operations\n- Update all existing convertFile call sites to pass dry_run value from command-line args",
            "status": "pending",
            "testStrategy": "Unit tests verifying function accepts dry_run parameter, integration tests calling convertFile with dry_run=true and dry_run=false"
          },
          {
            "id": 2,
            "title": "Implement conditional write operation logic in convertFile",
            "description": "Add logic to skip YAML file write operations when dry_run flag is true while still performing parse and transform validation",
            "dependencies": [
              1
            ],
            "details": "In src/main.zig convertFile function:\n- After successful transformation to YAML, check if dry_run is true before writing output file\n- If dry_run=true: print 'Would convert: {input_path} → {output_path}' to stdout using std.debug.print or std.io.getStdOut().writer()\n- If dry_run=false: proceed with normal file write using std.fs.cwd().createFile() or equivalent\n- Ensure parse and transform steps still execute to detect errors even in dry-run mode\n- Parse errors, transform errors, and other validation failures should still be reported as normal error returns\n- This allows users to validate their Bru files without side effects",
            "status": "pending",
            "testStrategy": "Unit tests: dry_run=true with valid file (no write, prints message), dry_run=false with valid file (writes file), dry_run=true with invalid Bru (returns error, no write), verify std.fs file system not modified in dry-run mode"
          },
          {
            "id": 3,
            "title": "Implement conditional delete operation logic in convertFile",
            "description": "Add logic to skip original file deletion when dry_run flag is true, with preview message for what would be deleted",
            "dependencies": [
              1
            ],
            "details": "In src/main.zig convertFile function:\n- After successful conversion, check both delete_original and dry_run flags before deleting source file\n- If delete_original=true AND dry_run=true: print 'Would delete: {input_path}' to stdout, do not delete file\n- If delete_original=true AND dry_run=false: delete the original Bru file using std.fs.cwd().deleteFile()\n- If delete_original=false: skip deletion regardless of dry_run value\n- Ensure deletion logic only triggers after successful conversion (no errors)\n- This prevents accidental data loss during dry-run previews",
            "status": "pending",
            "testStrategy": "Unit tests: dry_run=true with delete_original=true (prints message, file not deleted), dry_run=false with delete_original=true (file deleted), dry_run=true with delete_original=false (no message, no deletion), verify file system state with std.testing.tmpDir()"
          },
          {
            "id": 4,
            "title": "Add --dry-run command-line flag parsing to main function",
            "description": "Extend command-line argument parsing to recognize and handle --dry-run flag, storing result in configuration",
            "dependencies": [],
            "details": "In src/main.zig main function:\n- Add --dry-run (and optionally -n as short form) to command-line argument parser\n- Parse flag using std.process.argsAlloc() or existing arg parsing logic\n- Store dry_run boolean in configuration/options struct or pass directly to convertFile calls\n- Default value should be false (normal operation) if flag not specified\n- Add help text describing --dry-run: 'Preview conversion actions without writing files or deleting originals'\n- Ensure --dry-run flag works in combination with --delete flag\n- Update usage/help message to document new flag",
            "status": "pending",
            "testStrategy": "Integration tests: parse args with --dry-run flag, parse args without flag (defaults to false), test combined --dry-run --delete flags, verify help text includes --dry-run documentation"
          },
          {
            "id": 5,
            "title": "Implement dry-run summary output reporting",
            "description": "Add summary reporting at end of conversion showing what actions would be performed in dry-run mode versus what was actually done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "In src/main.zig after conversion loop completes:\n- Track counts: files_would_convert, files_would_delete (dry-run) or files_converted, files_deleted (normal mode)\n- After all conversions, print summary based on dry_run flag:\n  - If dry_run=true: 'Dry run complete. Would convert X files. Would delete Y files. No changes made.'\n  - If dry_run=false: 'Conversion complete. Converted X files. Deleted Y files.'\n- Include error count: 'Z files failed with errors.'\n- Print to stdout for visibility\n- Errors still counted and reported normally in both modes\n- Summary provides clear indication of dry-run vs actual operation mode",
            "status": "pending",
            "testStrategy": "Integration tests: dry-run mode with multiple files (verify summary shows 'would' language), normal mode with multiple files (verify summary shows actual counts), mixed success/error scenarios in both modes, verify counts accurate"
          }
        ]
      },
      {
        "id": 40,
        "title": "Implement verbose logging and progress reporting",
        "description": "Detailed progress output for debugging and monitoring conversion process",
        "details": "Add to `src/main.zig`:\n- Global verbose flag from CLI args\n- Log functions: logInfo, logDebug, logError with conditional output\n- Progress reporting: 'Processing 1/10: request.bru'\n- Detailed error messages with file path and line number\n- Parse warnings (unknown blocks, deprecated syntax)\n- Timing information in verbose mode\n- Use std.debug.print with color coding (optional)\n- Summary statistics: files processed, time elapsed, success rate\nEnhanced user feedback.",
        "testStrategy": "Unit tests: verbose output capture, non-verbose silence, error formatting, progress calculation",
        "priority": "low",
        "dependencies": [
          38
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add CLI verbose flag and global logging configuration",
            "description": "Implement command-line argument parsing to accept --verbose or -v flag and create global logging configuration state",
            "dependencies": [],
            "details": "In src/main.zig, add CLI argument parsing using std.process.argsAlloc to accept --verbose or -v flag. Create a LogConfig struct with fields: verbose (bool), use_colors (bool), start_time (i64). Initialize logging configuration from CLI args in main() function. Store config in a global or pass through function parameters. Add helper function to detect if terminal supports colors (check isatty for stdout).",
            "status": "pending",
            "testStrategy": "Unit tests: parse --verbose flag, parse -v flag, verify default is non-verbose, test color detection on different terminal types"
          },
          {
            "id": 2,
            "title": "Implement core logging functions with conditional output",
            "description": "Create logInfo, logDebug, and logError functions that respect verbose flag and support optional color coding",
            "dependencies": [
              1
            ],
            "details": "In src/main.zig, create logging module with functions: logInfo(config: LogConfig, comptime fmt: []const u8, args: anytype), logDebug (only outputs if verbose=true), logError (always outputs to stderr). Implement ANSI color codes: blue for info, gray for debug, red for error. Use std.debug.print for output. Add timestamp formatting in verbose mode using std.time. Each log function should check config.verbose and config.use_colors before formatting output.",
            "status": "pending",
            "testStrategy": "Unit tests: capture stderr/stdout, verify logDebug is silent when verbose=false, verify colors are applied when use_colors=true, verify logError always outputs"
          },
          {
            "id": 3,
            "title": "Implement progress reporting with file counting",
            "description": "Add progress tracking that displays 'Processing X/Y: filename.bru' messages during conversion",
            "dependencies": [
              2
            ],
            "details": "Create ProgressTracker struct with fields: total_files (usize), current_file (usize), files_processed ([]const u8). Add methods: init(total: usize), reportProgress(config: LogConfig, filename: []const u8). In main conversion loop, count total .bru files first, then call reportProgress before processing each file. Format output as 'Processing 1/10: request.bru' using logInfo. Track successful and failed conversions separately for summary statistics.",
            "status": "pending",
            "testStrategy": "Unit tests: verify progress output format, test with 1 file, test with multiple files, verify counter increments correctly"
          },
          {
            "id": 4,
            "title": "Add detailed error messages with file path and line number tracking",
            "description": "Enhance error reporting to include file path, line number, and parse warnings for unknown blocks or deprecated syntax",
            "dependencies": [
              2
            ],
            "details": "Create DetailedError struct with fields: file_path ([]const u8), line (usize), column (usize), message ([]const u8), severity (enum: Error, Warning). Implement formatError function that outputs: '[ERROR] file.bru:42:5: Invalid syntax'. Add parseWarning function for non-fatal issues like unknown blocks or deprecated syntax. Warnings should only display in verbose mode. Integrate with parser to capture position information. Create error context tracking that accumulates warnings during parse phase.",
            "status": "pending",
            "testStrategy": "Unit tests: error formatting with line/column, warning suppression in non-verbose mode, multiple warnings accumulation, verify stderr output"
          },
          {
            "id": 5,
            "title": "Implement timing information and summary statistics reporting",
            "description": "Add timing measurements in verbose mode and final summary showing files processed, time elapsed, and success rate",
            "dependencies": [
              3,
              4
            ],
            "details": "Use std.time.Timer to measure: total execution time, per-file processing time (verbose only). In verbose mode, log 'Processed file.bru in 15ms' after each file. Create SummaryStats struct: total_files, successful, failed, warnings_count, elapsed_ms. Implement printSummary function that outputs: 'Summary: 8/10 files converted successfully, 2 failed, 5 warnings, completed in 1.2s'. Calculate success rate as percentage. Display summary at program exit using logInfo. Format elapsed time as human-readable (ms, s, or m:s).",
            "status": "pending",
            "testStrategy": "Unit tests: timer accuracy verification, summary formatting with various stats, success rate calculation, time formatting (ms/s/m:s), verify summary always displays even on partial failure"
          }
        ]
      },
      {
        "id": 41,
        "title": "Implement help and version commands",
        "description": "Display usage information and version number",
        "details": "Add to `src/main.zig`:\n- `printHelp() void` - format usage text with all flags and examples\n- `printVersion() void` - extract version from build.zig.zon and print\n- Help text includes: synopsis, description, options table, examples\n- Examples from PRD: single file, directory, recursive, dry-run\n- Exit code 0 for both help and version\n- Triggered by -h, --help, --version\nUser documentation in CLI.",
        "testStrategy": "Manual testing: verify help text completeness, version number accuracy, formatting",
        "priority": "low",
        "dependencies": [
          34
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define command-line flag constants and structures",
            "description": "Create data structures to represent command-line flags for help and version options in src/main.zig",
            "dependencies": [],
            "details": "Add flag constants and enums to src/main.zig:\n- Define boolean flags for -h, --help, and --version\n- Create a CommandLineArgs struct to hold parsed flag values\n- Add hasHelpFlag and hasVersionFlag helper functions\n- These structures will be used by the argument parser to detect when help or version output is needed\n- Follow Zig conventions for naming (snake_case for functions, PascalCase for types)",
            "status": "pending",
            "testStrategy": "Unit tests: verify flag detection logic, ensure struct can hold expected values"
          },
          {
            "id": 2,
            "title": "Implement printVersion function",
            "description": "Create function to extract version from build.zig.zon and display it to stdout",
            "dependencies": [
              1
            ],
            "details": "Add printVersion() void function to src/main.zig:\n- Read version field from build.zig.zon at compile time using @embedFile or similar\n- Parse .version = \"0.0.0\" line from the .zon file\n- Print to stdout in format: 'bru2oc version 0.0.0'\n- Use std.debug.print or std.io.getStdOut().writer()\n- Handle case where version cannot be determined (fallback to 'unknown')\n- Exit with code 0 after printing",
            "status": "pending",
            "testStrategy": "Manual testing: run with --version flag, verify output matches build.zig.zon version; test version parsing logic"
          },
          {
            "id": 3,
            "title": "Create help text content and formatting",
            "description": "Design and implement the complete help text with synopsis, description, options, and examples",
            "dependencies": [
              1
            ],
            "details": "Add printHelp() void function to src/main.zig with formatted help text:\n- Synopsis: 'Usage: bru2oc [OPTIONS] <INPUT>'\n- Description: 'Convert Bruno API client files (.bru) to OpenCollection format (.yml)'\n- Options table with all flags from PRD task 34:\n  * -h, --help: Display this help message\n  * --version: Display version information\n  * -o, --output: Output file/directory path\n  * -r, --recursive: Process directories recursively\n  * --dry-run: Preview without writing files\n  * --delete: Delete original .bru files after conversion\n  * --keep-comments: Preserve comments in output\n- Examples section with 4 examples from PRD:\n  * Single file: bru2oc request.bru\n  * Directory: bru2oc -o output/ input/\n  * Recursive: bru2oc -r collection/\n  * Dry-run: bru2oc --dry-run --delete request.bru\n- Use proper indentation and alignment\n- Exit with code 0 after printing",
            "status": "pending",
            "testStrategy": "Manual testing: verify all sections present, check formatting/alignment, ensure examples match PRD specifications"
          },
          {
            "id": 4,
            "title": "Integrate help and version flags into argument parser",
            "description": "Modify main() to check for help/version flags early and call appropriate print functions",
            "dependencies": [
              2,
              3
            ],
            "details": "Update main() function in src/main.zig:\n- Parse command-line arguments using std.process.args()\n- Check for -h, --help flags first (before other validation)\n- Check for --version flag\n- If help flag detected: call printHelp(), then return (exit code 0)\n- If version flag detected: call printVersion(), then return (exit code 0)\n- Help/version take precedence over all other flags and validation errors\n- Continue to normal processing if neither flag present\n- Both flags should work standalone or combined with other flags",
            "status": "pending",
            "testStrategy": "Unit tests: verify flag detection precedence, ensure help/version bypass other validation; Integration tests: test with various flag combinations"
          },
          {
            "id": 5,
            "title": "Add comprehensive manual tests for help and version output",
            "description": "Create test suite to verify help and version commands work correctly with various invocations",
            "dependencies": [
              4
            ],
            "details": "Manual testing checklist:\n- Test bru2oc -h (short help flag)\n- Test bru2oc --help (long help flag)\n- Test bru2oc --version\n- Test bru2oc --help --version (help takes precedence)\n- Test bru2oc --help invalid_args (help still shows)\n- Test bru2oc --version invalid_args (version still shows)\n- Verify exit code 0 in all cases\n- Verify help text completeness: all sections present, examples correct\n- Verify version number matches build.zig.zon exactly\n- Verify formatting is readable and well-aligned\n- Test on different terminal widths if relevant\n- Document any issues found and iterate on printHelp/printVersion",
            "status": "pending",
            "testStrategy": "Manual testing as described in details; create automated shell script tests if time permits to verify exit codes and output format"
          }
        ]
      },
      {
        "id": 42,
        "title": "Implement main CLI orchestration and exit codes",
        "description": "Integrate all components into complete CLI application with proper error handling and exit codes",
        "details": "Complete `src/main.zig`:\n- `main() !void` entry point\n- Parse CLI args\n- Handle help/version early exit\n- Validate paths\n- Discover files (single or directory walk)\n- Error if no .bru files found\n- Call convertBatch\n- Print summary: X files found, Y converted, Z failed, W deleted\n- Exit codes: 0 = all success, 1 = some failed (partial success), 2 = bad args or no files\n- Use std.process.exit() for exit codes\n- Proper memory cleanup with defer and allocator.deinit\n- Global error handler for panic/unhandled errors\nComplete CLI application.",
        "testStrategy": "Integration tests: end-to-end CLI with various argument combinations, exit code verification, memory leak detection",
        "priority": "high",
        "dependencies": [
          38,
          39,
          40,
          41
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement CLI argument parsing with std.process.argsAlloc",
            "description": "Parse command-line arguments to extract flags and path, validating input early",
            "dependencies": [],
            "details": "In src/main.zig, create a Config struct to hold parsed options: path ([]const u8), recursive (bool), delete (bool), output (?[]const u8), dry_run (bool), verbose (bool), keep_comments (bool), help (bool), version (bool). Implement parseArgs() function using std.process.argsAlloc(allocator) to iterate through arguments. Recognize long flags (--recursive, --delete, --output, --dry-run, --verbose, --keep-comments, --help, --version) and short flags (-r, -d, -o, -v, -h). Extract the positional path argument. Return error.UnknownFlag for unrecognized options, error.MissingPath if no path provided. Handle --help and --version as special early-exit cases by setting flags in Config.",
            "status": "pending",
            "testStrategy": "Unit tests: parse valid flag combinations, test short and long flags, verify error.UnknownFlag for invalid flags, test error.MissingPath when path omitted, validate Config struct populated correctly"
          },
          {
            "id": 2,
            "title": "Implement help/version display and early exit handling",
            "description": "Display help message or version information when requested, then exit cleanly",
            "dependencies": [
              1
            ],
            "details": "In src/main.zig, implement printHelp() and printVersion() functions. printHelp() outputs usage information: 'Usage: bru2oc [options] <path>', lists all flags with descriptions (--recursive/-r, --delete/-d, --output/-o, --dry-run, --verbose/-v, --keep-comments, --help/-h, --version), and exit codes (0=success, 1=partial failure, 2=invalid args). printVersion() outputs 'bru2oc version 1.0.0' or reads from build.zig metadata. In main(), check Config.help or Config.version flags immediately after parseArgs(). If set, call appropriate print function and return normally (exit code 0). Use std.io.getStdOut().writer() for output. These are not errors, so don't use stderr.",
            "status": "pending",
            "testStrategy": "Integration tests: invoke with --help, verify help text printed and exit code 0, invoke with --version, verify version string and exit code 0, test -h and -v short forms"
          },
          {
            "id": 3,
            "title": "Implement file discovery with single file and directory walk support",
            "description": "Find all .bru files from the provided path, supporting both single files and recursive directory traversal",
            "dependencies": [
              1
            ],
            "details": "In src/main.zig, implement discoverFiles(allocator: Allocator, path: []const u8, recursive: bool) ![][]const u8 function. First validate path exists using std.fs.cwd().statFile() - return error.FileNotFound if missing. Check if path is a file or directory using .kind field from stat result. If file: verify it ends with '.bru' extension, return single-element slice. If directory: use std.fs.cwd().openDir() then iterate with dir.walk() if recursive=true or dir.iterate() if recursive=false. Collect all entries ending with '.bru' into ArrayList([]const u8). Use allocator.dupe() to copy paths into stable memory. Return error.NoFilesFound if resulting slice is empty. Handle permission errors gracefully.",
            "status": "pending",
            "testStrategy": "Unit tests: single .bru file returns one path, directory with recursive=false finds only top-level, recursive=true finds nested files, non-.bru files filtered out, error.FileNotFound for invalid path, error.NoFilesFound when no .bru files exist"
          },
          {
            "id": 4,
            "title": "Implement batch conversion orchestration and result tracking",
            "description": "Process all discovered files through the conversion pipeline and track success/failure counts",
            "dependencies": [
              3
            ],
            "details": "In src/main.zig, implement convertBatch(allocator: Allocator, files: [][]const u8, config: Config) !ConversionResults function. Define ConversionResults struct with fields: total (usize), converted (usize), failed (usize), deleted (usize). Iterate through files slice, calling convertFile(allocator, file_path, config) for each (this function will be implemented in dependencies from task 38, 39, 40, 41). Wrap convertFile in error handling - catch and log errors but continue processing. Increment appropriate counters: converted on success, failed on error. If config.delete=true and conversion succeeded and config.dry_run=false, call std.fs.cwd().deleteFile() and increment deleted counter. Accumulate all errors into an ArrayList for reporting. Return ConversionResults populated with final counts.",
            "status": "pending",
            "testStrategy": "Integration tests: batch with all successful conversions, batch with some failures (verify partial success), verify delete counter increments when --delete used, dry-run mode skips deletes, error accumulation works correctly"
          },
          {
            "id": 5,
            "title": "Implement main entry point with summary output and exit code logic",
            "description": "Wire all components together in main() with proper memory management, error handling, and exit codes",
            "dependencies": [
              2,
              4
            ],
            "details": "In src/main.zig, implement complete main() !void function. Use std.heap.GeneralPurposeAllocator for leak detection: var gpa = std.heap.GeneralPurposeAllocator(.{}){}; defer _ = gpa.deinit(); const allocator = gpa.allocator();. Call parseArgs() with defer args.deinit(allocator) for cleanup. Handle early exits for help/version from subtask 2. Call discoverFiles() to get file list with defer allocator.free(files). Print 'Found X .bru files' to stdout. Call convertBatch() to process files. Print summary: 'Converted Y files successfully, Z failed' and 'Deleted W original files' if delete enabled. Determine exit code: 0 if failed=0 (all success), 1 if failed>0 and converted>0 (partial), 2 if failed=total or bad args. Call std.process.exit(exit_code) explicitly. Wrap entire main in catch to handle panics and unhandled errors - print error to stderr and exit 2.",
            "status": "pending",
            "testStrategy": "Integration tests: end-to-end CLI execution with various scenarios (all success, partial failure, total failure, no files found, invalid args), verify exit codes match specification (0/1/2), memory leak detection with GeneralPurposeAllocator in test mode, test proper cleanup on error paths"
          }
        ]
      },
      {
        "id": 43,
        "title": "Create test fixture corpus",
        "description": "Develop comprehensive test fixtures covering all Bru language features and edge cases",
        "details": "Create `test/fixtures/` directory structure:\n- Subdirectories: primitives/, composites/, multistrings/, annotations/, auth/, body/, scripts/, edge_cases/\n- Each .bru file paired with expected .yml output\n- Primitives: null, booleans, numbers (int, float, large, scientific), quoted/unquoted strings\n- Composites: multimaps, arrays, nested structures, duplicate keys\n- Multistrings: basic, nested quotes, indentation variations, trailing newlines\n- Annotations: @disabled, @description, @enum, @file, custom annotations\n- Auth: all types (bearer, basic, oauth2, awsv4, digest)\n- Body: all types (json, xml, text, form-urlencoded, multipart-form, graphql)\n- Scripts: pre-request, post-response, tests\n- Edge cases: empty values, tilde-disabled, comments, special chars, number fidelity\n- Draw from Bruno docs examples and bru-spec-v1 examples\nComprehensive test data set.",
        "testStrategy": "Validate fixtures themselves: ensure .bru files are valid per spec, .yml files are valid YAML and match OpenCollection schema",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test fixture directory structure and primitives fixtures",
            "description": "Set up the test/fixtures/ directory hierarchy and create comprehensive primitive type test cases with paired .bru and .yml files",
            "dependencies": [],
            "details": "Create directory structure:\n- test/fixtures/primitives/ for basic types\n- test/fixtures/composites/ for complex structures\n- test/fixtures/multistrings/ for multi-line strings\n- test/fixtures/annotations/ for metadata\n- test/fixtures/auth/ for authentication types\n- test/fixtures/body/ for request body types\n- test/fixtures/scripts/ for pre-request and post-response scripts\n- test/fixtures/edge_cases/ for corner cases\n\nCreate primitives fixtures in test/fixtures/primitives/:\n- null.bru + null.yml: null values in various contexts\n- boolean.bru + boolean.yml: true/false values\n- numbers_int.bru + numbers_int.yml: integers (positive, negative, zero, large values)\n- numbers_float.bru + numbers_float.yml: floating point (decimals, scientific notation)\n- string_unquoted.bru + string_unquoted.yml: unquoted string values\n- string_quoted.bru + string_quoted.yml: quoted strings with special characters\n- string_empty.bru + string_empty.yml: empty string handling\n\nEach .yml file contains the expected OpenCollection YAML output that should be generated from parsing and transforming the corresponding .bru file.",
            "status": "pending",
            "testStrategy": "Validate that each .bru file is syntactically valid according to Bru spec, validate each .yml file is valid YAML and conforms to OpenCollection schema structure"
          },
          {
            "id": 2,
            "title": "Create composites and multistrings test fixtures",
            "description": "Develop test fixtures for complex data structures (multimaps, arrays, nested structures) and multi-line string handling",
            "dependencies": [
              1
            ],
            "details": "Create composite structure fixtures in test/fixtures/composites/:\n- multimap_simple.bru + multimap_simple.yml: basic multimap structures\n- multimap_nested.bru + multimap_nested.yml: nested multimap hierarchies\n- multimap_duplicate_keys.bru + multimap_duplicate_keys.yml: duplicate key handling\n- array_primitives.bru + array_primitives.yml: arrays of primitive values\n- array_nested.bru + array_nested.yml: nested array structures\n- composite_mixed.bru + composite_mixed.yml: mixed composites with arrays and multimaps\n\nCreate multistring fixtures in test/fixtures/multistrings/:\n- multistring_basic.bru + multistring_basic.yml: simple multi-line strings\n- multistring_nested_quotes.bru + multistring_nested_quotes.yml: multistrings with quote characters\n- multistring_indentation.bru + multistring_indentation.yml: various indentation patterns\n- multistring_trailing_newlines.bru + multistring_trailing_newlines.yml: newline preservation\n- multistring_special_chars.bru + multistring_special_chars.yml: special characters in multistrings\n\nEnsure each fixture tests edge cases like empty multistrings, indentation edge cases, and proper whitespace handling.",
            "status": "pending",
            "testStrategy": "Validate complex nested structures parse correctly, verify multistring indentation and whitespace is preserved accurately, check YAML output matches expected format"
          },
          {
            "id": 3,
            "title": "Create annotations and authentication test fixtures",
            "description": "Build comprehensive test fixtures for all Bru annotation types and authentication methods",
            "dependencies": [
              1
            ],
            "details": "Create annotation fixtures in test/fixtures/annotations/:\n- disabled.bru + disabled.yml: @disabled annotation usage\n- description.bru + description.yml: @description annotation\n- enum.bru + enum.yml: @enum annotation for restricted values\n- file.bru + file.yml: @file annotation for file references\n- custom_annotations.bru + custom_annotations.yml: custom annotation handling\n- annotation_combined.bru + annotation_combined.yml: multiple annotations on single element\n\nCreate authentication fixtures in test/fixtures/auth/:\n- auth_bearer.bru + auth_bearer.yml: Bearer token authentication\n- auth_basic.bru + auth_basic.yml: Basic authentication with username/password\n- auth_oauth2.bru + auth_oauth2.yml: OAuth2 authentication flow\n- auth_awsv4.bru + auth_awsv4.yml: AWS Signature V4\n- auth_digest.bru + auth_digest.yml: Digest authentication\n- auth_none.bru + auth_none.yml: requests without authentication\n- auth_inherit.bru + auth_inherit.yml: inherited authentication settings\n\nInclude edge cases like disabled auth blocks and auth with annotations.",
            "status": "pending",
            "testStrategy": "Verify all annotation types are correctly parsed and transformed to OpenCollection format, validate each auth type's specific fields are properly mapped, check that tilde-disabled (~) auth blocks are handled"
          },
          {
            "id": 4,
            "title": "Create request body and scripts test fixtures",
            "description": "Generate test fixtures covering all request body types and script blocks (pre-request, post-response, tests)",
            "dependencies": [
              1
            ],
            "details": "Create body type fixtures in test/fixtures/body/:\n- body_json.bru + body_json.yml: JSON request body\n- body_xml.bru + body_xml.yml: XML request body\n- body_text.bru + body_text.yml: plain text body\n- body_form_urlencoded.bru + body_form_urlencoded.yml: form URL-encoded data\n- body_multipart_form.bru + body_multipart_form.yml: multipart form data\n- body_graphql.bru + body_graphql.yml: GraphQL query body\n- body_none.bru + body_none.yml: requests without body (GET, DELETE)\n- body_disabled.bru + body_disabled.yml: tilde-disabled body blocks\n\nCreate script fixtures in test/fixtures/scripts/:\n- script_pre_request.bru + script_pre_request.yml: pre-request script blocks\n- script_post_response.bru + script_post_response.yml: post-response script blocks\n- script_tests.bru + script_tests.yml: test/assertion script blocks\n- script_combined.bru + script_combined.yml: requests with multiple script types\n- script_multiline.bru + script_multiline.yml: scripts with complex multi-line code\n\nInclude examples with JavaScript code snippets from Bruno documentation.",
            "status": "pending",
            "testStrategy": "Validate all body types are correctly identified and their content is preserved, verify script blocks maintain code integrity including whitespace, ensure GraphQL and multipart bodies handle special formatting"
          },
          {
            "id": 5,
            "title": "Create edge cases and comprehensive integration fixtures",
            "description": "Develop edge case fixtures and complete request examples combining multiple features from Bruno spec and real-world usage",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create edge case fixtures in test/fixtures/edge_cases/:\n- empty_values.bru + empty_values.yml: empty headers, params, body\n- tilde_disabled.bru + tilde_disabled.yml: tilde (~) prefix for disabled blocks\n- comments.bru + comments.yml: comment handling (# syntax)\n- special_chars.bru + special_chars.yml: special characters in keys/values (colons, brackets, quotes)\n- number_fidelity.bru + number_fidelity.yml: large integers, floating point precision\n- unicode.bru + unicode.yml: Unicode characters in various fields\n- whitespace_variations.bru + whitespace_variations.yml: tabs vs spaces, trailing whitespace\n- minimal_request.bru + minimal_request.yml: absolute minimum valid request\n- maximal_request.bru + maximal_request.yml: request using every possible Bru feature\n\nSource examples from:\n- Bruno official documentation (https://docs.usebruno.com/)\n- bru-spec-v1 repository examples\n- Real-world Bruno collection files\n\nCreate integration fixtures combining features:\n- complete_get_request.bru + complete_get_request.yml: GET with headers, params, auth\n- complete_post_request.bru + complete_post_request.yml: POST with JSON body, auth, scripts\n- complex_workflow.bru + complex_workflow.yml: request with annotations, scripts, auth, all features combined\n\nEnsure total coverage of at least 40-50 fixture pairs.",
            "status": "pending",
            "testStrategy": "Validate edge cases handle gracefully without errors, verify maximal_request exercises every code path in parser, confirm all fixtures combined provide complete spec coverage, cross-reference with bru-spec-v1 to ensure no features missed"
          }
        ]
      },
      {
        "id": 44,
        "title": "Implement parser unit tests",
        "description": "Comprehensive unit tests for all parser components using test fixtures",
        "details": "Add tests to `src/bru_parser.zig`:\n- Use Zig's test blocks and std.testing\n- Test tokenizer: all token types, position tracking, CRLF normalization\n- Test primitive parsers: null, bool, number, quoted/unquoted strings, edge cases\n- Test annotation parser: valid/invalid names, arguments, position\n- Test multistring parser: indentation, nested quotes, trailing newlines\n- Test array parser: primitives, multistrings, nested arrays\n- Test multimap parser: simple, nested, duplicate keys, disabled entries\n- Test comment handler: valid comments, inline # error\n- Test complete parser: load fixtures, compare parsed IR to expectations\n- Test error cases: unclosed blocks, bad indentation, syntax errors\n- Memory leak detection with std.testing.allocator\nExhaustive parser testing.",
        "testStrategy": "Run zig build test, verify all test fixtures parse correctly, ensure error cases fail appropriately, check for memory leaks",
        "priority": "high",
        "dependencies": [
          10,
          43
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test infrastructure and fixtures directory",
            "description": "Set up test fixture directory structure and helper utilities for parser testing",
            "dependencies": [],
            "details": "Create directory `test/fixtures/` for storing Bru test files and expected IR outputs. Implement test helper functions in parser test file:\n- `loadFixture(path: []const u8) ![]const u8` to read fixture files\n- `expectParsedIR(source: []const u8, expected: BruDocument) !void` to compare parsed results\n- `expectParseError(source: []const u8, expected_error: ParseError) !void` to validate error cases\n- Set up fixture files for: simple values (null, bool, number, string), complex structures (arrays, multimaps, multistrings), annotations, comments, and various error cases\n- Use std.testing.allocator for memory leak detection in all tests\nEstablish foundation for comprehensive parser testing with reusable fixtures and assertions.",
            "status": "pending",
            "testStrategy": "Verify fixture loading works, test helper functions with known inputs, confirm std.testing.allocator detects leaks in test examples"
          },
          {
            "id": 2,
            "title": "Implement tokenizer and primitive parser tests",
            "description": "Write comprehensive tests for tokenizer and all primitive value parsers (null, bool, number, string)",
            "dependencies": [
              1
            ],
            "details": "Add test blocks to `src/bru_parser.zig` using Zig's `test` keyword:\n\nTokenizer tests:\n- Test all token types: LBrace, RBrace, LBracket, RBracket, Colon, String, Number, Bool, Null, Newline, Comment, Annotation, TripleQuote, Eof\n- Verify position tracking (line, column) accuracy across multiline input\n- Test CRLF normalization: input with \\r\\n should parse identically to \\n\n- Test peek() lookahead without consuming tokens\n\nPrimitive parser tests:\n- Null: 'null' keyword parsing\n- Bool: 'true' and 'false' parsing\n- Number: integers, floats, scientific notation (1, -42, 3.14, 1e10, -2.5e-3), preserve original string representation\n- Quoted strings: single quotes, double quotes, escape sequences (\\n, \\t, \\\", \\')\n- Unquoted strings: valid identifiers, keywords vs identifiers disambiguation\n- Edge cases: empty strings, very large numbers, unicode in strings\n\nUse std.testing.expect, expectEqual, expectError for assertions.",
            "status": "pending",
            "testStrategy": "Run `zig build test`, verify all primitive types parse correctly, ensure position tracking matches expected line/column, confirm memory safety with std.testing.allocator"
          },
          {
            "id": 3,
            "title": "Implement annotation and multistring parser tests",
            "description": "Create tests for annotation parsing and multistring (triple-quoted) value parsing with indentation handling",
            "dependencies": [
              2
            ],
            "details": "Add test blocks for annotation and multistring parsing:\n\nAnnotation parser tests:\n- Valid annotations: @disabled, @description(\"text\"), @enum(1, 2, 3)\n- Annotation name validation: valid identifiers [_a-zA-Z][-_a-zA-Z0-9]*\n- Annotation arguments: primitives only (null, bool, number, string), no nested structures\n- Position validation: annotations must be at line start\n- Invalid cases: @123invalid, @name[nested], annotation mid-line\n\nMultistring parser tests:\n- Basic multistring: triple quotes (''' or \"\"\") with content\n- Indentation handling: content indented relative to opening quote, indentation stripped consistently\n- Nested quotes: single/double quotes inside multistring without escaping\n- Trailing newlines: preserve or strip according to spec\n- Edge cases: empty multistring, multistring with only whitespace, deeply indented content\n- Error cases: unclosed multistring, inconsistent indentation\n\nVerify correct IR structures created (Annotation and Value.multistring).",
            "status": "pending",
            "testStrategy": "Test valid annotations parse to correct IR with name and args, verify multistrings preserve content with proper indentation stripping, test error cases return appropriate ParseError variants"
          },
          {
            "id": 4,
            "title": "Implement array and multimap parser tests",
            "description": "Write tests for composite structure parsing including arrays and nested multimaps",
            "dependencies": [
              3
            ],
            "details": "Add test blocks for array and multimap parsing:\n\nArray parser tests:\n- Primitive arrays: [1, 2, 3], [true, false, null], [\"a\", \"b\", \"c\"]\n- Mixed type arrays: [1, \"two\", true, null]\n- Nested arrays: [[1, 2], [3, 4]], [[[1]]]\n- Arrays containing multistrings: ['''text''', \"other\"]\n- Empty array: []\n- Whitespace and newlines: arrays split across multiple lines\n- Error cases: unclosed bracket, missing comma, trailing comma handling\n\nMultimap parser tests:\n- Simple multimap: {key1: value1, key2: value2}\n- Nested multimaps: {outer: {inner: value}}\n- Complex values: {arr: [1,2], str: '''text'''}\n- Disabled entries: ~key: value or key with @disabled annotation\n- Duplicate keys: verify handling according to spec (last wins or error)\n- Implicit top-level multimap: entire .bru file without braces\n- Error cases: unclosed brace, missing colon, invalid key format\n\nTest Entry and Value IR structures accurately represent parsed data.",
            "status": "pending",
            "testStrategy": "Verify arrays and multimaps parse to correct IR structures (Value.array, Value.multimap), test nesting works correctly, ensure disabled entries marked properly, validate error cases caught"
          },
          {
            "id": 5,
            "title": "Implement comment handling and complete parser integration tests",
            "description": "Add tests for comment handling, complete document parsing with fixtures, and error case validation",
            "dependencies": [
              4
            ],
            "details": "Add final test blocks for comments and full parser integration:\n\nComment handler tests:\n- Valid comments: # at line start (after whitespace)\n- Comment content: text from # to end of line\n- Comment position: must not appear after value on same line (syntax error)\n- Comments with --keep-comments flag: verify stored in parse context\n- Comments without flag: verify discarded\n- Edge cases: empty comment (#\\n), comment at EOF, multiple consecutive comments\n\nComplete parser tests with fixtures:\n- Load fixtures from test/fixtures/ directory\n- Parse complete .bru files with all features: meta, http, headers, body, auth, etc.\n- Compare parsed IR to expected structure (BruDocument with correct entries)\n- Test realistic request files with multiple blocks and annotations\n\nError case tests:\n- Unclosed blocks: {without}, [without], '''without closing'''\n- Bad indentation: multistring content not properly indented\n- Syntax errors: missing colons, invalid tokens, malformed numbers\n- Position tracking: verify error diagnostic shows correct file:line:column\n\nMemory leak detection:\n- Run all tests with std.testing.allocator\n- Verify arena allocator cleanup in BruDocument.deinit()\n- Test with valgrind or Zig's leak detection\n\nComplete parser testing ensuring production readiness.",
            "status": "pending",
            "testStrategy": "Run `zig build test`, verify all fixtures parse correctly with expected IR, ensure error cases fail with appropriate ParseError and diagnostic info, confirm no memory leaks detected by std.testing.allocator"
          }
        ]
      },
      {
        "id": 45,
        "title": "Implement transformer unit tests",
        "description": "Comprehensive unit tests for IR to OpenCollection transformation",
        "details": "Add tests to `src/transformer.zig`:\n- Test each transform function individually: meta, method, headers, params, body, auth, scripts, assertions, vars, docs\n- Test annotation resolution: @disabled, @description, @enum, custom\n- Test complete transformation: load parsed IR from fixtures, transform, verify OpenCollection structure\n- Test error cases: multiple methods, contradictory blocks, invalid annotations\n- Test edge cases: missing optional blocks, empty arrays, disabled entries\n- Verify field mappings match OpenCollection spec\n- Memory management with std.testing.allocator\nComplete transformer coverage.",
        "testStrategy": "Run zig build test, verify all fixtures transform correctly, validate OpenCollection structure, check error handling",
        "priority": "high",
        "dependencies": [
          23,
          43
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test structure and individual transform function unit tests",
            "description": "Implement unit tests for each individual transform function in transformer.zig",
            "dependencies": [],
            "details": "Create comprehensive unit tests for each transform function:\n- transformMeta: test metadata extraction (name, version, type)\n- transformHttpMethod: test method parsing and validation (GET, POST, PUT, DELETE, PATCH, etc.)\n- transformHeaders: test header multimap to array conversion with enabled/disabled states\n- transformParams: test query parameter transformation with all annotation types\n- transformBody: test all body types (json, xml, text, multipart, form-urlencoded)\n- transformAuth: test authentication block transformation (basic, bearer, api-key)\n- transformScripts: test pre-request and post-response script extraction\n- transformAssertions: test assertion block transformation\n- transformVars: test variable collection (request and environment scopes)\n- transformDocs: test documentation field extraction\n\nUse std.testing.allocator for memory management in all tests. Each function should have positive test cases with valid inputs and verify correct OpenCollection structure output.",
            "status": "pending",
            "testStrategy": "Run `zig build test` to execute all unit tests, verify each transform function produces correct OpenCollection structure, check memory is properly managed with testing allocator"
          },
          {
            "id": 2,
            "title": "Implement annotation resolution test cases",
            "description": "Test all annotation types and their resolution in transformer",
            "dependencies": [
              1
            ],
            "details": "Create comprehensive annotation resolution tests:\n- @disabled annotation: verify sets enabled=false on entries (headers, params, etc.)\n- @description annotation: test description text extraction and population\n- @enum annotation: verify enum array creation with valid values\n- @file annotation: test multipart file handling\n- @content-type annotation: test MIME type assignment\n- Custom/unknown annotations: verify preservation in entry.annotations array\n- Annotation argument validation: test correct argument count and types\n- Annotation application: verify annotations only applied to valid entry types\n\nTest both valid annotations and error cases (invalid arguments, wrong entry types). Ensure annotations properly transform from IR to OpenCollection format.",
            "status": "pending",
            "testStrategy": "Unit tests verify annotation parsing and application, test error handling for invalid annotations, ensure custom annotations preserved correctly"
          },
          {
            "id": 3,
            "title": "Implement complete transformation integration tests with fixtures",
            "description": "Test end-to-end transformation using fixture files from task 43",
            "dependencies": [
              1,
              2
            ],
            "details": "Create integration tests for complete BruDocument to OpenCollectionRequest transformation:\n- Load parsed IR from test/fixtures/ directory (various .bru examples)\n- Call main transform() function with complete BruDocument input\n- Verify entire OpenCollectionRequest structure matches expected .yml output\n- Test all sections present: info, http, headers, params, body, auth, runtime, docs\n- Validate field mappings: ensure all BruDocument data correctly maps to OpenCollection fields\n- Test fixtures covering: simple GET requests, complex POST with body/auth, requests with scripts/assertions, requests with variables and documentation\n- Compare transformed output structure against expected OpenCollection schema\n\nUse multiple fixture files to ensure comprehensive coverage of real-world scenarios.",
            "status": "pending",
            "testStrategy": "Load IR fixtures, transform to OpenCollection, validate structure matches expected output, verify all sections populated correctly"
          },
          {
            "id": 4,
            "title": "Implement error case and edge case test coverage",
            "description": "Test transformer error handling and edge cases",
            "dependencies": [
              3
            ],
            "details": "Comprehensive error and edge case testing:\n\nError cases:\n- Multiple HTTP methods in single request (should return DuplicateMethod error)\n- Multiple body blocks (should return DuplicateBody error)\n- Multiple auth blocks (should return DuplicateAuth error)\n- Invalid annotation arguments (wrong type, count, or format)\n- Contradictory configurations (e.g., GET with body)\n- Missing required meta block\n\nEdge cases:\n- Missing optional blocks (headers, params, body, auth, scripts, assertions, vars, docs)\n- Empty arrays (no headers, no params, etc.)\n- Disabled entries (@disabled annotation applied)\n- Empty string values in fields\n- Maximum length strings\n- Unicode characters in values\n- Special characters requiring escaping\n- Nested structures at maximum depth\n\nVerify transformer handles all cases gracefully with appropriate errors or default values.",
            "status": "pending",
            "testStrategy": "Test all error paths return appropriate error types, verify edge cases handle gracefully without crashes, ensure memory safety in all failure scenarios"
          },
          {
            "id": 5,
            "title": "Verify OpenCollection spec compliance and memory management",
            "description": "Final validation of transformer output against OpenCollection specification and memory safety",
            "dependencies": [
              4
            ],
            "details": "Final comprehensive validation:\n\nOpenCollection spec compliance:\n- Verify all required fields present in output structure\n- Validate field types match OpenCollection schema (strings, arrays, objects)\n- Check field naming matches spec exactly (camelCase, naming conventions)\n- Ensure optional fields handled correctly (null vs omitted)\n- Validate enum values for fields like HTTP methods, auth types\n- Verify array structures (headers, params) match expected format\n- Test that unknown Bru blocks generate warnings but don't fail transformation\n\nMemory management:\n- All tests use std.testing.allocator for leak detection\n- Verify no memory leaks in success paths\n- Verify no memory leaks in error paths (early returns, error propagation)\n- Test allocator properly tracks all allocations/deallocations\n- Run tests with Zig's memory safety checks enabled\n\nCreate summary test that validates complete transformer meets all requirements from task description.",
            "status": "pending",
            "testStrategy": "Run `zig build test` with full test suite, verify no memory leaks detected by std.testing.allocator, validate all outputs match OpenCollection spec, confirm 100% coverage of transformer.zig functions"
          }
        ]
      },
      {
        "id": 46,
        "title": "Implement YAML emitter unit tests",
        "description": "Comprehensive unit tests for OpenCollection to YAML emission",
        "details": "Add tests to `src/yaml_emitter.zig`:\n- Test core infrastructure: indentation, string quoting, block scalars\n- Test each emit function: info, http, headers, params, body, auth, runtime, docs\n- Test complete emission: load OpenCollection from transformer, emit YAML, validate output\n- Test YAML syntax: proper indentation, quoting, block scalars, array formatting\n- Compare emitted YAML to expected fixture .yml files\n- Test edge cases: empty sections, special characters, long strings\n- Parse emitted YAML with external validator (if available) to ensure validity\nComplete emitter coverage.",
        "testStrategy": "Run zig build test, compare emitted YAML to fixture expectations, validate YAML syntax externally, verify round-trip (parse bru → transform → emit → parse yaml)",
        "priority": "high",
        "dependencies": [
          32,
          43
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core YAML infrastructure test suite",
            "description": "Create tests for fundamental YAML emission capabilities including indentation handling, string quoting rules, and block scalar formatting",
            "dependencies": [],
            "details": "Create test file `src/yaml_emitter.zig` with test blocks for core infrastructure:\n- Test indentation helper functions: verify 2-space indentation per level, correct nesting levels\n- Test string quoting logic: single quotes for simple strings, double quotes for strings with escapes, block scalars (|, |-) for multiline content\n- Test escape sequence handling: newlines, tabs, quotes, backslashes\n- Test block scalar emission: preserve line breaks, handle trailing newline control (| vs |-)\n- Test special character handling: colons, hashes, brackets in string values\nUse std.testing.expectEqualStrings to compare emitted YAML against expected output strings. Create minimal test cases focusing on infrastructure without full OpenCollection context.",
            "status": "pending",
            "testStrategy": "Run 'zig build test', verify indentation produces exact spacing, validate quoted strings escape correctly, ensure block scalars preserve formatting, test edge cases like empty strings and strings with only whitespace"
          },
          {
            "id": 2,
            "title": "Implement emit function unit tests for each OpenCollection section",
            "description": "Create comprehensive tests for individual emit functions covering info, http, headers, params, body, auth, runtime, and docs sections",
            "dependencies": [
              1
            ],
            "details": "Add test blocks to `src/yaml_emitter.zig` for each emit function:\n- emitInfo: test name, version, description fields, handle missing optional fields\n- emitHttp: test method, url, all HTTP methods (GET, POST, PUT, DELETE, PATCH, etc.)\n- emitHeaders: test header key-value pairs, array formatting, special header values\n- emitParams: test query parameters, URL encoding, array params\n- emitBody: test different body types (json, xml, form-data, raw), content-type handling\n- emitAuth: test auth types (bearer, basic, api-key, oauth2), credentials structure\n- emitRuntime: test runtime variables and scripts\n- emitDocs: test documentation section, markdown content as block scalar\nCreate OpenCollectionRequest test fixtures with each section populated. Verify YAML output matches expected structure and formatting.",
            "status": "pending",
            "testStrategy": "Unit tests for each emit function in isolation, validate YAML keys and nesting, test with minimal and fully-populated sections, verify optional fields are omitted when null/empty, check array and object formatting"
          },
          {
            "id": 3,
            "title": "Implement complete emission integration tests",
            "description": "Create end-to-end tests that load OpenCollection IR from transformer output and emit complete valid YAML documents",
            "dependencies": [
              2
            ],
            "details": "Add integration test blocks to `src/yaml_emitter.zig`:\n- Create sample OpenCollectionRequest structs representing complete HTTP requests\n- Test emitRequest function that orchestrates all section emitters\n- Verify complete YAML document structure: correct section ordering, proper nesting, consistent indentation throughout\n- Test various request types: simple GET, POST with JSON body, authenticated requests, requests with headers/params\n- Test minimal requests: only required fields (method, url)\n- Test maximal requests: all sections populated with complex data\n- Validate YAML document starts with correct OpenCollection version marker\nUse ArenaAllocator for test memory management. Emit to ArrayList(u8) buffer and verify complete output.",
            "status": "pending",
            "testStrategy": "Integration tests with complete request objects, validate emitted YAML structure from top to bottom, verify section ordering follows OpenCollection spec, test memory cleanup with testing allocator"
          },
          {
            "id": 4,
            "title": "Implement fixture-based validation tests",
            "description": "Create tests that compare emitted YAML against expected fixture .yml files to ensure output correctness",
            "dependencies": [
              3
            ],
            "details": "Extend `src/yaml_emitter.zig` with fixture comparison tests:\n- Create `fixtures/yaml/` directory with expected .yml output files\n- Create corresponding test cases that construct OpenCollectionRequest objects\n- Emit YAML to buffer and compare byte-for-byte against fixture file contents\n- Fixtures should cover: simple request, complex request with all sections, edge cases (empty arrays, null values, special characters)\n- Use std.testing.expectEqualStrings for comparison with detailed diff on mismatch\n- Test both success cases (perfect match) and intentional failures (verify test sensitivity)\n- Include fixtures for different formatting scenarios: single-line arrays, multi-line arrays, various block scalar types\nFixtures serve as regression tests and documentation of expected output format.",
            "status": "pending",
            "testStrategy": "Compare emitted YAML to fixture files using std.testing.expectEqualStrings, verify test failures show useful diffs, test with 3-5 representative fixtures covering common and edge cases, manually validate fixture YAML is syntactically correct"
          },
          {
            "id": 5,
            "title": "Implement edge case and YAML validity tests",
            "description": "Create tests for edge cases including empty sections, special characters, long strings, and external YAML syntax validation",
            "dependencies": [
              4
            ],
            "details": "Add comprehensive edge case tests to `src/yaml_emitter.zig`:\n- Empty section handling: empty arrays [], empty objects {}, null values\n- Special characters: Unicode, control characters, YAML reserved chars (:[]{}#&*!|>'\"%@`)\n- Long strings: test block scalar threshold, line wrapping behavior\n- String edge cases: strings starting with YAML keywords (true, false, null, numbers), strings with only whitespace\n- Numeric edge cases: very large numbers, decimals, scientific notation, negative numbers\n- Deeply nested structures: validate indentation at multiple nesting levels\n- External validation (if yamlvalidator or similar available): pipe emitted YAML to external parser, verify no syntax errors\n- Round-trip test concept: emit YAML → parse with external tool → compare structure (prepare for future YAML parser integration)\nThese tests ensure robustness and standards compliance.",
            "status": "pending",
            "testStrategy": "Unit tests for each edge case category, verify special chars are properly escaped/quoted, test boundary conditions (empty, very long), attempt external validation with 'yamllint' or similar if available in CI, document known limitations if external validator unavailable"
          }
        ]
      },
      {
        "id": 47,
        "title": "Implement integration tests for end-to-end conversion",
        "description": "Test complete bru2oc workflow from .bru files to verified .yml output",
        "details": "Create integration test suite:\n- Test single file conversion: input.bru → output.yml, verify output matches expected\n- Test directory conversion: batch process test/fixtures/, verify all outputs\n- Test recursive mode: nested directory structure\n- Test --delete mode: verify originals deleted only after successful verification\n- Test --dry-run mode: verify no writes/deletes occur\n- Test --output mode: verify output directory structure preservation\n- Test --keep-comments mode: verify comments preserved\n- Test error handling: malformed .bru files, I/O errors, permission errors\n- Test exit codes: success (0), partial failure (1), bad args (2)\n- Use temporary directories for test isolation\n- Validate output with Bruno v3 CLI if available\nEnd-to-end validation.",
        "testStrategy": "Run integration tests in CI, verify all fixtures convert correctly, test in fresh temporary directories, cleanup after tests",
        "priority": "high",
        "dependencies": [
          42,
          43
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create integration test infrastructure and temporary directory helpers",
            "description": "Set up the integration test framework in build.zig and implement test utilities for temporary directory management and test isolation",
            "dependencies": [],
            "details": "Create test/integration/ directory structure. In build.zig, add an integration test step separate from unit tests that can be run with 'zig build test-integration'. Implement test helper functions in test/integration/helpers.zig: createTempTestDir() to create isolated temporary directories using std.testing.tmpDir(), cleanupTempDir() for cleanup with defer, copyFixturesToTemp() to copy .bru fixtures to temp locations, compareBruToYml() to compare generated .yml files against expected outputs using byte-by-byte comparison or YAML-aware comparison. Use std.testing.allocator for all allocations with leak detection. Create a test runner that discovers and executes all integration test files matching test/integration/*_test.zig pattern. Establish foundation for isolated, reproducible integration testing with automatic cleanup.",
            "status": "pending",
            "testStrategy": "Verify temp directory creation/cleanup works correctly, test fixture copying preserves file contents and structure, validate test discovery finds all integration test files, ensure no directory leaks between test runs"
          },
          {
            "id": 2,
            "title": "Implement single file and directory conversion integration tests",
            "description": "Test basic conversion scenarios: single .bru file conversion and directory batch processing with output verification",
            "dependencies": [
              1
            ],
            "details": "Create test/integration/basic_conversion_test.zig. Implement testSingleFileConversion(): copy a simple .bru fixture to temp dir, invoke bru2oc CLI (using std.ChildProcess or direct function call to main conversion logic), verify output.yml exists and matches expected fixture, verify exit code 0. Implement testDirectoryConversion(): copy test/fixtures/ directory structure to temp, run bru2oc on directory (non-recursive initially), verify all top-level .bru files converted to .yml, compare each output against expected fixture, verify exit code 0, count converted files matches expected. Implement testRecursiveMode(): create nested directory structure in temp with .bru files at multiple levels, run with --recursive flag, verify all nested .bru files converted including subdirectories, verify output directory structure mirrors input, validate all outputs. Use test helpers from subtask 1 for isolation and cleanup.",
            "status": "pending",
            "testStrategy": "Run with various fixture files (primitives, composites, auth types), verify output byte-for-byte or structurally matches expected, test empty directories (should exit with error), test mixed .bru and non-.bru files (non-.bru ignored), verify exit codes"
          },
          {
            "id": 3,
            "title": "Implement CLI mode integration tests (dry-run, delete, output, keep-comments)",
            "description": "Test all CLI mode flags and verify their behavior: --dry-run, --delete, --output, --keep-comments flags work correctly",
            "dependencies": [
              1,
              2
            ],
            "details": "Create test/integration/cli_modes_test.zig. Implement testDryRunMode(): copy fixtures to temp, run with --dry-run flag, verify NO .yml files created (assert output files don't exist), verify NO .bru files deleted, parse stdout/stderr for 'Would convert:' messages, verify exit code 0 for valid inputs. Implement testDeleteMode(): copy fixtures to temp, run conversion with --delete flag, verify .yml files created AND .bru files deleted (assert .bru files no longer exist), verify deletion only happens after successful conversion (test by including one malformed .bru and verify it's NOT deleted while valid ones are). Implement testOutputMode(): run with --output=/custom/path flag, verify .yml files written to specified output directory, verify original .bru files remain in source location, verify output directory structure mirrors input structure. Implement testKeepCommentsMode(): use fixtures with comments, run with --keep-comments flag, verify comments preserved in .yml output, run without flag and verify comments stripped. Test flag combinations (--dry-run with --delete should preview deletion but not execute).",
            "status": "pending",
            "testStrategy": "Test each flag independently and in valid combinations, verify file system state matches expectations (files exist/don't exist), parse CLI output for expected messages, verify --dry-run combined with other flags doesn't perform writes/deletes, test invalid flag combinations if applicable"
          },
          {
            "id": 4,
            "title": "Implement error handling and exit code integration tests",
            "description": "Test error scenarios and verify correct exit codes: malformed .bru files, I/O errors, permission errors, and partial failures",
            "dependencies": [
              1,
              2
            ],
            "details": "Create test/integration/error_handling_test.zig. Implement testMalformedBruFiles(): create invalid .bru fixtures (unclosed braces, syntax errors, bad indentation), run conversion, verify graceful error messages (not panics), verify exit code 1 (partial failure if some succeed) or specific error code, verify valid files still converted in batch. Implement testIOErrors(): test missing input file/directory (verify error.FileNotFound and appropriate exit code 2), test write permission errors by making output directory read-only (on Unix: chmod 444), verify error handling and exit code 1. Implement testPartialFailure(): mix valid and invalid .bru files in directory, run conversion, verify valid files converted successfully, invalid files logged as failures, exit code = 1 (partial success), summary shows 'X converted, Y failed'. Implement testBadArguments(): test with invalid flags (--unknown-flag), missing required path argument, conflicting flags if any, verify exit code 2 for bad arguments, verify helpful error messages. Implement testNoFilesFound(): run on directory with no .bru files, verify error message 'No .bru files found', verify exit code 2 or 1 as specified in task details.",
            "status": "pending",
            "testStrategy": "Verify exit codes match specification (0=all success, 1=partial failure, 2=bad args/no files), test error messages are helpful and include file names/line numbers where applicable, verify graceful degradation (no panics/crashes), test with various malformed inputs from edge_cases fixtures"
          },
          {
            "id": 5,
            "title": "Implement output validation and optional Bruno CLI verification tests",
            "description": "Validate generated .yml files are well-formed YAML and optionally verify compatibility with Bruno v3 CLI if available",
            "dependencies": [
              2,
              3
            ],
            "details": "Create test/integration/output_validation_test.zig. Implement testYamlWellFormedness(): for all integration test outputs, parse generated .yml files using a YAML library or external validator to verify syntactic correctness (no YAML parse errors), verify required OpenCollection fields present (name, method, url, etc.), verify data types match schema (strings, arrays, objects in correct places). Implement testBrunoCliCompatibility() as optional: check if Bruno CLI v3 is available in PATH (bruno --version), if available: invoke bruno validate or similar command on generated .yml files to verify OpenCollection format compatibility, if not available: skip test with warning/log message. Implement testContentFidelity(): for comprehensive fixtures (maximal_request, complete_post_request), verify all data from .bru file present in .yml output (headers, auth, body, scripts, annotations where supported), verify no data loss during conversion, verify correct type transformations (Bru primitives → YAML primitives). Create summary report showing: total integration tests run, files converted, validation pass rate, Bruno CLI compatibility results if tested.",
            "status": "pending",
            "testStrategy": "Test with all fixture types from task 43 (primitives, composites, auth, body, scripts, edge cases), validate YAML syntax externally using 'yamllint' if available in CI, attempt Bruno CLI validation in CI environment (install Bruno CLI as test dependency), verify content integrity through structural comparison, test round-trip concept if feasible (parse generated YAML and compare to expected structure)"
          }
        ]
      },
      {
        "id": 48,
        "title": "Setup build configuration for cross-compilation",
        "description": "Configure build.zig for multi-platform release builds",
        "details": "Modify `build.zig`:\n- Add cross-compilation targets: x86_64-linux, aarch64-linux, x86_64-macos, aarch64-macos, x86_64-windows\n- Create build step for each platform: zig build -Dtarget=<triple> -Doptimize=ReleaseFast\n- Strip debug symbols in release builds\n- Static linking where possible for standalone binaries\n- Output to platform-specific directories: zig-out/bin/<platform>/\n- Add 'release-all' build step that builds for all platforms\n- Configure optimization: ReleaseFast for performance\n- Ensure binary names: bru2oc (unix) and bru2oc.exe (windows)\nCross-platform build support.",
        "testStrategy": "Test builds for all platforms, verify binaries are static, test on actual target platforms if available",
        "priority": "medium",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define cross-compilation target triples and platform configuration",
            "description": "Add structured definitions for all target platforms with their respective build configurations in build.zig",
            "dependencies": [],
            "details": "Create a `Platform` struct or array at the top of build.zig that defines each target platform: x86_64-linux-gnu, aarch64-linux-gnu, x86_64-macos, aarch64-macos, x86_64-windows. For each platform, specify: target triple (std.Target.Query), output directory name (e.g., 'linux-x86_64'), binary name with extension ('bru2oc' or 'bru2oc.exe'), and any platform-specific build options. This provides a single source of truth for all cross-compilation targets.",
            "status": "pending",
            "testStrategy": "Verify the data structure compiles and contains all 5 required platforms. Run `zig build --help` to ensure no build errors were introduced."
          },
          {
            "id": 2,
            "title": "Create individual build steps for each target platform",
            "description": "Implement platform-specific build steps that compile optimized binaries for each target",
            "dependencies": [
              1
            ],
            "details": "For each platform defined in subtask 1, create a dedicated build step using `b.step()` with names like 'release-linux-x86_64', 'release-macos-aarch64', etc. Each step should: create an executable with `b.addExecutable()` using the platform's target triple and ReleaseFast optimization, configure static linking via `exe.linkage = .static` where supported (Linux/Windows), strip debug symbols with `exe.strip = true`, set output path to `zig-out/bin/<platform>/bru2oc[.exe]` using `b.installArtifact()` with custom install directory. Wire the step to depend on the artifact installation.",
            "status": "pending",
            "testStrategy": "Test each individual build step: `zig build release-linux-x86_64`, `zig build release-windows-x86_64`, etc. Verify binaries are created in correct platform-specific directories with correct names and are stripped (check file size reduction)."
          },
          {
            "id": 3,
            "title": "Implement 'release-all' meta build step",
            "description": "Create a single build step that triggers compilation for all platforms in parallel",
            "dependencies": [
              2
            ],
            "details": "Add a top-level build step `const release_all = b.step('release-all', 'Build release binaries for all platforms');` that depends on all individual platform build steps created in subtask 2. Use `release_all.dependOn()` for each platform step. This allows a single command `zig build release-all` to trigger parallel builds for all 5 target platforms (x86_64-linux, aarch64-linux, x86_64-macos, aarch64-macos, x86_64-windows). The Zig build system will automatically parallelize independent builds.",
            "status": "pending",
            "testStrategy": "Run `zig build release-all` and verify that binaries are created for all 5 platforms in their respective directories: zig-out/bin/linux-x86_64/bru2oc, zig-out/bin/macos-aarch64/bru2oc, zig-out/bin/windows-x86_64/bru2oc.exe, etc. Check build parallelization with verbose output."
          },
          {
            "id": 4,
            "title": "Configure static linking and platform-specific linker options",
            "description": "Set up static linking for standalone binaries and handle platform-specific linking requirements",
            "dependencies": [
              2
            ],
            "details": "For each platform executable created in subtask 2, configure static linking: Linux targets should use `exe.linkage = .static` and `exe.link_libc = true` with musl; macOS targets should use dynamic linking (macOS doesn't support fully static binaries) but ensure minimal dependencies; Windows targets should use `exe.linkage = .static` and link against static CRT. Add conditional logic based on target OS using `target.result.os.tag` to apply appropriate settings. Ensure binaries are truly standalone by not linking against external shared libraries where possible.",
            "status": "pending",
            "testStrategy": "Build binaries for each platform and verify static linking: on Linux run `ldd zig-out/bin/linux-x86_64/bru2oc` (should show 'not a dynamic executable' or minimal deps); on Windows check with `dumpbin /dependents` (minimal DLL dependencies); test binaries on clean systems without Zig runtime."
          },
          {
            "id": 5,
            "title": "Add optimization and debug symbol stripping configuration",
            "description": "Configure ReleaseFast optimization mode and ensure debug symbols are stripped from all release builds",
            "dependencies": [
              2
            ],
            "details": "For each platform build step created in subtask 2, explicitly set optimization to ReleaseFast using `.optimize = .ReleaseFast` in the executable configuration (overriding the default `b.standardOptimizeOption()`). Enable symbol stripping with `exe.strip = true` for all release builds to minimize binary size. For Windows, additionally set `exe.subsystem = .Console` to ensure proper console application behavior. Verify that the optimization level is hardcoded for release builds rather than user-selectable, ensuring consistent release artifacts.",
            "status": "pending",
            "testStrategy": "Build all platforms with `zig build release-all` and verify: binaries are significantly smaller than debug builds, no debug symbols present (use `file` command on Linux/macOS, `dumpbin /headers` on Windows), performance is optimized (run basic benchmarks comparing Debug vs ReleaseFast builds)."
          }
        ]
      },
      {
        "id": 49,
        "title": "Setup CI/CD pipeline for automated testing and releases",
        "description": "Configure GitHub Actions for testing, building, and releasing bru2oc",
        "details": "Create `.github/workflows/` configuration:\n- Test workflow: run on every push/PR, execute `zig build test`, multiple Zig versions (0.15.2+)\n- Build workflow: cross-compile for all platforms on release tags\n- Release workflow: triggered by version tags (v*), build all platforms, create GitHub release, upload binaries\n- Artifact naming: bru2oc-<version>-<platform>.tar.gz (unix), .zip (windows)\n- Include checksums (SHA256) for all artifacts\n- Matrix strategy for parallel builds\n- Cache Zig installation and build artifacts\n- Lint check: zig fmt --check\nAutomated quality and release process.",
        "testStrategy": "Test workflows locally with act or push to test branch, verify artifact generation, test release process with pre-release tags",
        "priority": "medium",
        "dependencies": [
          47,
          48
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create test workflow for continuous integration",
            "description": "Create .github/workflows/test.yml that runs tests on every push and pull request",
            "dependencies": [],
            "details": "Create `.github/workflows/test.yml` workflow file:\n- Trigger on: push to all branches, pull_request events\n- Matrix strategy for Zig versions: ['0.15.2', 'master'] (test both stable and latest)\n- Jobs: checkout code, setup Zig using goto-bus-stop/setup-zig@v2 action\n- Cache Zig installation: uses: actions/cache@v4 with key based on Zig version\n- Cache build artifacts: cache ~/.cache/zig for faster rebuilds\n- Run `zig fmt --check src/` to verify code formatting\n- Run `zig build test` to execute all test blocks (both mod_tests and exe_tests defined in build.zig:121-143)\n- Upload test results as artifacts if needed for debugging\n- Fail fast: false to test all Zig versions even if one fails",
            "status": "pending",
            "testStrategy": "Push to a test branch and verify workflow runs successfully, check that both formatting and tests are executed, verify caching works on subsequent runs"
          },
          {
            "id": 2,
            "title": "Create cross-platform build workflow",
            "description": "Create .github/workflows/build.yml for cross-compiling to all supported platforms",
            "dependencies": [
              1
            ],
            "details": "Create `.github/workflows/build.yml` workflow file:\n- Trigger on: release tags (v*), manual workflow_dispatch for testing\n- Matrix strategy for targets: [x86_64-linux, aarch64-linux, x86_64-macos, aarch64-macos, x86_64-windows]\n- Each job: checkout code, setup Zig 0.15.2 (stable for releases)\n- Cache Zig installation and build artifacts\n- Build command: `zig build -Dtarget=<target> -Doptimize=ReleaseFast` (as per build.zig:14,18)\n- Output binary location: zig-out/bin/bru2oc (or bru2oc.exe for Windows per build.zig:60,90)\n- Strip debug symbols if not already done by ReleaseFast\n- Create platform-specific archives: .tar.gz for unix (linux/macos), .zip for windows\n- Naming convention: bru2oc-${{github.ref_name}}-<platform>.{tar.gz|zip}\n- Upload each archive as workflow artifact for release workflow to consume",
            "status": "pending",
            "testStrategy": "Trigger manually with workflow_dispatch, verify all 5 platform binaries are built, download artifacts and test on actual platforms if available, verify archive contents"
          },
          {
            "id": 3,
            "title": "Create checksums generation step",
            "description": "Add checksum generation (SHA256) for all build artifacts in the build workflow",
            "dependencies": [
              2
            ],
            "details": "Extend `.github/workflows/build.yml` with checksum generation:\n- After creating each platform archive, generate SHA256 checksum\n- Use shell command: `sha256sum <archive> > <archive>.sha256` (Linux/macOS) or `certutil -hashfile <archive> SHA256 > <archive>.sha256` (Windows)\n- Or use cross-platform approach: create a dedicated step that runs on ubuntu-latest to generate all checksums after downloading all artifacts\n- Checksum file naming: bru2oc-${{github.ref_name}}-<platform>.{tar.gz|zip}.sha256\n- Also create a combined checksums file: SHA256SUMS containing all checksums in one file\n- Format: `<hash>  <filename>` (two spaces, standard sha256sum format)\n- Upload checksum files as artifacts alongside the binaries",
            "status": "pending",
            "testStrategy": "Verify checksum files are created for each platform, verify SHA256SUMS contains all entries, manually verify checksum by downloading artifact and running sha256sum locally"
          },
          {
            "id": 4,
            "title": "Create GitHub release workflow with artifact publishing",
            "description": "Create .github/workflows/release.yml that creates GitHub releases and uploads all build artifacts",
            "dependencies": [
              3
            ],
            "details": "Create `.github/workflows/release.yml` workflow file:\n- Trigger on: push tags matching 'v*' pattern (e.g., v0.1.0, v1.0.0)\n- First job calls build.yml workflow using workflow_call to generate all artifacts\n- Second job creates GitHub release:\n  - Download all build artifacts from build workflow\n  - Extract version from tag: ${GITHUB_REF#refs/tags/}\n  - Create release using softprops/action-gh-release@v1 or gh CLI\n  - Release title: 'bru2oc ${{github.ref_name}}'\n  - Auto-generate release notes from commits or use changelog if exists\n  - Upload all platform archives (.tar.gz, .zip files)\n  - Upload all checksum files (.sha256, SHA256SUMS)\n  - Mark as draft: false, prerelease: check if version contains '-' (e.g., v1.0.0-beta)\n- Ensure executable permissions are preserved in Unix archives",
            "status": "pending",
            "testStrategy": "Create a test tag (e.g., v0.0.1-test), push it, verify release is created on GitHub with all 5 platform binaries and checksums, verify archives extract correctly with proper permissions"
          },
          {
            "id": 5,
            "title": "Add workflow documentation and optimization",
            "description": "Document CI/CD workflows, add status badges, and optimize workflow performance",
            "dependencies": [
              4
            ],
            "details": "Finalize CI/CD setup:\n- Create or update README.md with CI/CD status badges:\n  - Test workflow: ![Tests](https://github.com/<owner>/bru2oc/workflows/test/badge.svg)\n  - Build workflow badge if applicable\n- Add CONTRIBUTING.md or CI/CD section in README documenting:\n  - How to run tests locally: `zig build test`\n  - How to trigger builds: push version tags (v*)\n  - How to test workflows locally using 'act' tool if desired\n  - Matrix versions tested and supported platforms\n- Optimize caching strategies:\n  - Review cache keys to ensure proper invalidation\n  - Use restore-keys for fallback caching\n  - Consider caching Zig downloads separately from build artifacts\n- Add concurrency groups to cancel outdated workflow runs on new pushes\n- Set appropriate timeout-minutes for each job (e.g., 15-30 min)",
            "status": "pending",
            "testStrategy": "Review README displays badges correctly, verify documentation is clear and accurate, test a full release cycle end-to-end from tag push to release publication"
          }
        ]
      },
      {
        "id": 50,
        "title": "Create documentation and README",
        "description": "Write comprehensive user documentation and README with installation and usage instructions",
        "details": "Create/update documentation files:\n- README.md: project description, installation (download binary, build from source), usage examples, features, limitations\n- Installation section: links to releases, platform-specific instructions, Zig version requirement\n- Usage section: command syntax, all flags with descriptions, examples from PRD\n- Features section: what it does, what it doesn't do, OpenCollection compatibility\n- Examples: single file, directory, recursive, dry-run, output directory\n- Troubleshooting section: common errors, how to report bugs\n- Contributing section: how to build, run tests, submit PRs\n- License information (choose appropriate license)\n- Link to Bru spec and OpenCollection spec\nUser-facing documentation.",
        "testStrategy": "Manual review, test all examples in README, verify installation instructions on fresh machine",
        "priority": "medium",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create comprehensive README.md with project overview",
            "description": "Write the main README.md file with project description, features overview, and quick start section",
            "dependencies": [],
            "details": "Create README.md in project root with:\n- Project title and one-line description: 'bru2oc - Convert Bruno API Collection (.bru) files to OpenCollection YAML format'\n- Badges section (build status, license, Zig version)\n- Overview explaining what bru2oc does: converts .bru files (Bruno API client format) to OpenCollection YAML\n- Key features: single file conversion, directory conversion, recursive directory traversal, dry-run mode, custom output directory\n- What it does and doesn't do: focuses on HTTP request conversion, doesn't support scripts/assertions yet\n- Link to Bru specification and OpenCollection specification\n- Quick start example showing basic usage: './bru2oc input.bru output.yml'",
            "status": "pending",
            "testStrategy": "Manual review of README.md for clarity and completeness, verify all markdown renders correctly on GitHub, ensure links to specs are valid"
          },
          {
            "id": 2,
            "title": "Write installation section with multiple installation methods",
            "description": "Document installation instructions for downloading binaries, building from source, and platform-specific requirements",
            "dependencies": [
              1
            ],
            "details": "Add Installation section to README.md with:\n- Prerequisites: Zig 0.15.2 or later (specified in .tool-versions)\n- Option 1: Download pre-built binaries from GitHub Releases (link to releases page once available)\n- Option 2: Build from source instructions:\n  1. Clone repository: 'git clone <repo-url>'\n  2. Navigate to directory: 'cd bru2oc'\n  3. Build with Zig: 'zig build -Doptimize=ReleaseFast'\n  4. Binary location: 'zig-out/bin/bru2oc'\n  5. Optional: Add to PATH or install to system location\n- Platform-specific notes: macOS, Linux (x86_64, ARM64), Windows support\n- Verify Zig installation: 'zig version' should show 0.15.2+\n- Troubleshooting: minimum Zig version requirement from build.zig.zon",
            "status": "pending",
            "testStrategy": "Test installation instructions on a fresh machine without bru2oc installed, verify each build command works, confirm binary is created in expected location"
          },
          {
            "id": 3,
            "title": "Document usage syntax with all command-line flags and options",
            "description": "Create comprehensive usage documentation showing command syntax, all available flags, and their descriptions",
            "dependencies": [
              1
            ],
            "details": "Add Usage section to README.md with:\n- Basic command syntax: 'bru2oc [options] <input> [output]'\n- Positional arguments:\n  - input: path to .bru file or directory\n  - output: (optional) output path for .yml file or directory\n- Command-line flags (based on PRD requirements):\n  - '--recursive' or '-r': recursively process directories\n  - '--dry-run' or '-n': show what would be converted without writing files\n  - '--output-dir' or '-o': specify output directory for batch conversions\n  - '--verbose' or '-v': enable verbose logging\n  - '--help' or '-h': show help message\n  - '--version': show version information\n- Exit codes: 0 (success), 1 (errors occurred), 2 (invalid arguments)\n- Error handling: file not found, invalid .bru syntax, write permission errors",
            "status": "pending",
            "testStrategy": "Test each flag combination with actual bru2oc binary once implemented, verify help text matches documentation, confirm exit codes are correct"
          },
          {
            "id": 4,
            "title": "Create examples section with practical usage scenarios",
            "description": "Write detailed examples covering all common use cases from single file conversion to batch processing with dry-run",
            "dependencies": [
              3
            ],
            "details": "Add Examples section to README.md with working examples:\n1. Single file conversion:\n   'bru2oc request.bru request.yml'\n   Shows converting one .bru file to YAML\n\n2. Convert to stdout:\n   'bru2oc request.bru -'\n   Useful for piping to other tools\n\n3. Directory conversion:\n   'bru2oc ./bruno-collection ./opencollection-output'\n   Converts all .bru files in directory, preserves structure\n\n4. Recursive conversion:\n   'bru2oc -r ./bruno-collection ./opencollection-output'\n   Processes nested directories\n\n5. Dry-run mode:\n   'bru2oc --dry-run -r ./bruno-collection'\n   Preview what would be converted without writing files\n\n6. Custom output directory:\n   'bru2oc -r -o ./yaml-output ./bruno-collection'\n   Specify different output location\n\nEach example includes:\n- Command\n- Description of what it does\n- Expected output/behavior\n- When to use this pattern",
            "status": "pending",
            "testStrategy": "Run each example command once tool is implemented, verify output matches documentation, test examples produce expected files/output, ensure examples cover PRD requirements"
          },
          {
            "id": 5,
            "title": "Add troubleshooting, contributing, and license sections",
            "description": "Complete README with troubleshooting guide, contribution guidelines, and license information",
            "dependencies": [
              2,
              4
            ],
            "details": "Add final sections to README.md:\n\nTroubleshooting:\n- Common error: 'Zig version mismatch' - Solution: upgrade to Zig 0.15.2+\n- Common error: 'ParseError: UnclosedBlock' - Solution: check .bru file syntax, ensure all blocks are closed\n- Common error: 'FileNotFound' - Solution: verify input path exists\n- Common error: 'PermissionDenied' - Solution: check write permissions on output directory\n- How to report bugs: link to GitHub issues with template (include .bru file, error message, Zig version)\n\nContributing:\n- How to build: 'zig build'\n- How to run tests: 'zig build test'\n- Code style: follow Zig standard library conventions\n- How to submit PRs: fork, create feature branch, add tests, submit PR\n- Link to CONTRIBUTING.md (create if needed)\n\nLicense:\n- Choose appropriate license (MIT or Apache 2.0 recommended for Zig projects)\n- Add LICENSE file to repository\n- Include license badge in README header\n- Copyright year and author information\n\nFooter:\n- Link to related projects: Bruno, OpenCollection spec\n- Credits and acknowledgments",
            "status": "pending",
            "testStrategy": "Manual review of all sections, verify GitHub issue link works, test build and test commands from contributing section, validate license file exists and matches README, check all external links are valid"
          }
        ]
      },
      {
        "id": 51,
        "title": "Create example usage scripts and sample files",
        "description": "Provide example .bru files and shell scripts demonstrating common usage patterns",
        "details": "Create `examples/` directory:\n- Sample .bru files: simple_get.bru, post_with_auth.bru, complex_request.bru\n- Shell scripts: convert_single.sh, convert_directory.sh, migrate_collection.sh\n- Shell scripts demonstrate common patterns: dry-run first, then convert with delete, verify output\n- Include comments explaining each step\n- README in examples/ explaining each example\n- Sample Bruno collection structure for realistic testing\nUser learning resources.",
        "testStrategy": "Manual testing of all example scripts, verify examples work end-to-end, ensure they're beginner-friendly",
        "priority": "low",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create examples directory structure and sample .bru files",
            "description": "Set up the examples/ directory with realistic sample Bruno request files covering different HTTP methods and features",
            "dependencies": [],
            "details": "Create examples/ directory at project root. Create three sample .bru files:\n1. simple_get.bru - Basic GET request with query parameters and headers\n2. post_with_auth.bru - POST request with JSON body and Bearer token authentication\n3. complex_request.bru - Advanced example with multipart form data, multiple headers, variables, and GraphQL\nEach .bru file should demonstrate proper Bru language syntax including meta blocks, http blocks, auth blocks, headers, query params, and body content. Files should represent realistic API testing scenarios that users would commonly encounter.",
            "status": "pending",
            "testStrategy": "Validate each .bru file can be successfully parsed by the bru2oc converter once implemented, verify syntax matches Bruno specification, ensure examples demonstrate diverse feature set"
          },
          {
            "id": 2,
            "title": "Create convert_single.sh script with dry-run pattern",
            "description": "Write shell script demonstrating single file conversion with safety best practices",
            "dependencies": [
              1
            ],
            "details": "Create examples/convert_single.sh that demonstrates:\n1. Check if input .bru file exists\n2. Run bru2oc with --dry-run flag first to preview changes\n3. Prompt user to confirm before actual conversion\n4. Run actual conversion with output to specific .yml file\n5. Verify output file was created successfully\n6. Optionally delete source .bru file with --delete flag if confirmed\nInclude detailed comments explaining each step and why dry-run is important. Show error handling for missing files and failed conversions. Use shellcheck-compliant bash syntax.",
            "status": "pending",
            "testStrategy": "Run script with sample .bru files, verify dry-run output matches expectations, test error conditions like missing files, ensure script is POSIX-compliant and works on Linux/macOS"
          },
          {
            "id": 3,
            "title": "Create convert_directory.sh for batch conversion",
            "description": "Write shell script for converting all .bru files in a directory with progress reporting",
            "dependencies": [
              1
            ],
            "details": "Create examples/convert_directory.sh that demonstrates:\n1. Accept directory path as argument with validation\n2. Find all .bru files in directory (non-recursive by default)\n3. Show count of files to be converted\n4. Optional --dry-run mode to preview all conversions\n5. Loop through files with progress counter\n6. Convert each file to .yml in same directory or --output-dir\n7. Summary report showing success/failure counts\n8. Option to delete source files after successful conversion\nInclude comments explaining parallel processing considerations and error handling. Show how to handle spaces in filenames.",
            "status": "pending",
            "testStrategy": "Test with directory containing multiple .bru files, verify all files converted, test --output-dir flag, ensure proper error handling for permission issues, validate progress reporting accuracy"
          },
          {
            "id": 4,
            "title": "Create migrate_collection.sh for recursive Bruno collection migration",
            "description": "Write comprehensive migration script handling nested Bruno collection structure",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Create examples/migrate_collection.sh that demonstrates:\n1. Accept Bruno collection root directory as argument\n2. Recursively find all .bru files maintaining directory structure\n3. Create parallel directory structure in output location\n4. Handle bruno.json collection metadata file\n5. Convert all requests while preserving folder hierarchy\n6. Generate migration report with file paths and status\n7. Backup option to create .tar.gz of original before conversion\n8. Rollback capability if migration fails\nInclude detailed comments explaining Bruno collection structure, environment handling, and migration best practices. Reference official bru2oc CLI flags.",
            "status": "pending",
            "testStrategy": "Create realistic Bruno collection directory structure with nested folders, run migration script, verify directory structure preserved, test backup and rollback features, ensure bruno.json metadata handled correctly"
          },
          {
            "id": 5,
            "title": "Create examples/README.md documentation",
            "description": "Write comprehensive README explaining all examples and usage patterns",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create examples/README.md with:\n1. Overview section explaining purpose of examples directory\n2. Sample Files section describing each .bru file and what features it demonstrates\n3. Shell Scripts section with description of each script's purpose and usage\n4. Usage Examples showing command-line invocations for each script with expected output\n5. Best Practices section covering dry-run workflow, backup strategies, and common pitfalls\n6. Bruno Collection Structure explanation with example directory tree\n7. Prerequisites listing required tools (bru2oc binary, bash, etc.)\n8. Troubleshooting section for common issues\nUse clear markdown formatting with code blocks, examples, and cross-references to main project README. Make it beginner-friendly for users new to CLI tools.",
            "status": "pending",
            "testStrategy": "Manual review for clarity and completeness, verify all script examples are copy-paste ready, test that code blocks have correct syntax highlighting, ensure cross-references to project README are valid"
          }
        ]
      },
      {
        "id": 52,
        "title": "Implement performance benchmarks",
        "description": "Create benchmarks for parser, transformer, and emitter to measure performance",
        "details": "Create `bench/` directory:\n- Benchmark suite using Zig's benchmark capabilities or std.time.Timer\n- Parse benchmark: measure parse time for various .bru file sizes\n- Transform benchmark: measure transformation time\n- Emit benchmark: measure YAML emission time\n- End-to-end benchmark: measure complete conversion time\n- Generate large test files (100s of requests) for realistic benchmarks\n- Track memory usage with std.heap.GeneralPurposeAllocator\n- Report: ops/sec, MB/sec, memory usage\n- Compare performance across Zig versions and optimization modes\nPerformance monitoring.",
        "testStrategy": "Run benchmarks on various hardware, establish baseline performance, track regressions",
        "priority": "low",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create bench directory structure and benchmark runner framework",
            "description": "Set up the benchmark infrastructure with a main benchmark runner and test data generation utilities",
            "dependencies": [],
            "details": "Create `bench/` directory at project root. Implement `bench/runner.zig` with a benchmark harness using `std.time.Timer` for high-resolution timing measurements. Create `bench/data_generator.zig` to generate synthetic .bru files of various sizes (small: 10 requests, medium: 100 requests, large: 500 requests, xlarge: 1000 requests). The generator should create realistic Bruno request files with varying complexity (headers, bodies, auth blocks, scripts). Implement utility functions for running benchmarks N times and calculating statistics (mean, median, min, max, std deviation). Set up `std.heap.GeneralPurposeAllocator` with tracking enabled to monitor memory allocations during benchmarks.",
            "status": "pending",
            "testStrategy": "Verify data generator creates valid .bru files that can be parsed, test timer accuracy with known delays, validate allocator tracking captures memory metrics"
          },
          {
            "id": 2,
            "title": "Implement parse benchmark suite",
            "description": "Create benchmarks specifically for measuring Bru parser performance across different file sizes and complexity levels",
            "dependencies": [
              1
            ],
            "details": "Create `bench/parse_bench.zig` to benchmark the Bru tokenizer and parser. Measure tokenization time separately from full parse time. Test with generated files of varying sizes (10, 100, 500, 1000 requests). Measure ops/sec (files parsed per second) and throughput in MB/sec. Track memory allocations per parse operation. Test different request complexities: simple GET requests, complex POST with large bodies, requests with many headers (20+), requests with auth blocks and scripts. Report breakdown of time spent in tokenization vs AST construction. Include benchmarks for edge cases like files with many comments, long string literals, deeply nested structures.",
            "status": "pending",
            "testStrategy": "Validate benchmarks run without errors, verify memory is properly freed after each iteration, compare results across different file sizes show expected scaling"
          },
          {
            "id": 3,
            "title": "Implement transformation benchmark suite",
            "description": "Create benchmarks for measuring the performance of transforming parsed Bru IR to OpenCollection format",
            "dependencies": [
              1
            ],
            "details": "Create `bench/transform_bench.zig` to benchmark the transformer module. Measure transformation time from BruDocument to OpenCollectionRequest. Test with pre-parsed IR structures of varying complexity. Benchmark individual transformation functions: transformMeta, transformHttpMethod, transformHeaders, transformBody, transformAuth, transformScript. Measure aggregate transformation performance for complete request conversion. Track memory allocations during transformation. Report ops/sec and memory overhead per transformation type. Include benchmarks for batch transformation of multiple requests to measure overhead of repeated allocations and potential optimization opportunities.",
            "status": "pending",
            "testStrategy": "Verify transformer benchmarks produce valid OpenCollection output, validate memory tracking shows no leaks, ensure statistical measurements are stable across multiple runs"
          },
          {
            "id": 4,
            "title": "Implement YAML emission benchmark suite",
            "description": "Create benchmarks for measuring YAML serialization performance and output file writing",
            "dependencies": [
              1
            ],
            "details": "Create `bench/emit_bench.zig` to benchmark YAML emitter performance. Measure time to serialize OpenCollectionRequest to YAML string. Benchmark file writing operations (buffered vs unbuffered). Test with varying output sizes and nesting depths. Measure throughput in MB/sec for YAML generation. Track memory allocations during emission, especially string building. Benchmark different YAML formatting options (indentation levels, line wrapping). Include benchmarks for emitting large collections (100s of requests to single file vs multiple files). Report memory usage patterns and identify potential buffer sizing optimizations.",
            "status": "pending",
            "testStrategy": "Validate emitted YAML is valid and matches expected format, verify file I/O benchmarks properly measure disk operations, ensure memory tracking accounts for all allocations"
          },
          {
            "id": 5,
            "title": "Implement end-to-end benchmarks and reporting integration",
            "description": "Create complete conversion benchmarks and integrate all benchmark suites into build system with reporting",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Create `bench/e2e_bench.zig` for end-to-end conversion benchmarks measuring complete .bru to .yml pipeline. Benchmark full conversion workflow: read file → parse → transform → emit → write. Test with realistic workloads: single file conversion, batch conversion of 10/100/1000 files, directory traversal and conversion. Compare performance across Zig optimization modes (Debug, ReleaseSafe, ReleaseFast, ReleaseSmall). Update `build.zig` to add a `bench` step that compiles and runs all benchmark suites. Implement `bench/report.zig` to generate formatted benchmark reports with tables showing: ops/sec, MB/sec, memory usage (peak/average), comparison across optimization modes. Generate JSON output for CI/CD integration and historical tracking. Include recommendations based on results (e.g., optimal batch sizes, memory tuning suggestions).",
            "status": "pending",
            "testStrategy": "Run benchmarks across all optimization modes and verify results scale appropriately, validate report generation produces readable output, test benchmark suite integration with zig build bench command"
          }
        ]
      },
      {
        "id": 53,
        "title": "Add fuzzing support for parser robustness",
        "description": "Implement fuzz testing to discover edge cases and parser bugs",
        "details": "Add fuzz tests:\n- Use Zig's std.testing.fuzz framework (see existing fuzz example in main.zig)\n- Fuzz parser with random/mutated Bru input\n- Fuzz targets: tokenizer, primitive parsers, multistring parser, complete parser\n- Verify: no panics, all errors are graceful, no memory leaks\n- Use AddressSanitizer and UndefinedBehaviorSanitizer in fuzz builds\n- Run with: zig build test --fuzz\n- Collect crash inputs for regression tests\n- Consider integrating with OSS-Fuzz or similar fuzzing infrastructure\nRobustness testing.",
        "testStrategy": "Run fuzzing for extended periods (hours/days), verify no crashes, add discovered cases to regression suite",
        "priority": "low",
        "dependencies": [
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create fuzz test infrastructure in build.zig",
            "description": "Add build configuration to support fuzzing with sanitizers enabled",
            "dependencies": [],
            "details": "Modify build.zig to add: (1) A new build option for enabling AddressSanitizer and UndefinedBehaviorSanitizer using .sanitize_c = true and .sanitize_thread = false on test executables; (2) A dedicated fuzz test step accessible via 'zig build fuzz' that runs tests with --fuzz flag; (3) Configure extended timeout for fuzz runs; (4) Add option to specify fuzz iterations/duration. Reference existing test_step implementation in build.zig:141-143.",
            "status": "pending",
            "testStrategy": "Verify 'zig build fuzz' command works and sanitizers are enabled by checking build output"
          },
          {
            "id": 2,
            "title": "Implement tokenizer fuzz tests",
            "description": "Create comprehensive fuzz tests for the Bru tokenizer to catch edge cases in token parsing",
            "dependencies": [
              1
            ],
            "details": "Add fuzz tests in src/bru_parser.zig targeting the tokenizer: (1) Create FuzzTokenizerContext with testOne() method that accepts random byte slices; (2) Initialize tokenizer with mutated input and iterate through all tokens; (3) Verify no panics occur, all errors are properly handled Error types, and tokenizer returns Eof eventually; (4) Test with various input lengths (0 bytes to 10KB); (5) Follow pattern from main.zig:18-27 using std.testing.fuzz. Focus on boundary conditions: empty input, single chars, malformed UTF-8, extremely long lines.",
            "status": "pending",
            "testStrategy": "Run fuzzer for at least 1 hour, verify no crashes with AddressSanitizer enabled, collect any crash inputs"
          },
          {
            "id": 3,
            "title": "Implement primitive parser fuzz tests",
            "description": "Add fuzz testing for all primitive value parsers (null, bool, number, string)",
            "dependencies": [
              1
            ],
            "details": "Create fuzz tests for primitive parsers in src/bru_parser.zig: (1) parseNull fuzzer - test with random strings near 'null' keyword; (2) parseBool fuzzer - test true/false variants and typos; (3) parseNumber fuzzer - test edge cases: very large numbers, scientific notation, signs, decimals, malformed numbers; (4) parseQuotedString fuzzer - test escape sequences, unterminated strings, nested quotes, invalid UTF-8; (5) parseUnquotedString fuzzer - test whitespace handling, special characters. Each fuzzer should verify graceful error handling and no memory leaks detected by sanitizers.",
            "status": "pending",
            "testStrategy": "Run each primitive fuzzer independently, verify all invalid inputs return proper errors not panics, check memory leak detection"
          },
          {
            "id": 4,
            "title": "Implement multistring and complete parser fuzz tests",
            "description": "Create fuzz tests for multistring parser and the complete Bru document parser",
            "dependencies": [
              1
            ],
            "details": "Add advanced fuzz tests: (1) Multistring parser fuzzer - test triple-quote sequences, indentation variations, embedded quotes, large strings (>1MB), unusual whitespace patterns; (2) Complete parser fuzzer that generates random Bru-like documents with braces, brackets, colons, annotations, nested structures; (3) Test parser with truncated inputs, mismatched delimiters, deeply nested structures (100+ levels); (4) Verify parser always returns either valid AST or graceful error; (5) Use allocator with leak detection. Generate inputs by mutating valid Bru samples from test fixtures.",
            "status": "pending",
            "testStrategy": "Run for extended period (4+ hours), verify no UndefinedBehavior detected, ensure max memory usage is bounded"
          },
          {
            "id": 5,
            "title": "Create crash input corpus and OSS-Fuzz integration",
            "description": "Set up infrastructure to collect crash inputs and explore OSS-Fuzz integration for continuous fuzzing",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Implement crash corpus management: (1) Create tests/fuzz_corpus/ directory to store crash-causing inputs; (2) Add mechanism to save crash inputs with timestamps and fuzzer name (tokenizer/primitives/multistring/complete); (3) Create regression test suite that replays all corpus inputs to prevent regressions; (4) Document corpus format in tests/fuzz_corpus/README.md; (5) Research OSS-Fuzz integration requirements for Zig projects; (6) Create fuzzing documentation in docs/FUZZING.md with instructions for running fuzzers locally and interpreting results; (7) Add corpus inputs to version control for CI validation.",
            "status": "pending",
            "testStrategy": "Verify crash inputs are saved correctly, regression suite catches known failures, documentation is clear for contributors"
          }
        ]
      },
      {
        "id": 54,
        "title": "Implement reverse conversion support (YAML to Bru)",
        "description": "Add optional reverse conversion from OpenCollection YAML back to Bru format",
        "details": "Create `src/yaml_parser.zig` and `src/bru_emitter.zig`:\n- Parse OpenCollection YAML into OpenCollectionRequest struct\n- Transform OpenCollectionRequest to BruDocument IR\n- Emit Bru format from IR\n- Reverse annotation mapping: enabled: false → @disabled\n- Reverse body mapping: type: json → body:json block\n- CLI flag: --reverse or separate `oc2bru` command\n- Handle cases where YAML has no Bru equivalent (warn and skip)\n- Preserve comments if present in YAML\nFuture enhancement for bidirectional workflow.",
        "testStrategy": "Round-trip tests: .bru → .yml → .bru, verify fidelity, test with OpenCollection fixtures",
        "priority": "low",
        "dependencies": [
          42
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create YAML parser module and OpenCollectionRequest deserializer",
            "description": "Implement src/yaml_parser.zig to parse OpenCollection YAML format into OpenCollectionRequest structs",
            "dependencies": [],
            "details": "Create `src/yaml_parser.zig` with YAML parsing functionality:\n- Use Zig YAML library (e.g., zig-yaml or similar) for parsing\n- Implement `parse(allocator: Allocator, yaml_content: []const u8) !OpenCollectionRequest`\n- Parse YAML structure into OpenCollectionRequest struct (defined in src/opencollection.zig from task 6)\n- Handle all OpenCollection sections: info, http (method, url, headers, params, body, auth), runtime (scripts, assertions, vars), settings, docs\n- Map YAML keys to struct fields: 'name' → Info.name, 'method' → Http.method, etc.\n- Parse optional fields correctly (runtime, settings, docs can be null)\n- Handle YAML arrays for headers, params, scripts, assertions, vars\n- Parse tagged union types (Body, Auth) based on YAML structure\n- Validate required fields exist (name, method, url)\n- Return detailed parse errors with line numbers using existing error system from task 1\n- Use ArenaAllocator for memory management similar to Bru parser",
            "status": "pending",
            "testStrategy": "Unit tests: parse minimal valid YAML (name, method, url only), parse complete YAML with all sections, parse with optional fields missing (null runtime/settings/docs), test each Body variant (json, xml, text, form-urlencoded, multipart, graphql), test each Auth variant (bearer, basic, oauth2, awsv4, digest), verify arrays parsed correctly, error on missing required fields, error on malformed YAML, verify line number tracking in errors, test with std.testing.allocator for memory safety"
          },
          {
            "id": 2,
            "title": "Implement transformer from OpenCollectionRequest to BruDocument IR",
            "description": "Create reverse transformation logic to convert OpenCollectionRequest structs into BruDocument intermediate representation",
            "dependencies": [
              1
            ],
            "details": "Add `src/reverse_transformer.zig` with transformation functions:\n- Main function: `transform(allocator: Allocator, request: OpenCollectionRequest) !BruDocument`\n- Create BruDocument with ArenaAllocator (same structure as task 2)\n- Transform Info to meta block: name → 'meta { name: \"...\", type: \"...\", seq: ..., tags: [...] }'\n- Transform Http.method to method block: 'GET', 'POST', etc. → 'get {' or 'post {'\n- Transform Http.url to url entry: url → 'url: \"...\"'\n- Transform headers: []Header → 'headers { name: value, ... }' with enabled:false → @disabled annotation\n- Transform params: []Param → 'params:query { ... }' or 'params:path { ... }' based on ParamType, enabled:false → @disabled\n- Transform Body variants to body blocks: Json → 'body:json { ... }', FormUrlEncoded → 'body:form-urlencoded { ... }', etc.\n- Transform Auth variants to auth blocks: Bearer → 'auth:bearer { token: \"...\" }', Basic → 'auth:basic { username: \"...\", password: \"...\" }', etc.\n- Transform Runtime scripts/assertions/vars to vars:pre-request, vars:post-response, tests blocks\n- Transform docs to docs block with multistring content\n- Reverse annotation mapping: enabled:false → @disabled annotation on Entry\n- Handle cases with no Bru equivalent: warn and skip (e.g., OpenCollection-specific features)\n- Return populated BruDocument with all transformed Entry objects",
            "status": "pending",
            "testStrategy": "Unit tests: transform minimal request (name, method, url), transform complete request with all sections, verify meta block structure, verify each HTTP method maps correctly, verify headers with disabled entries get @disabled, verify body type mappings (json→body:json, xml→body:xml, etc.), verify auth type mappings, verify runtime transformations, test unknown/unsupported features emit warnings, integration test: round-trip transformation (BruDocument→OpenCollectionRequest→BruDocument should match structure), verify memory management with ArenaAllocator"
          },
          {
            "id": 3,
            "title": "Create Bru format emitter for IR to text conversion",
            "description": "Implement src/bru_emitter.zig to emit Bru format text from BruDocument intermediate representation",
            "dependencies": [
              2
            ],
            "details": "Create `src/bru_emitter.zig` with Bru text emission:\n- Main function: `emit(allocator: Allocator, doc: BruDocument) ![]const u8`\n- Use ArrayList(u8) as buffer with buffered writer\n- Emit top-level entries in order (meta first, then method block, url, headers, params, body, auth, runtime, docs)\n- Emit Entry as 'key: value' format\n- Handle Value variants: Null → 'null', Bool → 'true'/'false', Number → preserve string representation, String → quote if needed, Multimap → '{ ... }' with indentation, Array → '[ ... ]', Multistring → triple-quote block with proper indentation\n- Emit annotations before entries: @disabled, @description(...), custom annotations\n- Emit disabled entries with tilde prefix: ~key: value (alternative to @disabled)\n- Handle multistring content: emit as '''\\n content \\n''' with proper indentation (2 spaces per level)\n- Preserve formatting: blank lines between major blocks (meta, method, body, etc.)\n- Emit comments if present in IR (from --keep-comments mode)\n- Return owned slice from ArrayList.toOwnedSlice()\n- Handle write errors gracefully with errdefer cleanup",
            "status": "pending",
            "testStrategy": "Unit tests: emit minimal document (meta, method, url), emit each Value variant type, emit multimap with nested entries, emit array values, emit multistring with triple quotes and indentation, emit entries with annotations (@disabled, @description), emit disabled entries with tilde prefix, emit complete document with all block types, verify formatting (indentation, blank lines, spacing), verify quote escaping in strings, round-trip test (parse Bru → BruDocument → emit → compare with original), test with std.testing.allocator for memory safety, verify output is valid Bru syntax"
          },
          {
            "id": 4,
            "title": "Add CLI flag and integrate reverse conversion pipeline",
            "description": "Add --reverse flag to CLI and wire up YAML→Bru conversion pipeline in main.zig",
            "dependencies": [
              3
            ],
            "details": "Extend `src/main.zig` with reverse conversion support:\n- Add CLI flag: --reverse or -r for reverse mode, OR create separate oc2bru command\n- Update CLI argument parsing to detect reverse mode flag\n- Implement reverse conversion function: `convertYamlToBru(allocator: Allocator, input_path: []const u8, output_path: []const u8, options: ConvertOptions) !void`\n- Read YAML file from input_path using std.fs.cwd().readFileAlloc\n- Call yaml_parser.parse() to get OpenCollectionRequest\n- Call reverse_transformer.transform() to get BruDocument\n- Call bru_emitter.emit() to get Bru text\n- Write output to file or stdout\n- Handle errors at each stage with proper error propagation and user-friendly messages\n- Support glob patterns for batch conversion: *.yml → *.bru\n- Integrate with existing dry-run mode (from task 39) for preview\n- Integrate with existing delete mode to remove source YAML files\n- Update help text to document --reverse flag and reverse conversion usage\n- Ensure memory cleanup with defer statements for all allocations",
            "status": "pending",
            "testStrategy": "Integration tests: convert single YAML file to Bru with --reverse flag, convert multiple files with glob pattern, verify output file created with correct name (.bru extension), test dry-run with --reverse (no files written), test --reverse --delete (YAML files removed), test error handling (invalid YAML, missing files, permission errors), verify help text includes --reverse documentation, end-to-end test: convert sample.yml → sample.bru and verify content, test with various ConvertOptions combinations"
          },
          {
            "id": 5,
            "title": "Implement round-trip testing and comment preservation",
            "description": "Create comprehensive round-trip tests and implement YAML comment preservation for bidirectional workflow",
            "dependencies": [
              4
            ],
            "details": "Add round-trip testing and comment support:\n- Create test fixtures in test/ directory: sample .bru files and corresponding .yml files\n- Implement round-trip test suite: .bru → .yml → .bru, verify fidelity\n- Test bidirectional conversion: .yml → .bru → .yml, verify equivalence\n- Identify lossy conversions (features that don't survive round-trip) and document them\n- Extend yaml_parser.zig to preserve YAML comments (if present) during parsing\n- Store comments in BruDocument IR with position hints\n- Emit preserved comments in bru_emitter.zig as Bru # comments\n- Handle edge cases: YAML-only features that have no Bru equivalent (emit warning and skip)\n- Create conversion validation: compare semantic content ignoring formatting differences\n- Test with real-world OpenCollection examples from fixtures\n- Document limitations: features lost in conversion, format differences, known issues\n- Add integration test using OpenCollection test suite if available\n- Verify memory safety across entire pipeline with memory leak detection",
            "status": "pending",
            "testStrategy": "Round-trip integration tests: bru→yml→bru preserves structure, yml→bru→yml preserves content, test with minimal fixtures (name/method/url only), test with complete fixtures (all sections populated), test comment preservation both directions, test with OpenCollection official examples, verify semantic equivalence (parse both and compare IR), document lossy transformations in test comments, test error cases (YAML features with no Bru mapping), verify warnings emitted for unsupported features, benchmark conversion performance, full memory leak detection with valgrind or similar, cross-validate output with external Bru parser"
          }
        ]
      }
    ],
    "metadata": {
      "created": "2026-02-17T02:48:23.111Z",
      "updated": "2026-02-17T02:48:23.111Z",
      "description": "Tasks for master context"
    }
  }
}